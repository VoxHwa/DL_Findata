{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnum_epochs = 1000\\n# 设置优化器和损失函数\\noptimizer = optim.Adam(model.parameters(),lr=1e-5)\\ncriterion = nn.L1Loss()\\n# 训练模型\\ntrain_model(model, optimizer, criterion, train_src_tensor, train_tgt_tensor, val_src_tensor, val_tgt_tensor, num_epochs=num_epochs, batch_size=128)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# 定义Add & Norm层\n",
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(AddNorm, self).__init__()\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "    def forward(self, x, residual):\n",
    "        return self.norm(x + residual)\n",
    "\n",
    "# 定义Feed Forward层\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, hidden_dim, ff_dim):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(hidden_dim, ff_dim)\n",
    "        self.linear2 = nn.Linear(ff_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "# 定义Multi-Head Attention层\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "\n",
    "        self.query = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.key = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.value = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        Q = self.query(query).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.key(key).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.value(value).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        energy = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.head_dim)\n",
    "\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "\n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "\n",
    "        x = torch.matmul(attention, V).transpose(1, 2).contiguous().view(batch_size, -1, self.hidden_dim)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# 定义Encoder层\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads, ff_dim):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(hidden_dim, num_heads)\n",
    "        self.ff = FeedForward(hidden_dim, ff_dim)\n",
    "        self.norm1 = AddNorm(hidden_dim)\n",
    "        self.norm2 = AddNorm(hidden_dim)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        residual = x\n",
    "        x = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x, residual)\n",
    "\n",
    "        residual = x\n",
    "        x = self.ff(x)\n",
    "        x = self.norm2(x, residual)\n",
    "\n",
    "        return x\n",
    "\n",
    "# 定义Decoder层\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads, ff_dim):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(hidden_dim, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(hidden_dim, num_heads)\n",
    "        self.ff = FeedForward(hidden_dim, ff_dim)\n",
    "        self.norm1 = AddNorm(hidden_dim)\n",
    "        self.norm2 = AddNorm(hidden_dim)\n",
    "        self.norm3 = AddNorm(hidden_dim)\n",
    "\n",
    "    def forward(self, x, enc_outputs, src_mask=None, tgt_mask=None):\n",
    "        residual = x\n",
    "        x = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x, residual)\n",
    "\n",
    "        residual = x\n",
    "        x = self.cross_attn(x, enc_outputs, enc_outputs, src_mask)\n",
    "        x = self.norm2(x, residual)\n",
    "\n",
    "        residual = x\n",
    "        x = self.ff(x)\n",
    "        x = self.norm3(x, residual)\n",
    "\n",
    "        return x\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, hidden_dim, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, hidden_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, hidden_dim, 2).float() * (-np.log(10000.0) / hidden_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_heads, num_layers, ff_dim):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.input_embedding = nn.Linear(input_dim, hidden_dim)\n",
    "        self.output_embedding = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(hidden_dim, num_heads, ff_dim) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(hidden_dim, num_heads, ff_dim) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, src, tgt_mask=None):\n",
    "        src_embed = self.input_embedding(src)\n",
    "\n",
    "        enc_outputs = src_embed\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_outputs = enc_layer(enc_outputs)\n",
    "\n",
    "        dec_outputs = enc_outputs\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_outputs = dec_layer(dec_outputs, enc_outputs, tgt_mask=tgt_mask)\n",
    "\n",
    "        outputs = self.output_embedding(dec_outputs[:, -1, :])\n",
    "        return outputs\n",
    "'''\n",
    "# 定义Transformer模型\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_heads, num_layers, ff_dim):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.input_embedding = nn.Linear(input_dim, hidden_dim)\n",
    "        self.pos_encoding = PositionalEncoding(hidden_dim)\n",
    "        self.output_embedding = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(hidden_dim, num_heads, ff_dim) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(hidden_dim, num_heads, ff_dim) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, src, tgt_mask=None):\n",
    "        src_embed = self.input_embedding(src)\n",
    "        src_embed = self.pos_encoding(src_embed)\n",
    "\n",
    "        enc_outputs = src_embed\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_outputs = enc_layer(enc_outputs)\n",
    "\n",
    "        dec_outputs = enc_outputs\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_outputs = dec_layer(dec_outputs, enc_outputs, tgt_mask=tgt_mask)\n",
    "\n",
    "        outputs = self.output_embedding(dec_outputs[:, -1, :])\n",
    "        return outputs'''\n",
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "data = pd.read_csv('tmp2.csv')\n",
    "#data_stock=data[data.name=='万科A']\n",
    "\n",
    "features = ['company_id','open', 'close', 'high', 'low', 'volume', 'money_netflow', 'money_inflow', 'money_outflow',\n",
    "       'net_inflow_rate', 'list_sector', 'CPI', '无风险利率',\n",
    "       'total_market_cap', 'float_market_cap', 'pe_ttm', 'pb',\n",
    "       'dividend_yield_ratio', 'major_id', 'minor_id',\n",
    "       'change_ratio']\n",
    "data = data[features]\n",
    "\n",
    "# %%\n",
    "import numpy as np\n",
    "\n",
    "def create_dataset(data, look_back=5):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(data) - look_back - 1):\n",
    "        X.append(data[i:(i + look_back), :-1])\n",
    "        Y.append(data[i + look_back, -1])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "# %%\n",
    "def create_all_dataset(data, look_back=5):\n",
    "    X, Y = [], []\n",
    "    # 划分\n",
    "    for i in range(len(data.company_id.unique())):\n",
    "        minidata=data[data.company_id==i]\n",
    "        x, y = create_dataset(minidata.values, look_back) # array合并\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "    return np.concatenate(X), np.concatenate(Y)\n",
    "\n",
    "# %%\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# 创建 StandardScaler 对象\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# 对数据进行归一化处理\n",
    "\n",
    "scale_need_cols = [ 'open', 'close', 'high', 'low', 'volume', 'money_netflow', 'money_inflow', 'money_outflow',\n",
    "       'net_inflow_rate', 'CPI', '无风险利率',\n",
    "       'total_market_cap', 'float_market_cap', 'pe_ttm', 'pb',\n",
    "       'dividend_yield_ratio',\n",
    "       'change_ratio']\n",
    "\n",
    "data.loc[:, scale_need_cols] = scaler.fit_transform(data[scale_need_cols])\n",
    "\n",
    "# %%\n",
    "look_back = 5\n",
    "X, Y = create_all_dataset(data, look_back)\n",
    "\n",
    "# %%\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "#设置训练设备\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'\n",
    "# 设置模型参数\n",
    "input_dim = 20\n",
    "hidden_dim = 128\n",
    "output_dim = 1\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "ff_dim = 512\n",
    "\n",
    "# 创建模型实例\n",
    "model = Transformer(input_dim, hidden_dim, output_dim, num_heads, num_layers, ff_dim)\n",
    "model.to(device)\n",
    "\n",
    "# %%\n",
    "def directional_accuracy(actual, predicted):\n",
    "    return np.mean(np.sign(actual) == np.sign(predicted))\n",
    "\n",
    "# %%\n",
    "import os\n",
    "def train_model(model, optimizer, criterion, train_src, train_tgt, val_src, val_tgt, num_epochs, batch_size, eval_interval=100, checkpoint_interval=100, checkpoint_dir='checkpoints'):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)  # 创建checkpoint目录\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # 训练模式\n",
    "        model.train()\n",
    "        epoch_train_loss = 0\n",
    "        for i in range(0, len(train_src), batch_size):\n",
    "            batch_src = train_src[i:i+batch_size]\n",
    "            batch_tgt = train_tgt[i:i+batch_size]\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_src)\n",
    "            loss = criterion(outputs.view(-1), batch_tgt.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_train_loss += loss.item()\n",
    "        train_losses.append(epoch_train_loss / (len(train_src) / batch_size))\n",
    "        \n",
    "        # 每隔eval_interval个epoch进行一次评估\n",
    "        if (epoch + 1) % eval_interval == 0:\n",
    "            # 评估模式\n",
    "            model.eval()\n",
    "            model.to('cpu')  # 将模型移动到CPU\n",
    "            with torch.no_grad():\n",
    "                train_outputs = model(train_src.to('cpu'))  # 将输入数据移动到CPU\n",
    "                val_outputs = model(val_src.to('cpu'))  # 将输入数据移动到CPU \n",
    "                train_loss = criterion(train_outputs.view(-1), train_tgt.view(-1).to('cpu'))  # 将目标数据移动到CPU\n",
    "                val_loss = criterion(val_outputs.view(-1), val_tgt.view(-1).to('cpu'))  # 将目标数据移动到CPU\n",
    "                train_mae = torch.mean(torch.abs(train_tgt.to('cpu') - train_outputs.view(-1)))  # 将目标数据移动到CPU\n",
    "                val_mae = torch.mean(torch.abs(val_tgt.to('cpu') - val_outputs.view(-1)))  # 将目标数据移动到CPU\n",
    "                train_rmse = torch.sqrt(torch.mean((train_tgt.to('cpu') - train_outputs.view(-1))**2))  # 将目标数据移动到CPU\n",
    "                val_rmse = torch.sqrt(torch.mean((val_tgt.to('cpu') - val_outputs.view(-1))**2))  # 将目标数据移动到CPU\n",
    "                train_da = directional_accuracy(train_tgt.to('cpu'), train_outputs.view(-1))  # 将目标数据移动到CPU\n",
    "                val_da = directional_accuracy(val_tgt.to('cpu'), val_outputs.view(-1))  # 将目标数据移动到CPU\n",
    "            model.to(device)  # 将模型移动回GPU\n",
    "            val_losses.append(val_loss.item())\n",
    "            \n",
    "            # 如果当前epoch的验证损失是最好的，就保存当前模型\n",
    "            if val_loss.item() < best_val_loss:\n",
    "                best_val_loss = val_loss.item()\n",
    "                torch.save(model.state_dict(), f\"{checkpoint_dir}/best_model.pt\")\n",
    "            \n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss.item():.8f}, Train MAE: {train_mae.item():.8f}, Train RMSE: {train_rmse.item():.8f}, Train DA: {train_da:.8f}, Val Loss: {val_loss.item():.8f}, Val MAE: {val_mae.item():.8f}, Val RMSE: {val_rmse.item():.8f}, Val DA: {val_da:.8f}\")\n",
    "        \n",
    "        # 每隔checkpoint_interval个epoch保存一次模型\n",
    "        if (epoch + 1) % checkpoint_interval == 0:\n",
    "            torch.save(model.state_dict(), f\"{checkpoint_dir}/model_epoch_{epoch+1}.pt\")\n",
    "\n",
    "# %%\n",
    "\n",
    "# 准备训练和验证数据\n",
    "train_src_data = X_train\n",
    "train_tgt_data = Y_train\n",
    "val_src_data = X_test\n",
    "val_tgt_data = Y_test\n",
    "\n",
    "train_src_tensor = torch.tensor(train_src_data,dtype=torch.float32,device=device)\n",
    "#train_src_tensor = train_src_data.clone().detach().to(device)\n",
    "\n",
    "train_tgt_tensor = torch.tensor(train_tgt_data,dtype=torch.float32,device=device)\n",
    "val_src_tensor = torch.tensor(val_src_data,dtype=torch.float32,device=device)\n",
    "#val_src_tensor = val_src_data.clone().detach().to(device)\n",
    "\n",
    "val_tgt_tensor = torch.tensor(val_tgt_data,dtype=torch.float32,device=device)\n",
    "\n",
    "# 设置参数\n",
    "'''\n",
    "num_epochs = 1000\n",
    "# 设置优化器和损失函数\n",
    "optimizer = optim.Adam(model.parameters(),lr=1e-5)\n",
    "criterion = nn.L1Loss()\n",
    "# 训练模型\n",
    "train_model(model, optimizer, criterion, train_src_tensor, train_tgt_tensor, val_src_tensor, val_tgt_tensor, num_epochs=num_epochs, batch_size=128)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_transform_last_column(X, scaler):\n",
    "    # 获取原始数据的均值和标准差\n",
    "    mean = scaler.mean_[-1]  # 最后一列的均值\n",
    "    scale = scaler.scale_[-1]  # 最后一列的标准差\n",
    "\n",
    "    # 创建一个副本,避免修改原始数据\n",
    "    X_inv = X.copy()\n",
    "\n",
    "    # 对最后一列进行反归一化\n",
    "    X_inv = X_inv * scale + mean\n",
    "\n",
    "    return X_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('checkpoints/model_epoch_600.pt'))\n",
    "#model.load_state_dict(torch.load('640_1e-5.pth'))\n",
    "Y_test_inverse = inverse_transform_last_column(Y_test, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE):  0.00048017531372350914\n",
      "Mean Absolute Error (MAE):  0.013786783969065028\n",
      "R-squared (R^2):  0.1873341248143362\n",
      "DA: 0.7007074395253309\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "model.eval()\n",
    "model.to('cpu')  # 将模型移动到CPU\n",
    "with torch.no_grad():\n",
    "    Y_pred = model(val_src_tensor)\n",
    "    Y_pred = Y_pred.cpu().numpy().flatten()\n",
    "    Y_pred_inverse = inverse_transform_last_column(Y_pred, scaler)\n",
    "    \n",
    "\n",
    "    mse = mean_squared_error(Y_test_inverse, Y_pred_inverse)\n",
    "    mae = mean_absolute_error(Y_test_inverse, Y_pred_inverse)\n",
    "    r2 = r2_score(Y_test_inverse, Y_pred_inverse)\n",
    "    da = directional_accuracy(Y_test_inverse, Y_pred_inverse)\n",
    "    print(\"Mean Squared Error (MSE): \", mse)\n",
    "    print(\"Mean Absolute Error (MAE): \", mae)\n",
    "    print(\"R-squared (R^2): \", r2)\n",
    "    print(\"DA:\",da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.to('cpu')\n",
    "model(train_src_tensor.to('cpu'))\n",
    "\n",
    "model.eval()\n",
    "model.to('cpu')  # 将模型移动到CPU\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_outputs = model(train_src.to('cpu'))  # 将输入数据移动到CPU\n",
    "    val_outputs = model(val_src.to('cpu'))  # 将输入数据移动到CPU\n",
    "    \n",
    "    train_loss = criterion(train_outputs.view(-1), train_tgt.view(-1).to('cpu'))  # 将目标数据移动到CPU\n",
    "    val_loss = criterion(val_outputs.view(-1), val_tgt.view(-1).to('cpu'))  # 将目标数据移动到CPU\n",
    "\n",
    "    train_mae = torch.mean(torch.abs(train_tgt.to('cpu') - train_outputs.view(-1)))  # 将目标数据移动到CPU\n",
    "    val_mae = torch.mean(torch.abs(val_tgt.to('cpu') - val_outputs.view(-1)))  # 将目标数据移动到CPU\n",
    "    train_rmse = torch.sqrt(torch.mean((train_tgt.to('cpu') - train_outputs.view(-1))**2))  # 将目标数据移动到CPU\n",
    "    val_rmse = torch.sqrt(torch.mean((val_tgt.to('cpu') - val_outputs.view(-1))**2))  # 将目标数据移动到CPU\n",
    "    train_da = directional_accuracy(train_tgt.to('cpu'), train_outputs.view(-1))  # 将目标数据移动到CPU\n",
    "    val_da = directional_accuracy(val_tgt.to('cpu'), val_outputs.view(-1))  # 将目标数据移动到CPU\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
