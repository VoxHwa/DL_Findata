{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# 定义Add & Norm层\n",
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(AddNorm, self).__init__()\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "    def forward(self, x, residual):\n",
    "        return self.norm(x + residual)\n",
    "\n",
    "# 定义Feed Forward层\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, hidden_dim, ff_dim):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(hidden_dim, ff_dim)\n",
    "        self.linear2 = nn.Linear(ff_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "# 定义Multi-Head Attention层\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "\n",
    "        self.query = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.key = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.value = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        Q = self.query(query).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.key(key).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.value(value).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        energy = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.head_dim)\n",
    "\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "\n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "\n",
    "        x = torch.matmul(attention, V).transpose(1, 2).contiguous().view(batch_size, -1, self.hidden_dim)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# 定义Encoder层\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads, ff_dim):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(hidden_dim, num_heads)\n",
    "        self.ff = FeedForward(hidden_dim, ff_dim)\n",
    "        self.norm1 = AddNorm(hidden_dim)\n",
    "        self.norm2 = AddNorm(hidden_dim)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        residual = x\n",
    "        x = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x, residual)\n",
    "\n",
    "        residual = x\n",
    "        x = self.ff(x)\n",
    "        x = self.norm2(x, residual)\n",
    "\n",
    "        return x\n",
    "\n",
    "# 定义Decoder层\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads, ff_dim):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(hidden_dim, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(hidden_dim, num_heads)\n",
    "        self.ff = FeedForward(hidden_dim, ff_dim)\n",
    "        self.norm1 = AddNorm(hidden_dim)\n",
    "        self.norm2 = AddNorm(hidden_dim)\n",
    "        self.norm3 = AddNorm(hidden_dim)\n",
    "\n",
    "    def forward(self, x, enc_outputs, src_mask=None, tgt_mask=None):\n",
    "        residual = x\n",
    "        x = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x, residual)\n",
    "\n",
    "        residual = x\n",
    "        x = self.cross_attn(x, enc_outputs, enc_outputs, src_mask)\n",
    "        x = self.norm2(x, residual)\n",
    "\n",
    "        residual = x\n",
    "        x = self.ff(x)\n",
    "        x = self.norm3(x, residual)\n",
    "\n",
    "        return x\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, hidden_dim, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, hidden_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, hidden_dim, 2).float() * (-np.log(10000.0) / hidden_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_heads, num_layers, ff_dim):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.input_embedding = nn.Linear(input_dim, hidden_dim)\n",
    "        self.output_embedding = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(hidden_dim, num_heads, ff_dim) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(hidden_dim, num_heads, ff_dim) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, src, tgt_mask=None):\n",
    "        src_embed = self.input_embedding(src)\n",
    "\n",
    "        enc_outputs = src_embed\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_outputs = enc_layer(enc_outputs)\n",
    "\n",
    "        dec_outputs = enc_outputs\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_outputs = dec_layer(dec_outputs, enc_outputs, tgt_mask=tgt_mask)\n",
    "\n",
    "        outputs = self.output_embedding(dec_outputs[:, -1, :])\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('tmp2.csv')\n",
    "\n",
    "features = ['instrument','company_id','open', 'close', 'high', 'low', 'volume', 'money_netflow', 'money_inflow', 'money_outflow',\n",
    "       'net_inflow_rate', 'list_sector', 'CPI', '无风险利率',\n",
    "       'total_market_cap', 'float_market_cap', 'pe_ttm', 'pb',\n",
    "       'dividend_yield_ratio', 'major_id', 'minor_id',\n",
    "       'change_ratio']\n",
    "data = data[features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "instrument_list=[\n",
    "                '600519.SH',\n",
    "\n",
    "                '002304.SZ',\n",
    "\n",
    "                '000568.SZ',\n",
    "                '600000.SH',\n",
    "                '601988.SH',\n",
    "                '601398.SH',\n",
    "                '601288.SH',\n",
    "                '600919.SH',\n",
    "                '603259.SH',\n",
    "                '002737.SZ',\n",
    "                '600566.SH',\n",
    "                '300015.SZ',\n",
    "                '600721.SH',\n",
    "                '300750.SZ',\n",
    "                '002455.SZ',\n",
    "                '600104.SH',\n",
    "                '002594.SZ',\n",
    "                '601633.SH'\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect(data:pd.DataFrame,instrument_code:str)->np.ndarray:\n",
    "        '''\n",
    "        返回指定股票的价格和收益率\n",
    "        '''\n",
    "        minidata = data[data.instrument==instrument_code]\n",
    "        tmp = minidata.values[:,1:]\n",
    "        close_price = tmp[:,2] # close price\n",
    "        ratio = tmp[:,-1] # ratio\n",
    "        return close_price, ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (input_embedding): Linear(in_features=20, out_features=128, bias=True)\n",
       "  (output_embedding): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (encoder_layers): ModuleList(\n",
       "    (0-5): 6 x EncoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder_layers): ModuleList(\n",
       "    (0-5): 6 x DecoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (cross_attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm3): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_dataset(data, look_back=5):\n",
    "    X, Y, Z = [], [], []\n",
    "    for i in range(len(data) - look_back - 1):\n",
    "        X.append(data[i:(i + look_back), :-1])\n",
    "        Y.append(data[i + look_back, -1])\n",
    "        Z.append(data[i+1:(i + look_back),-1])\n",
    "    return np.array(X), np.array(Y), np.array(Z)\n",
    "\n",
    "\n",
    "def create_all_dataset(data, instrument_list, look_back=5):\n",
    "    X, Y, Z = [], [], []\n",
    "    # 划分\n",
    "    for i in instrument_list:\n",
    "        minidata=data[data.instrument==i]\n",
    "        x, y, z = create_dataset(minidata.values[:,1:], look_back)#去除instrument列 # array合并\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "        Z.append(z)\n",
    "    return X, Y, Z\n",
    "\n",
    "# %%\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# 创建 StandardScaler 对象\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# 对数据进行归一化处理\n",
    "\n",
    "scale_need_cols = [ 'open', 'close', 'high', 'low', 'volume', 'money_netflow', 'money_inflow', 'money_outflow',\n",
    "       'net_inflow_rate', 'CPI', '无风险利率',\n",
    "       'total_market_cap', 'float_market_cap', 'pe_ttm', 'pb',\n",
    "       'dividend_yield_ratio',\n",
    "       'change_ratio']\n",
    "data_copy=data.copy()\n",
    "data_copy.loc[:, scale_need_cols] = scaler.fit_transform(data_copy[scale_need_cols])\n",
    "\n",
    "\n",
    "\n",
    "look_back = 5\n",
    "X, Y, Z = create_all_dataset(data_copy, instrument_list, look_back)\n",
    " \n",
    "\n",
    "\n",
    "#设置训练设备\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = 'cpu'\n",
    "# 设置模型参数\n",
    "input_dim = 20\n",
    "hidden_dim = 128\n",
    "output_dim = 1\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "ff_dim = 512\n",
    "\n",
    "# 创建模型实例\n",
    "model = Transformer(input_dim, hidden_dim, output_dim, num_heads, num_layers, ff_dim)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rearrange(X, n, index):\n",
    "        tmp = {}\n",
    "        for j in range(n):\n",
    "            tmp[j] = X[j][index]   # (236,5,20)\n",
    "        return tmp\n",
    "_=rearrange(X, 18, 0)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_transform_column(X, scaler, column_number):\n",
    "    # 获取原始数据的均值和标准差\n",
    "    mean = scaler.mean_[column_number]  # 最后一列的均值\n",
    "    scale = scaler.scale_[column_number]  # 最后一列的标准差\n",
    "\n",
    "    # 创建一个副本,避免修改原始数据\n",
    "    X_inv = np.array(X).copy()\n",
    "\n",
    "    # 对最后一列进行反归一化\n",
    "    X_inv = X_inv * scale + mean\n",
    "\n",
    "    return X_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model,X:np.ndarray)->np.ndarray:\n",
    "    model.eval()\n",
    "    model.to('cpu')\n",
    "    # 预测收益率向量\n",
    "    X=X.astype(float)\n",
    "    #.reshape(18,5,20)\n",
    "    Input_tensor=torch.tensor(X,dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        R_pred = model(Input_tensor)\n",
    "        R_pred_inverse = inverse_transform_column(R_pred, scaler, -1)\n",
    "        R = R_pred_inverse.flatten()\n",
    "    return R\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n600519.SH 贵州茅台\\n*000858.SZ 五粮液 \\n002304.SZ 洋河股份\\n*603369.SH 今世缘\\n000568.SZ 泸州老窖\\n\\n600000.SH 浦发银行\\n601988.SH 中国银行\\n601398.SH 工商银行\\n601288.SH 农业银行\\n600919.SH 江苏银行\\n\\n603259.SH 药明康德\\n002737.SZ 葵花药业\\n600566.SH 济川药业\\n300015.SZ 爱尔眼科\\n600721.SH 百花医药\\n\\n300750.SZ 宁德时代\\n002455.SZ 百川股份\\n600104.SH 上汽集团\\n002594.SZ 比亚迪\\n601633.SH 长城汽车\\n\\ndef optimize():\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "600519.SH 贵州茅台\n",
    "*000858.SZ 五粮液 \n",
    "002304.SZ 洋河股份\n",
    "*603369.SH 今世缘\n",
    "000568.SZ 泸州老窖\n",
    "\n",
    "600000.SH 浦发银行\n",
    "601988.SH 中国银行\n",
    "601398.SH 工商银行\n",
    "601288.SH 农业银行\n",
    "600919.SH 江苏银行\n",
    "\n",
    "603259.SH 药明康德\n",
    "002737.SZ 葵花药业\n",
    "600566.SH 济川药业\n",
    "300015.SZ 爱尔眼科\n",
    "600721.SH 百花医药\n",
    "\n",
    "300750.SZ 宁德时代\n",
    "002455.SZ 百川股份\n",
    "600104.SH 上汽集团\n",
    "002594.SZ 比亚迪\n",
    "601633.SH 长城汽车\n",
    "\n",
    "def optimize():\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gurobipy import *\n",
    "\n",
    "path=\"128.pt\"\n",
    "model.load_state_dict(torch.load(path))\n",
    "# 定义模型参数\n",
    "n = 18 # 资产数量\n",
    "index=1 # window_slice_index\n",
    "\n",
    "p_previous=[]\n",
    "R_previous=[]\n",
    "p_real=[] # new add\n",
    "for i in instrument_list:\n",
    "    p1,R1=collect(data=data,instrument_code=i)\n",
    "    p_previous.append(p1[index:index+4])\n",
    "    R_previous.append(R1[index:index+4])\n",
    "    p_real.append(p1[index+4])\n",
    "\n",
    "p_real = np.stack(p_real)\n",
    "p_previous=np.stack(p_previous)\n",
    "R_previous=np.stack(R_previous)\n",
    "\n",
    "input=np.stack(list(_.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_p=predict(model, input) # X (18,5,20)-> R_p (18,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_full = np.hstack(( R_previous,R_p.reshape(18,1),)) # (18,) + (18,4) -> (19,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_4=[]\n",
    "for i in range(n):\n",
    "    p_4.append(p_previous[i][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_4=np.array(p_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.41101677e+04, 7.71709607e+02, 7.94909218e+03, 1.08226007e+02,\n",
       "       6.90227489e+00, 9.06619371e+00, 5.46713172e+00, 1.00885597e+01,\n",
       "       2.13936774e+02, 1.27178778e+02, 6.28937679e+01, 1.51208111e+03,\n",
       "       9.14822270e+00, 4.15749172e+02, 6.30258592e+01, 1.51380474e+02,\n",
       "       2.67775536e+02, 1.11956132e+02])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(R_p+1)*p_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cash_price=np.ones(5) # (19,5) + (1,5)\n",
    "p_full = np.hstack((p_previous,((R_p+1)*p_4).reshape(18,1)))\n",
    "\n",
    "#cash_rate=np.zeros(5)\n",
    "#R = np.concatenate((cash_rate,R_previous))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_full=R_full.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 18 # 资产数量\n",
    "Capital= 1e4 #本金\n",
    "p = p_full[:,-2] # 当前资产价格向量\n",
    "p_prime = p_full[:,-1] # 预测的资产价格向量\n",
    "R = R_p# 预测收益率向量\n",
    "c = 1e-4 # 交易成本率\n",
    "alpha = 0.5 # 风险容忍度\n",
    "Sigma = np.cov(R_full) # 资产收益率的协方差矩阵\n",
    "q_current = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0] # 当前的资产持有量向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_real.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter NumericFocus to value 3\n",
      "Gurobi Optimizer version 11.0.1 build v11.0.1rc0 (linux64 - \"Ubuntu 22.04.4 LTS\")\n",
      "\n",
      "CPU model: 13th Gen Intel(R) Core(TM) i7-13700K, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 24 physical cores, 24 logical processors, using up to 24 threads\n",
      "\n",
      "Optimize a model with 19 rows, 72 columns and 72 nonzeros\n",
      "Model fingerprint: 0xb6081f0d\n",
      "Model has 19 quadratic constraints\n",
      "Variable types: 54 continuous, 18 integer (0 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+04]\n",
      "  QMatrix range    [2e-07, 1e+04]\n",
      "  QLMatrix range   [5e+00, 1e+04]\n",
      "  Objective range  [5e-04, 1e+04]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+04, 1e+04]\n",
      "  QRHS range       [5e-01, 5e-01]\n",
      "Warning: Quadratic constraints contain large coefficient range\n",
      "Found heuristic solution: objective -0.0000000\n",
      "Presolve removed 18 rows and 38 columns\n",
      "Presolve time: 0.00s\n",
      "Presolved: 1242 rows, 340 columns, 3060 nonzeros\n",
      "Presolved model has 289 bilinear constraint(s)\n",
      "\n",
      "Solving non-convex MIQCP\n",
      "\n",
      "Variable types: 323 continuous, 17 integer (1 binary)\n",
      "Found heuristic solution: objective 223.8900815\n",
      "\n",
      "Root relaxation: objective 1.009304e+04, 11 iterations, 0.00 seconds (0.00 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0 10093.0393    0    5  223.89008 10093.0393  4408%     -    0s\n",
      "H    0     0                    10091.453416 10092.9716  0.02%     -    0s\n",
      "     0     0 10092.9716    0    1 10091.4534 10092.9716  0.02%     -    0s\n",
      "     0     0 10092.9258    0    1 10091.4534 10092.9258  0.01%     -    0s\n",
      "H    0     0                    10092.042792 10092.9134  0.01%     -    0s\n",
      "\n",
      "Explored 1 nodes (44 simplex iterations) in 0.04 seconds (0.02 work units)\n",
      "Thread count was 24 (of 24 available processors)\n",
      "\n",
      "Solution count 5: 10092 10091.5 10075.1 ... -0\n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 1.009204279220e+04, best bound 1.009291336619e+04, gap 0.0086%\n",
      "Optimal objective value: 10092.042792204264\n",
      "q[0]: 0.0\n",
      "q[1]: 0.0\n",
      "q[2]: 0.0\n",
      "q[3]: -0.0\n",
      "q[4]: 1.0\n",
      "q[5]: 0.0\n",
      "q[6]: 0.0\n",
      "q[7]: 1.0\n",
      "q[8]: -0.0\n",
      "q[9]: -0.0\n",
      "q[10]: -0.0\n",
      "q[11]: 0.0\n",
      "q[12]: 0.0\n",
      "q[13]: 0.0\n",
      "q[14]: -0.0\n",
      "q[15]: -0.0\n",
      "q[16]: 0.0\n",
      "q[17]: 90.0\n",
      "w[0]: 0.0\n",
      "w[1]: 0.0\n",
      "w[2]: 0.0\n",
      "w[3]: 0.0\n",
      "w[4]: 0.0006935498637268696\n",
      "w[5]: 0.0\n",
      "w[6]: 0.0\n",
      "w[7]: 0.0010181223430366908\n",
      "w[8]: 0.0\n",
      "w[9]: 0.0\n",
      "w[10]: 0.0\n",
      "w[11]: 0.0\n",
      "w[12]: 0.0\n",
      "w[13]: 0.0\n",
      "w[14]: 0.0\n",
      "w[15]: 0.0\n",
      "w[16]: 0.0\n",
      "w[17]: 0.9982883277932362\n",
      "buy[0]: 0.0\n",
      "buy[1]: 0.0\n",
      "buy[2]: 0.0\n",
      "buy[3]: 0.0\n",
      "buy[4]: 1.0\n",
      "buy[5]: 0.0\n",
      "buy[6]: 0.0\n",
      "buy[7]: 1.0\n",
      "buy[8]: 0.0\n",
      "buy[9]: 0.0\n",
      "buy[10]: 0.0\n",
      "buy[11]: 0.0\n",
      "buy[12]: 0.0\n",
      "buy[13]: 0.0\n",
      "buy[14]: 0.0\n",
      "buy[15]: 0.0\n",
      "buy[16]: 0.0\n",
      "buy[17]: 90.0\n",
      "sell[0]: 0.0\n",
      "sell[1]: 0.0\n",
      "sell[2]: 0.0\n",
      "sell[3]: 0.0\n",
      "sell[4]: 0.0\n",
      "sell[5]: 0.0\n",
      "sell[6]: 0.0\n",
      "sell[7]: 0.0\n",
      "sell[8]: 0.0\n",
      "sell[9]: 0.0\n",
      "sell[10]: 0.0\n",
      "sell[11]: 0.0\n",
      "sell[12]: 0.0\n",
      "sell[13]: 0.0\n",
      "sell[14]: 0.0\n",
      "sell[15]: 0.0\n",
      "sell[16]: 0.0\n",
      "sell[17]: 0.0\n"
     ]
    }
   ],
   "source": [
    "from gurobipy import GRB, quicksum\n",
    "\n",
    "# 创建优化模型\n",
    "m = Model(\"Portfolio Optimization\")\n",
    "# 设置 NumericFocus 参数提高数值稳定性\n",
    "m.Params.NumericFocus = 3\n",
    "# 定义决策变量\n",
    "q = m.addVars(n, lb=0, vtype=GRB.INTEGER, name=\"q\")\n",
    "#q[0].setAttr(\"vtype\", GRB.CONTINUOUS) # 除第一个元素外,其他元素为整数\n",
    "w = m.addVars(n, lb=0, ub=1, name=\"w\")\n",
    "buy = m.addVars(n, lb=0, name=\"buy\")\n",
    "sell = m.addVars(n, lb=0, name=\"sell\")\n",
    "\n",
    "# 设置目标函数\n",
    "obj = quicksum(q[i] * p_prime[i] for i in range(n)) - quicksum(c * (buy[i] + sell[i]) * p[i] for i in range(n))\n",
    "m.setObjective(obj, GRB.MAXIMIZE)\n",
    "\n",
    "# 添加约束条件\n",
    "m.addConstr(Capital >= quicksum(p[i] * q[i] for i in range(n)), \"budget\")\n",
    "m.addConstrs((quicksum(p[j] * q[j] for j in range(n))*w[i] == p[i] * q[i]   for i in range(n)), \"weights\")\n",
    "m.addConstr(\n",
    "    quicksum(Sigma[i][j] * w[i] * w[j] for i in range(n) for j in range(n)) <= alpha, \n",
    "    \"risk\"\n",
    ")\n",
    "m.addConstrs((q[i] == q_current[i] + buy[i] - sell[i] for i in range(n)), \"balance\")\n",
    "\n",
    "# 求解模型\n",
    "m.optimize()\n",
    "\n",
    "# 输出结果\n",
    "if m.status == GRB.OPTIMAL:\n",
    "    print(f\"Optimal objective value: {m.objVal}\")\n",
    "    for v in m.getVars():\n",
    "        print(f\"{v.varName}: {v.x}\")\n",
    "else:\n",
    "    print(\"Optimization failed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "sell=[var.X for var in sell.values()]\n",
    "print(sell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 90.0]\n"
     ]
    }
   ],
   "source": [
    "buy=[var.X for var in buy.values()]\n",
    "print(buy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, -0.0, 1.0, 0.0, 0.0, 1.0, -0.0, -0.0, -0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 90.0]\n"
     ]
    }
   ],
   "source": [
    "q=[var.X for var in q.values()]\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sell @ p_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.999921523255706"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost = c*((buy @ p) + (sell @ p))\n",
    "cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.21515408031622318\n"
     ]
    }
   ],
   "source": [
    "Capital_remain = Capital -(buy @ p) + (sell @ p) - cost\n",
    "print(Capital_remain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357.1131934083304\n"
     ]
    }
   ],
   "source": [
    "real_Profit = q @ p_real - Capital - cost\n",
    "print(real_Profit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost+Capital_remain+(buy @ p) - (sell @ p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
