{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# 定义Add & Norm层\n",
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(AddNorm, self).__init__()\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "    def forward(self, x, residual):\n",
    "        return self.norm(x + residual)\n",
    "\n",
    "# 定义Feed Forward层\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, hidden_dim, ff_dim):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(hidden_dim, ff_dim)\n",
    "        self.linear2 = nn.Linear(ff_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "# 定义Multi-Head Attention层\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "\n",
    "        self.query = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.key = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.value = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        Q = self.query(query).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.key(key).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.value(value).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        energy = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.head_dim)\n",
    "\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "\n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "\n",
    "        x = torch.matmul(attention, V).transpose(1, 2).contiguous().view(batch_size, -1, self.hidden_dim)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# 定义Encoder层\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads, ff_dim):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(hidden_dim, num_heads)\n",
    "        self.ff = FeedForward(hidden_dim, ff_dim)\n",
    "        self.norm1 = AddNorm(hidden_dim)\n",
    "        self.norm2 = AddNorm(hidden_dim)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        residual = x\n",
    "        x = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x, residual)\n",
    "\n",
    "        residual = x\n",
    "        x = self.ff(x)\n",
    "        x = self.norm2(x, residual)\n",
    "\n",
    "        return x\n",
    "\n",
    "# 定义Decoder层\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads, ff_dim):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(hidden_dim, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(hidden_dim, num_heads)\n",
    "        self.ff = FeedForward(hidden_dim, ff_dim)\n",
    "        self.norm1 = AddNorm(hidden_dim)\n",
    "        self.norm2 = AddNorm(hidden_dim)\n",
    "        self.norm3 = AddNorm(hidden_dim)\n",
    "\n",
    "    def forward(self, x, enc_outputs, src_mask=None, tgt_mask=None):\n",
    "        residual = x\n",
    "        x = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x, residual)\n",
    "\n",
    "        residual = x\n",
    "        x = self.cross_attn(x, enc_outputs, enc_outputs, src_mask)\n",
    "        x = self.norm2(x, residual)\n",
    "\n",
    "        residual = x\n",
    "        x = self.ff(x)\n",
    "        x = self.norm3(x, residual)\n",
    "\n",
    "        return x\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, hidden_dim, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, hidden_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, hidden_dim, 2).float() * (-np.log(10000.0) / hidden_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "# 定义Transformer模型\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_heads, num_layers, ff_dim):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.input_embedding = nn.Linear(input_dim, hidden_dim)\n",
    "        self.pos_encoding = PositionalEncoding(hidden_dim)\n",
    "        self.output_embedding = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(hidden_dim, num_heads, ff_dim) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(hidden_dim, num_heads, ff_dim) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, src, tgt_mask=None):\n",
    "        src_embed = self.input_embedding(src)\n",
    "        src_embed = self.pos_encoding(src_embed)\n",
    "\n",
    "        enc_outputs = src_embed\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_outputs = enc_layer(enc_outputs)\n",
    "\n",
    "        dec_outputs = enc_outputs\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_outputs = dec_layer(dec_outputs, enc_outputs, tgt_mask=tgt_mask)\n",
    "\n",
    "        outputs = self.output_embedding(dec_outputs[:, -1, :])\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass Transformer(nn.Module):\\n    def __init__(self, input_dim, hidden_dim, output_dim, num_heads, num_layers, ff_dim):\\n        super(Transformer, self).__init__()\\n        self.input_embedding = nn.Linear(input_dim, hidden_dim)\\n        self.output_embedding = nn.Linear(hidden_dim, output_dim)\\n\\n        self.encoder_layers = nn.ModuleList([EncoderLayer(hidden_dim, num_heads, ff_dim) for _ in range(num_layers)])\\n        self.decoder_layers = nn.ModuleList([DecoderLayer(hidden_dim, num_heads, ff_dim) for _ in range(num_layers)])\\n\\n    def forward(self, src, tgt_mask=None):\\n        src_embed = self.input_embedding(src)\\n\\n        enc_outputs = src_embed\\n        for enc_layer in self.encoder_layers:\\n            enc_outputs = enc_layer(enc_outputs)\\n\\n        dec_outputs = enc_outputs\\n        for dec_layer in self.decoder_layers:\\n            dec_outputs = dec_layer(dec_outputs, enc_outputs, tgt_mask=tgt_mask)\\n\\n        outputs = self.output_embedding(dec_outputs[:, -1, :])\\n        return outputs\\n        '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义Transformer模型\n",
    "'''\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_heads, num_layers, ff_dim):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.input_embedding = nn.Linear(input_dim, hidden_dim)\n",
    "        self.output_embedding = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(hidden_dim, num_heads, ff_dim) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(hidden_dim, num_heads, ff_dim) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, src, tgt_mask=None):\n",
    "        src_embed = self.input_embedding(src)\n",
    "\n",
    "        enc_outputs = src_embed\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_outputs = enc_layer(enc_outputs)\n",
    "\n",
    "        dec_outputs = enc_outputs\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_outputs = dec_layer(dec_outputs, enc_outputs, tgt_mask=tgt_mask)\n",
    "\n",
    "        outputs = self.output_embedding(dec_outputs[:, -1, :])\n",
    "        return outputs\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查是否有可用的GPU\n",
    "#device = torch.device('cpu')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (input_embedding): Linear(in_features=19, out_features=128, bias=True)\n",
       "  (pos_encoding): PositionalEncoding()\n",
       "  (output_embedding): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (encoder_layers): ModuleList(\n",
       "    (0): EncoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): EncoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (2): EncoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (3): EncoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (4): EncoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (5): EncoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder_layers): ModuleList(\n",
       "    (0): DecoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (cross_attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm3): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): DecoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (cross_attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm3): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (2): DecoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (cross_attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm3): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (3): DecoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (cross_attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm3): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (4): DecoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (cross_attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm3): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (5): DecoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (cross_attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm3): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 设置模型参数\n",
    "input_dim = 19\n",
    "hidden_dim = 128\n",
    "output_dim = 1\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "ff_dim = 512\n",
    "\n",
    "# 创建模型实例\n",
    "model = Transformer(input_dim, hidden_dim, output_dim, num_heads, num_layers, ff_dim)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('tmp2.csv')\n",
    "#data_stock=data[data.name=='万科A']\n",
    "\n",
    "features = ['company_id','date','open', 'close', 'high', 'low', 'volume', 'money_netflow', 'money_inflow', 'money_outflow',\n",
    "       'net_inflow_rate', 'list_sector', 'CPI', '无风险利率',\n",
    "       'total_market_cap', 'float_market_cap', 'pe_ttm', 'pb',\n",
    "       'dividend_yield_ratio', 'major_id', 'minor_id',\n",
    "       'change_ratio']\n",
    "data = data[features]\n",
    "#data=data.iloc[0:10000,:]\n",
    "#data=data[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\d2l\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scale_need_cols = [ 'open', 'close', 'high', 'low', 'volume', 'money_netflow', 'money_inflow', 'money_outflow',\n",
    "       'net_inflow_rate', 'CPI', '无风险利率',\n",
    "       'total_market_cap', 'float_market_cap', 'pe_ttm', 'pb',\n",
    "       'dividend_yield_ratio',\n",
    "       'change_ratio']\n",
    "# 创建MinMaxScaler对象\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data.loc[:, scale_need_cols] = scaler.fit_transform(data[scale_need_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_dataset(data, look_back=5):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(data) - look_back - 1):\n",
    "        X.append(data[i:(i + look_back), :-1])\n",
    "        Y.append(data[i + look_back, -1])\n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_back = 5\n",
    "X, Y = create_dataset(data.values, look_back)\n",
    "#X, Y = create_dataset(data.values, look_back)\n",
    "# 划分训练集和测试集\n",
    "train_size = int(len(X) * 0.7)\n",
    "X_train, Y_train = X[:train_size], Y[:train_size]\n",
    "X_test, Y_test = X[train_size:], Y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom transformers import BertTokenizer\\ndef tokenize_and_combine(data):\\n    text_data = data[:, :, 0]\\n    numeric_data = data[:, :, 1:].astype(float)\\n    \\n    # 创建中文BERT tokenizer\\n    tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\\n    \\n    # 对文本部分进行分词和编码\\n    encoded_text = []\\n    max_length = 10  # 指定一个固定的最大长度\\n    for text_batch in text_data:\\n        encoded_batch = tokenizer.batch_encode_plus(\\n            text_batch.tolist(),\\n            add_special_tokens=False,\\n            padding='max_length',\\n            truncation=True,\\n            max_length=max_length,\\n            return_tensors='pt'\\n        )\\n        encoded_text.append(encoded_batch['input_ids'])\\n    \\n    encoded_text = torch.stack(encoded_text)\\n    \\n    # 将数值部分转换为PyTorch张量\\n    numeric_tensor = torch.tensor(numeric_data, dtype=torch.float)\\n    \\n    # 组合编码后的文本部分和数值部分\\n    combined_tensor = torch.cat((encoded_text, numeric_tensor), dim=-1)\\n    \\n    return combined_tensor\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from transformers import BertTokenizer\n",
    "def tokenize_and_combine(data):\n",
    "    text_data = data[:, :, 0]\n",
    "    numeric_data = data[:, :, 1:].astype(float)\n",
    "    \n",
    "    # 创建中文BERT tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "    \n",
    "    # 对文本部分进行分词和编码\n",
    "    encoded_text = []\n",
    "    max_length = 10  # 指定一个固定的最大长度\n",
    "    for text_batch in text_data:\n",
    "        encoded_batch = tokenizer.batch_encode_plus(\n",
    "            text_batch.tolist(),\n",
    "            add_special_tokens=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        encoded_text.append(encoded_batch['input_ids'])\n",
    "    \n",
    "    encoded_text = torch.stack(encoded_text)\n",
    "    \n",
    "    # 将数值部分转换为PyTorch张量\n",
    "    numeric_tensor = torch.tensor(numeric_data, dtype=torch.float)\n",
    "    \n",
    "    # 组合编码后的文本部分和数值部分\n",
    "    combined_tensor = torch.cat((encoded_text, numeric_tensor), dim=-1)\n",
    "    \n",
    "    return combined_tensor\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train=tokenize_and_combine(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_test=tokenize_and_combine(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def directional_accuracy(actual, predicted):\n",
    "    actual_diff = np.diff(actual)\n",
    "    predicted_diff = np.diff(predicted)\n",
    "    return np.mean(np.sign(actual_diff) == np.sign(predicted_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Train Loss: 0.07819897, Train MAE: 0.24305756, Train RMSE: 0.27964076, Train DA: 0.50000000, Val Loss: 0.07329141, Val MAE: 0.24483667, Val RMSE: 0.27072385, Val DA: 0.54838710\n",
      "Epoch [2/1000], Train Loss: 0.04677438, Train MAE: 0.19293179, Train RMSE: 0.21627386, Train DA: 0.50000000, Val Loss: 0.04190959, Val MAE: 0.18396868, Val RMSE: 0.20471834, Val DA: 0.46774194\n",
      "Epoch [3/1000], Train Loss: 0.08534148, Train MAE: 0.25687343, Train RMSE: 0.29213265, Train DA: 0.45890411, Val Loss: 0.07839619, Val MAE: 0.25503129, Val RMSE: 0.27999321, Val DA: 0.59677419\n",
      "Epoch [4/1000], Train Loss: 0.04834809, Train MAE: 0.19663225, Train RMSE: 0.21988198, Train DA: 0.47260274, Val Loss: 0.04352744, Val MAE: 0.18827240, Val RMSE: 0.20863229, Val DA: 0.50000000\n",
      "Epoch [5/1000], Train Loss: 0.02037616, Train MAE: 0.09984462, Train RMSE: 0.14274511, Train DA: 0.47945205, Val Loss: 0.01458815, Val MAE: 0.08120679, Val RMSE: 0.12078143, Val DA: 0.54838710\n",
      "Epoch [6/1000], Train Loss: 0.02674598, Train MAE: 0.11554212, Train RMSE: 0.16354197, Train DA: 0.45205479, Val Loss: 0.02065463, Val MAE: 0.10452425, Val RMSE: 0.14371718, Val DA: 0.50000000\n",
      "Epoch [7/1000], Train Loss: 0.02725733, Train MAE: 0.13610183, Train RMSE: 0.16509792, Train DA: 0.47945205, Val Loss: 0.02201970, Val MAE: 0.12108654, Val RMSE: 0.14839037, Val DA: 0.51612903\n",
      "Epoch [8/1000], Train Loss: 0.02120345, Train MAE: 0.10138464, Train RMSE: 0.14561404, Train DA: 0.45890411, Val Loss: 0.01534411, Val MAE: 0.08359043, Val RMSE: 0.12387134, Val DA: 0.53225806\n",
      "Epoch [9/1000], Train Loss: 0.02012948, Train MAE: 0.09947544, Train RMSE: 0.14187838, Train DA: 0.45890411, Val Loss: 0.01436856, Val MAE: 0.08050147, Val RMSE: 0.11986892, Val DA: 0.51612903\n",
      "Epoch [10/1000], Train Loss: 0.02109799, Train MAE: 0.10965782, Train RMSE: 0.14525148, Train DA: 0.46575342, Val Loss: 0.01560570, Val MAE: 0.09206381, Val RMSE: 0.12492279, Val DA: 0.50000000\n",
      "Epoch [11/1000], Train Loss: 0.02084068, Train MAE: 0.10058891, Train RMSE: 0.14436302, Train DA: 0.45890411, Val Loss: 0.01500765, Val MAE: 0.08244026, Val RMSE: 0.12250570, Val DA: 0.51612903\n",
      "Epoch [12/1000], Train Loss: 0.01995980, Train MAE: 0.10211973, Train RMSE: 0.14127913, Train DA: 0.46575342, Val Loss: 0.01434747, Val MAE: 0.08340178, Val RMSE: 0.11978094, Val DA: 0.51612903\n",
      "Epoch [13/1000], Train Loss: 0.01981883, Train MAE: 0.10029654, Train RMSE: 0.14077936, Train DA: 0.45890411, Val Loss: 0.01415247, Val MAE: 0.08123253, Val RMSE: 0.11896417, Val DA: 0.51612903\n",
      "Epoch [14/1000], Train Loss: 0.01993943, Train MAE: 0.09938594, Train RMSE: 0.14120707, Train DA: 0.46575342, Val Loss: 0.01421151, Val MAE: 0.08035304, Val RMSE: 0.11921204, Val DA: 0.51612903\n",
      "Epoch [15/1000], Train Loss: 0.01992719, Train MAE: 0.10183280, Train RMSE: 0.14116371, Train DA: 0.46575342, Val Loss: 0.01430745, Val MAE: 0.08296409, Val RMSE: 0.11961375, Val DA: 0.51612903\n",
      "Epoch [16/1000], Train Loss: 0.01990466, Train MAE: 0.09942067, Train RMSE: 0.14108388, Train DA: 0.46575342, Val Loss: 0.01418522, Val MAE: 0.08038065, Val RMSE: 0.11910172, Val DA: 0.51612903\n",
      "Epoch [17/1000], Train Loss: 0.01983689, Train MAE: 0.10078087, Train RMSE: 0.14084347, Train DA: 0.47260274, Val Loss: 0.01418657, Val MAE: 0.08169062, Val RMSE: 0.11910739, Val DA: 0.53225806\n",
      "Epoch [18/1000], Train Loss: 0.01982228, Train MAE: 0.09989633, Train RMSE: 0.14079164, Train DA: 0.47260274, Val Loss: 0.01413862, Val MAE: 0.08085608, Val RMSE: 0.11890593, Val DA: 0.53225806\n",
      "Epoch [19/1000], Train Loss: 0.01981783, Train MAE: 0.10007560, Train RMSE: 0.14077583, Train DA: 0.47945205, Val Loss: 0.01414193, Val MAE: 0.08102857, Val RMSE: 0.11891983, Val DA: 0.53225806\n",
      "Epoch [20/1000], Train Loss: 0.01981899, Train MAE: 0.10030595, Train RMSE: 0.14077993, Train DA: 0.47260274, Val Loss: 0.01415271, Val MAE: 0.08124241, Val RMSE: 0.11896516, Val DA: 0.53225806\n",
      "Epoch [21/1000], Train Loss: 0.01982095, Train MAE: 0.09993399, Train RMSE: 0.14078690, Train DA: 0.47260274, Val Loss: 0.01413878, Val MAE: 0.08089308, Val RMSE: 0.11890660, Val DA: 0.53225806\n",
      "Epoch [22/1000], Train Loss: 0.01982062, Train MAE: 0.10037851, Train RMSE: 0.14078574, Train DA: 0.47945205, Val Loss: 0.01415693, Val MAE: 0.08131872, Val RMSE: 0.11898292, Val DA: 0.51612903\n",
      "Epoch [23/1000], Train Loss: 0.01981910, Train MAE: 0.10000066, Train RMSE: 0.14078033, Train DA: 0.47945205, Val Loss: 0.01413974, Val MAE: 0.08095767, Val RMSE: 0.11891063, Val DA: 0.53225806\n",
      "Epoch [24/1000], Train Loss: 0.01981947, Train MAE: 0.10032965, Train RMSE: 0.14078166, Train DA: 0.47260274, Val Loss: 0.01415392, Val MAE: 0.08126914, Val RMSE: 0.11897024, Val DA: 0.53225806\n",
      "Epoch [25/1000], Train Loss: 0.01981758, Train MAE: 0.10009933, Train RMSE: 0.14077491, Train DA: 0.47260274, Val Loss: 0.01414240, Val MAE: 0.08105295, Val RMSE: 0.11892182, Val DA: 0.53225806\n",
      "Epoch [26/1000], Train Loss: 0.01981867, Train MAE: 0.10029037, Train RMSE: 0.14077879, Train DA: 0.47260274, Val Loss: 0.01415148, Val MAE: 0.08123058, Val RMSE: 0.11895998, Val DA: 0.53225806\n",
      "Epoch [27/1000], Train Loss: 0.01981739, Train MAE: 0.10017548, Train RMSE: 0.14077425, Train DA: 0.47260274, Val Loss: 0.01414542, Val MAE: 0.08112667, Val RMSE: 0.11893453, Val DA: 0.53225806\n",
      "Epoch [28/1000], Train Loss: 0.01981846, Train MAE: 0.10027964, Train RMSE: 0.14077805, Train DA: 0.47260274, Val Loss: 0.01415075, Val MAE: 0.08122199, Val RMSE: 0.11895692, Val DA: 0.54838710\n",
      "Epoch [29/1000], Train Loss: 0.01981774, Train MAE: 0.10022794, Train RMSE: 0.14077550, Train DA: 0.47260274, Val Loss: 0.01414796, Val MAE: 0.08117770, Val RMSE: 0.11894519, Val DA: 0.54838710\n",
      "Epoch [30/1000], Train Loss: 0.01981860, Train MAE: 0.10028961, Train RMSE: 0.14077856, Train DA: 0.47260274, Val Loss: 0.01415119, Val MAE: 0.08123120, Val RMSE: 0.11895878, Val DA: 0.54838710\n",
      "Epoch [31/1000], Train Loss: 0.01981827, Train MAE: 0.10027060, Train RMSE: 0.14077739, Train DA: 0.47260274, Val Loss: 0.01415008, Val MAE: 0.08121527, Val RMSE: 0.11895408, Val DA: 0.54838710\n",
      "Epoch [32/1000], Train Loss: 0.01981889, Train MAE: 0.10031000, Train RMSE: 0.14077958, Train DA: 0.47945205, Val Loss: 0.01415218, Val MAE: 0.08125088, Val RMSE: 0.11896292, Val DA: 0.54838710\n",
      "Epoch [33/1000], Train Loss: 0.01981879, Train MAE: 0.10030573, Train RMSE: 0.14077921, Train DA: 0.47945205, Val Loss: 0.01415188, Val MAE: 0.08124673, Val RMSE: 0.11896169, Val DA: 0.54838710\n",
      "Epoch [34/1000], Train Loss: 0.01981942, Train MAE: 0.10033526, Train RMSE: 0.14078145, Train DA: 0.48630137, Val Loss: 0.01415362, Val MAE: 0.08127987, Val RMSE: 0.11896899, Val DA: 0.54838710\n",
      "Epoch [35/1000], Train Loss: 0.01981939, Train MAE: 0.10033742, Train RMSE: 0.14078136, Train DA: 0.48630137, Val Loss: 0.01415365, Val MAE: 0.08128302, Val RMSE: 0.11896911, Val DA: 0.54838710\n",
      "Epoch [36/1000], Train Loss: 0.01981996, Train MAE: 0.10036451, Train RMSE: 0.14078335, Train DA: 0.50000000, Val Loss: 0.01415515, Val MAE: 0.08131099, Val RMSE: 0.11897542, Val DA: 0.54838710\n",
      "Epoch [37/1000], Train Loss: 0.01982007, Train MAE: 0.10037085, Train RMSE: 0.14078377, Train DA: 0.49315068, Val Loss: 0.01415545, Val MAE: 0.08131798, Val RMSE: 0.11897669, Val DA: 0.56451613\n",
      "Epoch [38/1000], Train Loss: 0.01982057, Train MAE: 0.10039473, Train RMSE: 0.14078557, Train DA: 0.50000000, Val Loss: 0.01415677, Val MAE: 0.08134273, Val RMSE: 0.11898223, Val DA: 0.56451613\n",
      "Epoch [39/1000], Train Loss: 0.01982082, Train MAE: 0.10040407, Train RMSE: 0.14078644, Train DA: 0.50000000, Val Loss: 0.01415732, Val MAE: 0.08135265, Val RMSE: 0.11898453, Val DA: 0.54838710\n",
      "Epoch [40/1000], Train Loss: 0.01982140, Train MAE: 0.10042537, Train RMSE: 0.14078850, Train DA: 0.50000000, Val Loss: 0.01415861, Val MAE: 0.08137462, Val RMSE: 0.11898997, Val DA: 0.53225806\n",
      "Epoch [41/1000], Train Loss: 0.01982159, Train MAE: 0.10043655, Train RMSE: 0.14078915, Train DA: 0.50684932, Val Loss: 0.01415918, Val MAE: 0.08138692, Val RMSE: 0.11899235, Val DA: 0.51612903\n",
      "Epoch [42/1000], Train Loss: 0.01982210, Train MAE: 0.10045634, Train RMSE: 0.14079097, Train DA: 0.48630137, Val Loss: 0.01416035, Val MAE: 0.08140717, Val RMSE: 0.11899725, Val DA: 0.53225806\n",
      "Epoch [43/1000], Train Loss: 0.01982242, Train MAE: 0.10047052, Train RMSE: 0.14079212, Train DA: 0.50000000, Val Loss: 0.01416112, Val MAE: 0.08142120, Val RMSE: 0.11900051, Val DA: 0.54838710\n",
      "Epoch [44/1000], Train Loss: 0.01982282, Train MAE: 0.10048959, Train RMSE: 0.14079353, Train DA: 0.49315068, Val Loss: 0.01416202, Val MAE: 0.08143968, Val RMSE: 0.11900430, Val DA: 0.54838710\n",
      "Epoch [45/1000], Train Loss: 0.01982323, Train MAE: 0.10050626, Train RMSE: 0.14079501, Train DA: 0.49315068, Val Loss: 0.01416288, Val MAE: 0.08145563, Val RMSE: 0.11900789, Val DA: 0.54838710\n",
      "Epoch [46/1000], Train Loss: 0.01982374, Train MAE: 0.10052387, Train RMSE: 0.14079681, Train DA: 0.48630137, Val Loss: 0.01416391, Val MAE: 0.08147258, Val RMSE: 0.11901224, Val DA: 0.58064516\n",
      "Epoch [47/1000], Train Loss: 0.01982419, Train MAE: 0.10054119, Train RMSE: 0.14079839, Train DA: 0.50684932, Val Loss: 0.01416480, Val MAE: 0.08148921, Val RMSE: 0.11901595, Val DA: 0.56451613\n",
      "Epoch [48/1000], Train Loss: 0.01982465, Train MAE: 0.10055870, Train RMSE: 0.14080001, Train DA: 0.50000000, Val Loss: 0.01416572, Val MAE: 0.08150624, Val RMSE: 0.11901984, Val DA: 0.56451613\n",
      "Epoch [49/1000], Train Loss: 0.01982508, Train MAE: 0.10057569, Train RMSE: 0.14080156, Train DA: 0.51369863, Val Loss: 0.01416666, Val MAE: 0.08152304, Val RMSE: 0.11902378, Val DA: 0.58064516\n",
      "Epoch [50/1000], Train Loss: 0.01982561, Train MAE: 0.10059368, Train RMSE: 0.14080346, Train DA: 0.50000000, Val Loss: 0.01416782, Val MAE: 0.08154100, Val RMSE: 0.11902865, Val DA: 0.56451613\n",
      "Epoch [51/1000], Train Loss: 0.01982597, Train MAE: 0.10060988, Train RMSE: 0.14080472, Train DA: 0.50684932, Val Loss: 0.01416862, Val MAE: 0.08155720, Val RMSE: 0.11903201, Val DA: 0.56451613\n",
      "Epoch [52/1000], Train Loss: 0.01982658, Train MAE: 0.10063000, Train RMSE: 0.14080690, Train DA: 0.49315068, Val Loss: 0.01416978, Val MAE: 0.08157693, Val RMSE: 0.11903687, Val DA: 0.56451613\n",
      "Epoch [53/1000], Train Loss: 0.01982700, Train MAE: 0.10064518, Train RMSE: 0.14080837, Train DA: 0.50000000, Val Loss: 0.01417052, Val MAE: 0.08159225, Val RMSE: 0.11904000, Val DA: 0.56451613\n",
      "Epoch [54/1000], Train Loss: 0.01982742, Train MAE: 0.10066596, Train RMSE: 0.14080986, Train DA: 0.52739726, Val Loss: 0.01417131, Val MAE: 0.08161322, Val RMSE: 0.11904331, Val DA: 0.58064516\n",
      "Epoch [55/1000], Train Loss: 0.01982787, Train MAE: 0.10068431, Train RMSE: 0.14081147, Train DA: 0.52054795, Val Loss: 0.01417264, Val MAE: 0.08163337, Val RMSE: 0.11904889, Val DA: 0.56451613\n",
      "Epoch [56/1000], Train Loss: 0.01982815, Train MAE: 0.10070100, Train RMSE: 0.14081247, Train DA: 0.52739726, Val Loss: 0.01417309, Val MAE: 0.08165078, Val RMSE: 0.11905079, Val DA: 0.53225806\n",
      "Epoch [57/1000], Train Loss: 0.01982866, Train MAE: 0.10072515, Train RMSE: 0.14081427, Train DA: 0.53424658, Val Loss: 0.01417435, Val MAE: 0.08167709, Val RMSE: 0.11905607, Val DA: 0.53225806\n",
      "Epoch [58/1000], Train Loss: 0.01982890, Train MAE: 0.10074120, Train RMSE: 0.14081514, Train DA: 0.52054795, Val Loss: 0.01417506, Val MAE: 0.08169773, Val RMSE: 0.11905906, Val DA: 0.51612903\n",
      "Epoch [59/1000], Train Loss: 0.01982939, Train MAE: 0.10076655, Train RMSE: 0.14081685, Train DA: 0.53424658, Val Loss: 0.01417567, Val MAE: 0.08172640, Val RMSE: 0.11906164, Val DA: 0.51612903\n",
      "Epoch [60/1000], Train Loss: 0.01983007, Train MAE: 0.10078958, Train RMSE: 0.14081928, Train DA: 0.52739726, Val Loss: 0.01417715, Val MAE: 0.08175582, Val RMSE: 0.11906786, Val DA: 0.50000000\n",
      "Epoch [61/1000], Train Loss: 0.01983037, Train MAE: 0.10081128, Train RMSE: 0.14082035, Train DA: 0.54109589, Val Loss: 0.01417730, Val MAE: 0.08178493, Val RMSE: 0.11906846, Val DA: 0.53225806\n",
      "Epoch [62/1000], Train Loss: 0.01983147, Train MAE: 0.10084697, Train RMSE: 0.14082426, Train DA: 0.54109589, Val Loss: 0.01417858, Val MAE: 0.08182810, Val RMSE: 0.11907383, Val DA: 0.50000000\n",
      "Epoch [63/1000], Train Loss: 0.01983161, Train MAE: 0.10086973, Train RMSE: 0.14082474, Train DA: 0.54109589, Val Loss: 0.01417860, Val MAE: 0.08186362, Val RMSE: 0.11907392, Val DA: 0.51612903\n",
      "Epoch [64/1000], Train Loss: 0.01983265, Train MAE: 0.10091914, Train RMSE: 0.14082843, Train DA: 0.54109589, Val Loss: 0.01418034, Val MAE: 0.08193246, Val RMSE: 0.11908123, Val DA: 0.51612903\n",
      "Epoch [65/1000], Train Loss: 0.01983274, Train MAE: 0.10096347, Train RMSE: 0.14082876, Train DA: 0.55479452, Val Loss: 0.01417960, Val MAE: 0.08199862, Val RMSE: 0.11907814, Val DA: 0.51612903\n",
      "Epoch [66/1000], Train Loss: 0.01983380, Train MAE: 0.10104716, Train RMSE: 0.14083251, Train DA: 0.54794521, Val Loss: 0.01418154, Val MAE: 0.08212516, Val RMSE: 0.11908627, Val DA: 0.48387097\n",
      "Epoch [67/1000], Train Loss: 0.01983494, Train MAE: 0.10116100, Train RMSE: 0.14083655, Train DA: 0.53424658, Val Loss: 0.01418268, Val MAE: 0.08231375, Val RMSE: 0.11909105, Val DA: 0.53225806\n",
      "Epoch [68/1000], Train Loss: 0.01983936, Train MAE: 0.10143150, Train RMSE: 0.14085224, Train DA: 0.52739726, Val Loss: 0.01419250, Val MAE: 0.08279407, Val RMSE: 0.11913228, Val DA: 0.48387097\n",
      "Epoch [69/1000], Train Loss: 0.01988011, Train MAE: 0.10227597, Train RMSE: 0.14099686, Train DA: 0.52054795, Val Loss: 0.01428455, Val MAE: 0.08466160, Val RMSE: 0.11951800, Val DA: 0.48387097\n",
      "Epoch [70/1000], Train Loss: 0.02060910, Train MAE: 0.10794185, Train RMSE: 0.14355868, Train DA: 0.54109589, Val Loss: 0.01542147, Val MAE: 0.09296177, Val RMSE: 0.12418320, Val DA: 0.50000000\n",
      "Epoch [71/1000], Train Loss: 0.02169437, Train MAE: 0.11382478, Train RMSE: 0.14729008, Train DA: 0.54109589, Val Loss: 0.01659107, Val MAE: 0.09922243, Val RMSE: 0.12880632, Val DA: 0.50000000\n",
      "Epoch [72/1000], Train Loss: 0.02193581, Train MAE: 0.11501538, Train RMSE: 0.14810742, Train DA: 0.54109589, Val Loss: 0.01676036, Val MAE: 0.10004040, Val RMSE: 0.12946181, Val DA: 0.50000000\n",
      "Epoch [73/1000], Train Loss: 0.02103451, Train MAE: 0.11061999, Train RMSE: 0.14503279, Train DA: 0.52739726, Val Loss: 0.01612111, Val MAE: 0.09732673, Val RMSE: 0.12696892, Val DA: 0.48387097\n",
      "Epoch [74/1000], Train Loss: 0.02099195, Train MAE: 0.11035188, Train RMSE: 0.14488599, Train DA: 0.51369863, Val Loss: 0.01642683, Val MAE: 0.09879214, Val RMSE: 0.12816721, Val DA: 0.45161290\n",
      "Epoch [75/1000], Train Loss: 0.02236857, Train MAE: 0.11729740, Train RMSE: 0.14956127, Train DA: 0.53424658, Val Loss: 0.01741077, Val MAE: 0.10290553, Val RMSE: 0.13194989, Val DA: 0.46774194\n",
      "Epoch [76/1000], Train Loss: 0.02324714, Train MAE: 0.12115048, Train RMSE: 0.15247011, Train DA: 0.52054795, Val Loss: 0.01801216, Val MAE: 0.10501698, Val RMSE: 0.13420938, Val DA: 0.46774194\n",
      "Epoch [77/1000], Train Loss: 0.01998534, Train MAE: 0.10407902, Train RMSE: 0.14136952, Train DA: 0.53424658, Val Loss: 0.01557266, Val MAE: 0.09263831, Val RMSE: 0.12479047, Val DA: 0.46774194\n",
      "Epoch [78/1000], Train Loss: 0.02292372, Train MAE: 0.11996861, Train RMSE: 0.15140583, Train DA: 0.52739726, Val Loss: 0.01794848, Val MAE: 0.10528377, Val RMSE: 0.13397193, Val DA: 0.43548387\n",
      "Epoch [79/1000], Train Loss: 0.02211585, Train MAE: 0.11637570, Train RMSE: 0.14871399, Train DA: 0.54109589, Val Loss: 0.01742391, Val MAE: 0.10274411, Val RMSE: 0.13199966, Val DA: 0.45161290\n",
      "Epoch [80/1000], Train Loss: 0.02078790, Train MAE: 0.10933487, Train RMSE: 0.14418009, Train DA: 0.53424658, Val Loss: 0.01656340, Val MAE: 0.09919971, Val RMSE: 0.12869889, Val DA: 0.43548387\n",
      "Epoch [81/1000], Train Loss: 0.02204066, Train MAE: 0.11592972, Train RMSE: 0.14846097, Train DA: 0.52739726, Val Loss: 0.01749273, Val MAE: 0.10341528, Val RMSE: 0.13226010, Val DA: 0.43548387\n",
      "Epoch [82/1000], Train Loss: 0.02170836, Train MAE: 0.11444778, Train RMSE: 0.14733757, Train DA: 0.54109589, Val Loss: 0.01724275, Val MAE: 0.10200571, Val RMSE: 0.13131166, Val DA: 0.45161290\n",
      "Epoch [83/1000], Train Loss: 0.02195689, Train MAE: 0.11559180, Train RMSE: 0.14817858, Train DA: 0.53424658, Val Loss: 0.01750474, Val MAE: 0.10321868, Val RMSE: 0.13230549, Val DA: 0.45161290\n",
      "Epoch [84/1000], Train Loss: 0.02059193, Train MAE: 0.10769848, Train RMSE: 0.14349887, Train DA: 0.53424658, Val Loss: 0.01666533, Val MAE: 0.09923398, Val RMSE: 0.12909427, Val DA: 0.45161290\n",
      "Epoch [85/1000], Train Loss: 0.02288748, Train MAE: 0.11975744, Train RMSE: 0.15128610, Train DA: 0.53424658, Val Loss: 0.01828537, Val MAE: 0.10640989, Val RMSE: 0.13522343, Val DA: 0.45161290\n",
      "Epoch [86/1000], Train Loss: 0.02043389, Train MAE: 0.10702521, Train RMSE: 0.14294715, Train DA: 0.52054795, Val Loss: 0.01642011, Val MAE: 0.09730925, Val RMSE: 0.12814099, Val DA: 0.45161290\n",
      "Epoch [87/1000], Train Loss: 0.02333392, Train MAE: 0.12107766, Train RMSE: 0.15275444, Train DA: 0.54109589, Val Loss: 0.01863641, Val MAE: 0.10819649, Val RMSE: 0.13651526, Val DA: 0.45161290\n",
      "Epoch [88/1000], Train Loss: 0.02029192, Train MAE: 0.10625048, Train RMSE: 0.14244972, Train DA: 0.52054795, Val Loss: 0.01607513, Val MAE: 0.09540210, Val RMSE: 0.12678775, Val DA: 0.46774194\n",
      "Epoch [89/1000], Train Loss: 0.02260127, Train MAE: 0.11796738, Train RMSE: 0.15033720, Train DA: 0.53424658, Val Loss: 0.01832084, Val MAE: 0.10724543, Val RMSE: 0.13535450, Val DA: 0.46774194\n",
      "Epoch [90/1000], Train Loss: 0.02143717, Train MAE: 0.11257982, Train RMSE: 0.14641438, Train DA: 0.52739726, Val Loss: 0.01747513, Val MAE: 0.10269213, Val RMSE: 0.13219354, Val DA: 0.46774194\n",
      "Epoch [91/1000], Train Loss: 0.02167244, Train MAE: 0.11324741, Train RMSE: 0.14721562, Train DA: 0.52739726, Val Loss: 0.01774830, Val MAE: 0.10457735, Val RMSE: 0.13322274, Val DA: 0.46774194\n",
      "Epoch [92/1000], Train Loss: 0.02168765, Train MAE: 0.11319395, Train RMSE: 0.14726728, Train DA: 0.54109589, Val Loss: 0.01770510, Val MAE: 0.10435361, Val RMSE: 0.13306053, Val DA: 0.46774194\n",
      "Epoch [93/1000], Train Loss: 0.02192603, Train MAE: 0.11429276, Train RMSE: 0.14807442, Train DA: 0.53424658, Val Loss: 0.01803744, Val MAE: 0.10576972, Val RMSE: 0.13430355, Val DA: 0.46774194\n",
      "Epoch [94/1000], Train Loss: 0.02187223, Train MAE: 0.11392683, Train RMSE: 0.14789262, Train DA: 0.54109589, Val Loss: 0.01811988, Val MAE: 0.10614429, Val RMSE: 0.13461013, Val DA: 0.46774194\n",
      "Epoch [95/1000], Train Loss: 0.02119541, Train MAE: 0.11032563, Train RMSE: 0.14558645, Train DA: 0.54109589, Val Loss: 0.01775289, Val MAE: 0.10406749, Val RMSE: 0.13323998, Val DA: 0.46774194\n",
      "Epoch [96/1000], Train Loss: 0.02169503, Train MAE: 0.11305980, Train RMSE: 0.14729233, Train DA: 0.54109589, Val Loss: 0.01814655, Val MAE: 0.10626476, Val RMSE: 0.13470916, Val DA: 0.46774194\n",
      "Epoch [97/1000], Train Loss: 0.02163512, Train MAE: 0.11260117, Train RMSE: 0.14708883, Train DA: 0.54109589, Val Loss: 0.01809529, Val MAE: 0.10591940, Val RMSE: 0.13451874, Val DA: 0.46774194\n",
      "Epoch [98/1000], Train Loss: 0.02249240, Train MAE: 0.11694758, Train RMSE: 0.14997464, Train DA: 0.54794521, Val Loss: 0.01870541, Val MAE: 0.10869174, Val RMSE: 0.13676772, Val DA: 0.46774194\n",
      "Epoch [99/1000], Train Loss: 0.02159693, Train MAE: 0.11264125, Train RMSE: 0.14695895, Train DA: 0.53424658, Val Loss: 0.01810602, Val MAE: 0.10578153, Val RMSE: 0.13455860, Val DA: 0.46774194\n",
      "Epoch [100/1000], Train Loss: 0.02168884, Train MAE: 0.11318476, Train RMSE: 0.14727132, Train DA: 0.54109589, Val Loss: 0.01827699, Val MAE: 0.10631748, Val RMSE: 0.13519242, Val DA: 0.46774194\n",
      "Epoch [101/1000], Train Loss: 0.02145324, Train MAE: 0.11174665, Train RMSE: 0.14646924, Train DA: 0.54109589, Val Loss: 0.01844968, Val MAE: 0.10640211, Val RMSE: 0.13582958, Val DA: 0.45161290\n",
      "Epoch [102/1000], Train Loss: 0.02113813, Train MAE: 0.11020321, Train RMSE: 0.14538957, Train DA: 0.54109589, Val Loss: 0.01820029, Val MAE: 0.10550715, Val RMSE: 0.13490847, Val DA: 0.45161290\n",
      "Epoch [103/1000], Train Loss: 0.02164449, Train MAE: 0.11284664, Train RMSE: 0.14712065, Train DA: 0.54794521, Val Loss: 0.01834027, Val MAE: 0.10652535, Val RMSE: 0.13542624, Val DA: 0.46774194\n",
      "Epoch [104/1000], Train Loss: 0.02245441, Train MAE: 0.11695669, Train RMSE: 0.14984795, Train DA: 0.56164384, Val Loss: 0.01894657, Val MAE: 0.10962444, Val RMSE: 0.13764651, Val DA: 0.46774194\n",
      "Epoch [105/1000], Train Loss: 0.02263672, Train MAE: 0.11793289, Train RMSE: 0.15045506, Train DA: 0.54794521, Val Loss: 0.01903508, Val MAE: 0.10977890, Val RMSE: 0.13796771, Val DA: 0.43548387\n",
      "Epoch [106/1000], Train Loss: 0.02115982, Train MAE: 0.11043969, Train RMSE: 0.14546415, Train DA: 0.54794521, Val Loss: 0.01779762, Val MAE: 0.10360290, Val RMSE: 0.13340771, Val DA: 0.45161290\n",
      "Epoch [107/1000], Train Loss: 0.02032249, Train MAE: 0.09968228, Train RMSE: 0.14255697, Train DA: 0.53424658, Val Loss: 0.01646409, Val MAE: 0.08812768, Val RMSE: 0.12831247, Val DA: 0.50000000\n",
      "Epoch [108/1000], Train Loss: 0.02627404, Train MAE: 0.13321891, Train RMSE: 0.16209267, Train DA: 0.50000000, Val Loss: 0.02168019, Val MAE: 0.11937704, Val RMSE: 0.14724195, Val DA: 0.51612903\n",
      "Epoch [109/1000], Train Loss: 0.02299330, Train MAE: 0.11970924, Train RMSE: 0.15163539, Train DA: 0.53424658, Val Loss: 0.01797563, Val MAE: 0.10396191, Val RMSE: 0.13407324, Val DA: 0.48387097\n",
      "Epoch [110/1000], Train Loss: 0.01979129, Train MAE: 0.09998395, Train RMSE: 0.14068154, Train DA: 0.54109589, Val Loss: 0.01475521, Val MAE: 0.08311079, Val RMSE: 0.12147103, Val DA: 0.48387097\n",
      "Epoch [111/1000], Train Loss: 0.02089933, Train MAE: 0.10905254, Train RMSE: 0.14456600, Train DA: 0.56164384, Val Loss: 0.01566014, Val MAE: 0.09298031, Val RMSE: 0.12514046, Val DA: 0.48387097\n",
      "Epoch [112/1000], Train Loss: 0.02157415, Train MAE: 0.11298238, Train RMSE: 0.14688143, Train DA: 0.57534247, Val Loss: 0.01654852, Val MAE: 0.09747335, Val RMSE: 0.12864104, Val DA: 0.48387097\n",
      "Epoch [113/1000], Train Loss: 0.02030073, Train MAE: 0.10629644, Train RMSE: 0.14248061, Train DA: 0.56849315, Val Loss: 0.01591746, Val MAE: 0.09368922, Val RMSE: 0.12616442, Val DA: 0.48387097\n",
      "Epoch [114/1000], Train Loss: 0.02142551, Train MAE: 0.11258803, Train RMSE: 0.14637457, Train DA: 0.56164384, Val Loss: 0.01689988, Val MAE: 0.09902262, Val RMSE: 0.12999953, Val DA: 0.51612903\n",
      "Epoch [115/1000], Train Loss: 0.02179140, Train MAE: 0.11417403, Train RMSE: 0.14761910, Train DA: 0.55479452, Val Loss: 0.01713157, Val MAE: 0.09982777, Val RMSE: 0.13088764, Val DA: 0.51612903\n",
      "Epoch [116/1000], Train Loss: 0.02083227, Train MAE: 0.10922836, Train RMSE: 0.14433388, Train DA: 0.55479452, Val Loss: 0.01636733, Val MAE: 0.09576991, Val RMSE: 0.12793486, Val DA: 0.51612903\n",
      "Epoch [117/1000], Train Loss: 0.02119191, Train MAE: 0.11115501, Train RMSE: 0.14557442, Train DA: 0.55479452, Val Loss: 0.01672896, Val MAE: 0.09767947, Val RMSE: 0.12934047, Val DA: 0.51612903\n",
      "Epoch [118/1000], Train Loss: 0.02140639, Train MAE: 0.11227606, Train RMSE: 0.14630923, Train DA: 0.54794521, Val Loss: 0.01696846, Val MAE: 0.09879717, Val RMSE: 0.13026303, Val DA: 0.50000000\n",
      "Epoch [119/1000], Train Loss: 0.02098161, Train MAE: 0.11003466, Train RMSE: 0.14485031, Train DA: 0.54794521, Val Loss: 0.01664718, Val MAE: 0.09700932, Val RMSE: 0.12902397, Val DA: 0.51612903\n",
      "Epoch [120/1000], Train Loss: 0.02105318, Train MAE: 0.11039179, Train RMSE: 0.14509714, Train DA: 0.54109589, Val Loss: 0.01671505, Val MAE: 0.09743706, Val RMSE: 0.12928669, Val DA: 0.50000000\n",
      "Epoch [121/1000], Train Loss: 0.02123107, Train MAE: 0.11134547, Train RMSE: 0.14570887, Train DA: 0.53424658, Val Loss: 0.01691095, Val MAE: 0.09845997, Val RMSE: 0.13004214, Val DA: 0.50000000\n",
      "Epoch [122/1000], Train Loss: 0.02111279, Train MAE: 0.11072703, Train RMSE: 0.14530241, Train DA: 0.53424658, Val Loss: 0.01689036, Val MAE: 0.09822798, Val RMSE: 0.12996291, Val DA: 0.50000000\n",
      "Epoch [123/1000], Train Loss: 0.02111808, Train MAE: 0.11076024, Train RMSE: 0.14532062, Train DA: 0.55479452, Val Loss: 0.01694272, Val MAE: 0.09842528, Val RMSE: 0.13016421, Val DA: 0.50000000\n",
      "Epoch [124/1000], Train Loss: 0.02120310, Train MAE: 0.11120408, Train RMSE: 0.14561285, Train DA: 0.55479452, Val Loss: 0.01705107, Val MAE: 0.09894309, Val RMSE: 0.13057974, Val DA: 0.50000000\n",
      "Epoch [125/1000], Train Loss: 0.02118483, Train MAE: 0.11110397, Train RMSE: 0.14555009, Train DA: 0.55479452, Val Loss: 0.01710955, Val MAE: 0.09911832, Val RMSE: 0.13080347, Val DA: 0.50000000\n",
      "Epoch [126/1000], Train Loss: 0.02117621, Train MAE: 0.11105605, Train RMSE: 0.14552048, Train DA: 0.56164384, Val Loss: 0.01719902, Val MAE: 0.09940808, Val RMSE: 0.13114503, Val DA: 0.51612903\n",
      "Epoch [127/1000], Train Loss: 0.02119337, Train MAE: 0.11113513, Train RMSE: 0.14557941, Train DA: 0.56849315, Val Loss: 0.01729437, Val MAE: 0.09977444, Val RMSE: 0.13150805, Val DA: 0.50000000\n",
      "Epoch [128/1000], Train Loss: 0.02119212, Train MAE: 0.11113501, Train RMSE: 0.14557514, Train DA: 0.56164384, Val Loss: 0.01743083, Val MAE: 0.10023897, Val RMSE: 0.13202588, Val DA: 0.46774194\n",
      "Epoch [129/1000], Train Loss: 0.02118666, Train MAE: 0.11112785, Train RMSE: 0.14555638, Train DA: 0.56164384, Val Loss: 0.01760613, Val MAE: 0.10079046, Val RMSE: 0.13268809, Val DA: 0.48387097\n",
      "Epoch [130/1000], Train Loss: 0.02113856, Train MAE: 0.11089696, Train RMSE: 0.14539106, Train DA: 0.56849315, Val Loss: 0.01772457, Val MAE: 0.10106237, Val RMSE: 0.13313365, Val DA: 0.45161290\n",
      "Epoch [131/1000], Train Loss: 0.02100832, Train MAE: 0.11027288, Train RMSE: 0.14494248, Train DA: 0.55479452, Val Loss: 0.01781912, Val MAE: 0.10124018, Val RMSE: 0.13348825, Val DA: 0.46774194\n",
      "Epoch [132/1000], Train Loss: 0.02088236, Train MAE: 0.10971908, Train RMSE: 0.14450729, Train DA: 0.54794521, Val Loss: 0.01787078, Val MAE: 0.10158597, Val RMSE: 0.13368163, Val DA: 0.48387097\n",
      "Epoch [133/1000], Train Loss: 0.02104697, Train MAE: 0.11090934, Train RMSE: 0.14507574, Train DA: 0.53424658, Val Loss: 0.01837533, Val MAE: 0.10431401, Val RMSE: 0.13555564, Val DA: 0.46774194\n",
      "Epoch [134/1000], Train Loss: 0.01973605, Train MAE: 0.10405485, Train RMSE: 0.14048506, Train DA: 0.52054795, Val Loss: 0.01861571, Val MAE: 0.10266998, Val RMSE: 0.13643941, Val DA: 0.46774194\n",
      "Epoch [135/1000], Train Loss: 0.02090729, Train MAE: 0.10998657, Train RMSE: 0.14459354, Train DA: 0.54794521, Val Loss: 0.01662854, Val MAE: 0.09754130, Val RMSE: 0.12895170, Val DA: 0.54838710\n",
      "Epoch [136/1000], Train Loss: 0.02200599, Train MAE: 0.11552537, Train RMSE: 0.14834416, Train DA: 0.53424658, Val Loss: 0.01822894, Val MAE: 0.10501858, Val RMSE: 0.13501458, Val DA: 0.51612903\n",
      "Epoch [137/1000], Train Loss: 0.02119131, Train MAE: 0.11155926, Train RMSE: 0.14557238, Train DA: 0.55479452, Val Loss: 0.01735774, Val MAE: 0.10161793, Val RMSE: 0.13174877, Val DA: 0.53225806\n",
      "Epoch [138/1000], Train Loss: 0.01999393, Train MAE: 0.10501326, Train RMSE: 0.14139991, Train DA: 0.54109589, Val Loss: 0.01674297, Val MAE: 0.09736283, Val RMSE: 0.12939462, Val DA: 0.51612903\n",
      "Epoch [139/1000], Train Loss: 0.02064616, Train MAE: 0.10905007, Train RMSE: 0.14368774, Train DA: 0.55479452, Val Loss: 0.01809275, Val MAE: 0.10484304, Val RMSE: 0.13450931, Val DA: 0.51612903\n",
      "Epoch [140/1000], Train Loss: 0.02081289, Train MAE: 0.11029653, Train RMSE: 0.14426674, Train DA: 0.52739726, Val Loss: 0.01814936, Val MAE: 0.10662846, Val RMSE: 0.13471957, Val DA: 0.54838710\n",
      "Epoch [141/1000], Train Loss: 0.02099144, Train MAE: 0.11184204, Train RMSE: 0.14488424, Train DA: 0.50684932, Val Loss: 0.01923941, Val MAE: 0.11217971, Val RMSE: 0.13870619, Val DA: 0.54838710\n",
      "Epoch [142/1000], Train Loss: 0.01960030, Train MAE: 0.10425390, Train RMSE: 0.14000109, Train DA: 0.52739726, Val Loss: 0.01915817, Val MAE: 0.11096124, Val RMSE: 0.13841303, Val DA: 0.48387097\n",
      "Epoch [143/1000], Train Loss: 0.01941761, Train MAE: 0.10246601, Train RMSE: 0.13934709, Train DA: 0.50684932, Val Loss: 0.01609166, Val MAE: 0.09898545, Val RMSE: 0.12685291, Val DA: 0.53225806\n",
      "Epoch [144/1000], Train Loss: 0.01926314, Train MAE: 0.10123476, Train RMSE: 0.13879173, Train DA: 0.52054795, Val Loss: 0.01856871, Val MAE: 0.10393827, Val RMSE: 0.13626707, Val DA: 0.53225806\n",
      "Epoch [145/1000], Train Loss: 0.01894632, Train MAE: 0.10063390, Train RMSE: 0.13764565, Train DA: 0.52054795, Val Loss: 0.01647755, Val MAE: 0.10093616, Val RMSE: 0.12836492, Val DA: 0.51612903\n",
      "Epoch [146/1000], Train Loss: 0.02299241, Train MAE: 0.12023170, Train RMSE: 0.15163249, Train DA: 0.55479452, Val Loss: 0.02395208, Val MAE: 0.12730297, Val RMSE: 0.15476459, Val DA: 0.50000000\n",
      "Epoch [147/1000], Train Loss: 0.02482242, Train MAE: 0.12625653, Train RMSE: 0.15755132, Train DA: 0.54109589, Val Loss: 0.02690030, Val MAE: 0.13470812, Val RMSE: 0.16401312, Val DA: 0.43548387\n",
      "Epoch [148/1000], Train Loss: 0.01935542, Train MAE: 0.10094300, Train RMSE: 0.13912375, Train DA: 0.54109589, Val Loss: 0.01619291, Val MAE: 0.09775460, Val RMSE: 0.12725134, Val DA: 0.48387097\n",
      "Epoch [149/1000], Train Loss: 0.01923417, Train MAE: 0.10046540, Train RMSE: 0.13868733, Train DA: 0.54109589, Val Loss: 0.01699570, Val MAE: 0.09759222, Val RMSE: 0.13036753, Val DA: 0.48387097\n",
      "Epoch [150/1000], Train Loss: 0.02492313, Train MAE: 0.12833983, Train RMSE: 0.15787059, Train DA: 0.53424658, Val Loss: 0.02512895, Val MAE: 0.13389666, Val RMSE: 0.15852115, Val DA: 0.51612903\n",
      "Epoch [151/1000], Train Loss: 0.02255044, Train MAE: 0.11946545, Train RMSE: 0.15016806, Train DA: 0.54794521, Val Loss: 0.02447458, Val MAE: 0.13091885, Val RMSE: 0.15644352, Val DA: 0.50000000\n",
      "Epoch [152/1000], Train Loss: 0.02061290, Train MAE: 0.10528954, Train RMSE: 0.14357194, Train DA: 0.54109589, Val Loss: 0.01938746, Val MAE: 0.11245582, Val RMSE: 0.13923885, Val DA: 0.48387097\n",
      "Epoch [153/1000], Train Loss: 0.02115541, Train MAE: 0.10451717, Train RMSE: 0.14544898, Train DA: 0.54794521, Val Loss: 0.01725092, Val MAE: 0.10090171, Val RMSE: 0.13134274, Val DA: 0.45161290\n",
      "Epoch [154/1000], Train Loss: 0.02215110, Train MAE: 0.11779567, Train RMSE: 0.14883247, Train DA: 0.56849315, Val Loss: 0.02106320, Val MAE: 0.12108962, Val RMSE: 0.14513166, Val DA: 0.50000000\n",
      "Epoch [155/1000], Train Loss: 0.02838600, Train MAE: 0.13883643, Train RMSE: 0.16848145, Train DA: 0.57534247, Val Loss: 0.03692962, Val MAE: 0.16228610, Val RMSE: 0.19217081, Val DA: 0.46774194\n",
      "Epoch [156/1000], Train Loss: 0.01909917, Train MAE: 0.10162950, Train RMSE: 0.13819976, Train DA: 0.53424658, Val Loss: 0.01796658, Val MAE: 0.10744555, Val RMSE: 0.13403946, Val DA: 0.45161290\n",
      "Epoch [157/1000], Train Loss: 0.01951468, Train MAE: 0.10213511, Train RMSE: 0.13969496, Train DA: 0.54109589, Val Loss: 0.01663246, Val MAE: 0.10180787, Val RMSE: 0.12896690, Val DA: 0.45161290\n",
      "Epoch [158/1000], Train Loss: 0.02205253, Train MAE: 0.11653799, Train RMSE: 0.14850095, Train DA: 0.54794521, Val Loss: 0.02939256, Val MAE: 0.14037436, Val RMSE: 0.17144260, Val DA: 0.48387097\n",
      "Epoch [159/1000], Train Loss: 0.02261218, Train MAE: 0.11698125, Train RMSE: 0.15037346, Train DA: 0.53424658, Val Loss: 0.03505660, Val MAE: 0.15287641, Val RMSE: 0.18723410, Val DA: 0.48387097\n",
      "Epoch [160/1000], Train Loss: 0.01927379, Train MAE: 0.10272852, Train RMSE: 0.13883008, Train DA: 0.52739726, Val Loss: 0.01828995, Val MAE: 0.10866729, Val RMSE: 0.13524033, Val DA: 0.46774194\n",
      "Epoch [161/1000], Train Loss: 0.01953983, Train MAE: 0.10535277, Train RMSE: 0.13978495, Train DA: 0.52739726, Val Loss: 0.02021256, Val MAE: 0.11527204, Val RMSE: 0.14217089, Val DA: 0.45161290\n",
      "Epoch [162/1000], Train Loss: 0.02578965, Train MAE: 0.12784481, Train RMSE: 0.16059157, Train DA: 0.56164384, Val Loss: 0.03834573, Val MAE: 0.16226561, Val RMSE: 0.19582066, Val DA: 0.45161290\n",
      "Epoch [163/1000], Train Loss: 0.01985714, Train MAE: 0.10730878, Train RMSE: 0.14091538, Train DA: 0.53424658, Val Loss: 0.02776538, Val MAE: 0.13837512, Val RMSE: 0.16662946, Val DA: 0.45161290\n",
      "Epoch [164/1000], Train Loss: 0.01987006, Train MAE: 0.10493571, Train RMSE: 0.14096121, Train DA: 0.53424658, Val Loss: 0.02144592, Val MAE: 0.11979874, Val RMSE: 0.14644428, Val DA: 0.48387097\n",
      "Epoch [165/1000], Train Loss: 0.02439991, Train MAE: 0.11835770, Train RMSE: 0.15620473, Train DA: 0.54109589, Val Loss: 0.04432757, Val MAE: 0.16946608, Val RMSE: 0.21054114, Val DA: 0.48387097\n",
      "Epoch [166/1000], Train Loss: 0.02481369, Train MAE: 0.12383629, Train RMSE: 0.15752360, Train DA: 0.53424658, Val Loss: 0.04142629, Val MAE: 0.16702996, Val RMSE: 0.20353447, Val DA: 0.46774194\n",
      "Epoch [167/1000], Train Loss: 0.01987392, Train MAE: 0.10767769, Train RMSE: 0.14097491, Train DA: 0.52054795, Val Loss: 0.03093233, Val MAE: 0.14581394, Val RMSE: 0.17587590, Val DA: 0.46774194\n",
      "Epoch [168/1000], Train Loss: 0.02023255, Train MAE: 0.10372646, Train RMSE: 0.14224115, Train DA: 0.52739726, Val Loss: 0.02000250, Val MAE: 0.10795507, Val RMSE: 0.14143018, Val DA: 0.48387097\n",
      "Epoch [169/1000], Train Loss: 0.02040344, Train MAE: 0.11059932, Train RMSE: 0.14284061, Train DA: 0.52054795, Val Loss: 0.02052140, Val MAE: 0.11474757, Val RMSE: 0.14325292, Val DA: 0.45161290\n",
      "Epoch [170/1000], Train Loss: 0.02587960, Train MAE: 0.13248405, Train RMSE: 0.16087139, Train DA: 0.52739726, Val Loss: 0.02830614, Val MAE: 0.14200641, Val RMSE: 0.16824429, Val DA: 0.45161290\n",
      "Epoch [171/1000], Train Loss: 0.02111442, Train MAE: 0.11277286, Train RMSE: 0.14530803, Train DA: 0.54109589, Val Loss: 0.03368473, Val MAE: 0.15253672, Val RMSE: 0.18353400, Val DA: 0.48387097\n",
      "Epoch [172/1000], Train Loss: 0.02124084, Train MAE: 0.11194477, Train RMSE: 0.14574239, Train DA: 0.52054795, Val Loss: 0.03172171, Val MAE: 0.15069266, Val RMSE: 0.17810591, Val DA: 0.48387097\n",
      "Epoch [173/1000], Train Loss: 0.01990099, Train MAE: 0.10663867, Train RMSE: 0.14107087, Train DA: 0.50684932, Val Loss: 0.03728056, Val MAE: 0.15103455, Val RMSE: 0.19308174, Val DA: 0.46774194\n",
      "Epoch [174/1000], Train Loss: 0.03800006, Train MAE: 0.14489326, Train RMSE: 0.19493602, Train DA: 0.53424658, Val Loss: 0.10184156, Val MAE: 0.23301433, Val RMSE: 0.31912625, Val DA: 0.48387097\n",
      "Epoch [175/1000], Train Loss: 0.01901923, Train MAE: 0.09938572, Train RMSE: 0.13791023, Train DA: 0.54109589, Val Loss: 0.01685496, Val MAE: 0.09385100, Val RMSE: 0.12982666, Val DA: 0.45161290\n",
      "Epoch [176/1000], Train Loss: 0.01977737, Train MAE: 0.09936600, Train RMSE: 0.14063203, Train DA: 0.56849315, Val Loss: 0.01667569, Val MAE: 0.09051754, Val RMSE: 0.12913439, Val DA: 0.43548387\n",
      "Epoch [177/1000], Train Loss: 0.02398522, Train MAE: 0.12517233, Train RMSE: 0.15487161, Train DA: 0.57534247, Val Loss: 0.02134562, Val MAE: 0.11895790, Val RMSE: 0.14610140, Val DA: 0.41935484\n",
      "Epoch [178/1000], Train Loss: 0.02213373, Train MAE: 0.11715040, Train RMSE: 0.14877410, Train DA: 0.58219178, Val Loss: 0.02089723, Val MAE: 0.11602837, Val RMSE: 0.14455874, Val DA: 0.43548387\n",
      "Epoch [179/1000], Train Loss: 0.02028814, Train MAE: 0.10651369, Train RMSE: 0.14243644, Train DA: 0.59589041, Val Loss: 0.01892165, Val MAE: 0.10539732, Val RMSE: 0.13755599, Val DA: 0.46774194\n",
      "Epoch [180/1000], Train Loss: 0.02014478, Train MAE: 0.10736617, Train RMSE: 0.14193232, Train DA: 0.55479452, Val Loss: 0.01828225, Val MAE: 0.10520984, Val RMSE: 0.13521188, Val DA: 0.46774194\n",
      "Epoch [181/1000], Train Loss: 0.02362609, Train MAE: 0.12366007, Train RMSE: 0.15370782, Train DA: 0.54794521, Val Loss: 0.02143791, Val MAE: 0.12123872, Val RMSE: 0.14641690, Val DA: 0.45161290\n",
      "Epoch [182/1000], Train Loss: 0.02117497, Train MAE: 0.11274371, Train RMSE: 0.14551623, Train DA: 0.56849315, Val Loss: 0.02031294, Val MAE: 0.11440965, Val RMSE: 0.14252347, Val DA: 0.45161290\n",
      "Epoch [183/1000], Train Loss: 0.02028465, Train MAE: 0.10673027, Train RMSE: 0.14242418, Train DA: 0.55479452, Val Loss: 0.02018873, Val MAE: 0.10946468, Val RMSE: 0.14208706, Val DA: 0.45161290\n",
      "Epoch [184/1000], Train Loss: 0.02123862, Train MAE: 0.11207253, Train RMSE: 0.14573476, Train DA: 0.55479452, Val Loss: 0.02095863, Val MAE: 0.11516093, Val RMSE: 0.14477094, Val DA: 0.45161290\n",
      "Epoch [185/1000], Train Loss: 0.02181607, Train MAE: 0.11580151, Train RMSE: 0.14770266, Train DA: 0.56164384, Val Loss: 0.02139899, Val MAE: 0.11893721, Val RMSE: 0.14628394, Val DA: 0.43548387\n",
      "Epoch [186/1000], Train Loss: 0.02143452, Train MAE: 0.11430625, Train RMSE: 0.14640532, Train DA: 0.56849315, Val Loss: 0.02136134, Val MAE: 0.11812955, Val RMSE: 0.14615518, Val DA: 0.45161290\n",
      "Epoch [187/1000], Train Loss: 0.02065914, Train MAE: 0.10866901, Train RMSE: 0.14373289, Train DA: 0.56164384, Val Loss: 0.02061679, Val MAE: 0.11120398, Val RMSE: 0.14358547, Val DA: 0.43548387\n",
      "Epoch [188/1000], Train Loss: 0.02116876, Train MAE: 0.11029613, Train RMSE: 0.14549486, Train DA: 0.55479452, Val Loss: 0.02168236, Val MAE: 0.11461635, Val RMSE: 0.14724930, Val DA: 0.43548387\n",
      "Epoch [189/1000], Train Loss: 0.02151195, Train MAE: 0.11352163, Train RMSE: 0.14666954, Train DA: 0.56849315, Val Loss: 0.02238382, Val MAE: 0.11970721, Val RMSE: 0.14961223, Val DA: 0.41935484\n",
      "Epoch [190/1000], Train Loss: 0.02202365, Train MAE: 0.11632706, Train RMSE: 0.14840366, Train DA: 0.55479452, Val Loss: 0.02228548, Val MAE: 0.12063272, Val RMSE: 0.14928322, Val DA: 0.40322581\n",
      "Epoch [191/1000], Train Loss: 0.02146744, Train MAE: 0.11296110, Train RMSE: 0.14651772, Train DA: 0.54109589, Val Loss: 0.02182619, Val MAE: 0.11720038, Val RMSE: 0.14773689, Val DA: 0.40322581\n",
      "Epoch [192/1000], Train Loss: 0.02152287, Train MAE: 0.11075191, Train RMSE: 0.14670673, Train DA: 0.52739726, Val Loss: 0.02282800, Val MAE: 0.11717764, Val RMSE: 0.15108937, Val DA: 0.38709677\n",
      "Epoch [193/1000], Train Loss: 0.02114207, Train MAE: 0.10861050, Train RMSE: 0.14540315, Train DA: 0.50684932, Val Loss: 0.02252379, Val MAE: 0.11541702, Val RMSE: 0.15007928, Val DA: 0.35483871\n",
      "Epoch [194/1000], Train Loss: 0.02167632, Train MAE: 0.11369999, Train RMSE: 0.14722878, Train DA: 0.50684932, Val Loss: 0.02323617, Val MAE: 0.12150007, Val RMSE: 0.15243414, Val DA: 0.37096774\n",
      "Epoch [195/1000], Train Loss: 0.02304393, Train MAE: 0.12085260, Train RMSE: 0.15180227, Train DA: 0.49315068, Val Loss: 0.02413009, Val MAE: 0.12782010, Val RMSE: 0.15533862, Val DA: 0.40322581\n",
      "Epoch [196/1000], Train Loss: 0.02232854, Train MAE: 0.11779439, Train RMSE: 0.14942738, Train DA: 0.50000000, Val Loss: 0.02337275, Val MAE: 0.12376753, Val RMSE: 0.15288147, Val DA: 0.40322581\n",
      "Epoch [197/1000], Train Loss: 0.02156078, Train MAE: 0.11025228, Train RMSE: 0.14683591, Train DA: 0.49315068, Val Loss: 0.02347951, Val MAE: 0.12010098, Val RMSE: 0.15323025, Val DA: 0.37096774\n",
      "Epoch [198/1000], Train Loss: 0.02273640, Train MAE: 0.11077990, Train RMSE: 0.15078594, Train DA: 0.49315068, Val Loss: 0.02577282, Val MAE: 0.12123517, Val RMSE: 0.16053914, Val DA: 0.35483871\n",
      "Epoch [199/1000], Train Loss: 0.02135168, Train MAE: 0.11024261, Train RMSE: 0.14612214, Train DA: 0.48630137, Val Loss: 0.02384858, Val MAE: 0.12145312, Val RMSE: 0.15442987, Val DA: 0.32258065\n",
      "Epoch [200/1000], Train Loss: 0.02491805, Train MAE: 0.12824164, Train RMSE: 0.15785453, Train DA: 0.49315068, Val Loss: 0.02745126, Val MAE: 0.13873562, Val RMSE: 0.16568422, Val DA: 0.37096774\n",
      "Epoch [201/1000], Train Loss: 0.02872577, Train MAE: 0.14183706, Train RMSE: 0.16948678, Train DA: 0.50684932, Val Loss: 0.03023399, Val MAE: 0.14645818, Val RMSE: 0.17387925, Val DA: 0.37096774\n",
      "Epoch [202/1000], Train Loss: 0.02008216, Train MAE: 0.10640258, Train RMSE: 0.14171152, Train DA: 0.53424658, Val Loss: 0.02154863, Val MAE: 0.11593234, Val RMSE: 0.14679453, Val DA: 0.38709677\n",
      "Epoch [203/1000], Train Loss: 0.02239121, Train MAE: 0.11236782, Train RMSE: 0.14963694, Train DA: 0.52054795, Val Loss: 0.02503479, Val MAE: 0.12728880, Val RMSE: 0.15822384, Val DA: 0.37096774\n",
      "Epoch [204/1000], Train Loss: 0.02295847, Train MAE: 0.11945572, Train RMSE: 0.15152054, Train DA: 0.48630137, Val Loss: 0.02617205, Val MAE: 0.13107257, Val RMSE: 0.16177779, Val DA: 0.33870968\n",
      "Epoch [205/1000], Train Loss: 0.02796160, Train MAE: 0.13883637, Train RMSE: 0.16721725, Train DA: 0.50000000, Val Loss: 0.03133484, Val MAE: 0.14902557, Val RMSE: 0.17701650, Val DA: 0.37096774\n",
      "Epoch [206/1000], Train Loss: 0.02217310, Train MAE: 0.11730194, Train RMSE: 0.14890635, Train DA: 0.52739726, Val Loss: 0.02640904, Val MAE: 0.13212602, Val RMSE: 0.16250859, Val DA: 0.41935484\n",
      "Epoch [207/1000], Train Loss: 0.02140843, Train MAE: 0.11274220, Train RMSE: 0.14631617, Train DA: 0.49315068, Val Loss: 0.02528252, Val MAE: 0.12872243, Val RMSE: 0.15900478, Val DA: 0.41935484\n",
      "Epoch [208/1000], Train Loss: 0.02150776, Train MAE: 0.11004958, Train RMSE: 0.14665523, Train DA: 0.50000000, Val Loss: 0.02064684, Val MAE: 0.11384116, Val RMSE: 0.14369009, Val DA: 0.41935484\n",
      "Epoch [209/1000], Train Loss: 0.01999526, Train MAE: 0.10906793, Train RMSE: 0.14140458, Train DA: 0.52054795, Val Loss: 0.02199907, Val MAE: 0.11475980, Val RMSE: 0.14832085, Val DA: 0.40322581\n",
      "Epoch [210/1000], Train Loss: 0.02927750, Train MAE: 0.14167731, Train RMSE: 0.17110670, Train DA: 0.50684932, Val Loss: 0.03647161, Val MAE: 0.15522112, Val RMSE: 0.19097543, Val DA: 0.38709677\n",
      "Epoch [211/1000], Train Loss: 0.02278771, Train MAE: 0.12172286, Train RMSE: 0.15095599, Train DA: 0.51369863, Val Loss: 0.02979541, Val MAE: 0.14246817, Val RMSE: 0.17261347, Val DA: 0.41935484\n",
      "Epoch [212/1000], Train Loss: 0.02211110, Train MAE: 0.11494453, Train RMSE: 0.14869802, Train DA: 0.51369863, Val Loss: 0.02506039, Val MAE: 0.12916861, Val RMSE: 0.15830475, Val DA: 0.41935484\n",
      "Epoch [213/1000], Train Loss: 0.02364453, Train MAE: 0.12185797, Train RMSE: 0.15376776, Train DA: 0.45890411, Val Loss: 0.03122796, Val MAE: 0.14173394, Val RMSE: 0.17671435, Val DA: 0.40322581\n",
      "Epoch [214/1000], Train Loss: 0.02863888, Train MAE: 0.13660593, Train RMSE: 0.16923025, Train DA: 0.46575342, Val Loss: 0.04086103, Val MAE: 0.16575563, Val RMSE: 0.20214111, Val DA: 0.37096774\n",
      "Epoch [215/1000], Train Loss: 0.02372624, Train MAE: 0.12233490, Train RMSE: 0.15403324, Train DA: 0.47945205, Val Loss: 0.03474788, Val MAE: 0.15369208, Val RMSE: 0.18640783, Val DA: 0.38709677\n",
      "Epoch [216/1000], Train Loss: 0.02066606, Train MAE: 0.11156677, Train RMSE: 0.14375694, Train DA: 0.50000000, Val Loss: 0.02813958, Val MAE: 0.13733828, Val RMSE: 0.16774856, Val DA: 0.40322581\n",
      "Epoch [217/1000], Train Loss: 0.02505908, Train MAE: 0.12759817, Train RMSE: 0.15830061, Train DA: 0.49315068, Val Loss: 0.03774301, Val MAE: 0.15837128, Val RMSE: 0.19427560, Val DA: 0.37096774\n",
      "Epoch [218/1000], Train Loss: 0.03088002, Train MAE: 0.14402086, Train RMSE: 0.17572710, Train DA: 0.52739726, Val Loss: 0.04507162, Val MAE: 0.17360200, Val RMSE: 0.21230078, Val DA: 0.40322581\n",
      "Epoch [219/1000], Train Loss: 0.02148914, Train MAE: 0.11405096, Train RMSE: 0.14659175, Train DA: 0.51369863, Val Loss: 0.03024686, Val MAE: 0.14102407, Val RMSE: 0.17391624, Val DA: 0.45161290\n",
      "Epoch [220/1000], Train Loss: 0.02217362, Train MAE: 0.11372452, Train RMSE: 0.14890808, Train DA: 0.51369863, Val Loss: 0.02735311, Val MAE: 0.13532883, Val RMSE: 0.16538776, Val DA: 0.40322581\n",
      "Epoch [221/1000], Train Loss: 0.02576527, Train MAE: 0.12882148, Train RMSE: 0.16051562, Train DA: 0.47945205, Val Loss: 0.03637888, Val MAE: 0.15108505, Val RMSE: 0.19073251, Val DA: 0.41935484\n",
      "Epoch [222/1000], Train Loss: 0.03707148, Train MAE: 0.16007477, Train RMSE: 0.19253956, Train DA: 0.50684932, Val Loss: 0.05848717, Val MAE: 0.19237320, Val RMSE: 0.24184120, Val DA: 0.40322581\n",
      "Epoch [223/1000], Train Loss: 0.01998228, Train MAE: 0.10759682, Train RMSE: 0.14135869, Train DA: 0.50000000, Val Loss: 0.02269386, Val MAE: 0.11808687, Val RMSE: 0.15064479, Val DA: 0.48387097\n",
      "Epoch [224/1000], Train Loss: 0.02437944, Train MAE: 0.11357502, Train RMSE: 0.15613917, Train DA: 0.56164384, Val Loss: 0.02395613, Val MAE: 0.11354970, Val RMSE: 0.15477769, Val DA: 0.46774194\n",
      "Epoch [225/1000], Train Loss: 0.02680425, Train MAE: 0.13584228, Train RMSE: 0.16372004, Train DA: 0.51369863, Val Loss: 0.02556593, Val MAE: 0.13458155, Val RMSE: 0.15989348, Val DA: 0.45161290\n",
      "Epoch [226/1000], Train Loss: 0.03123307, Train MAE: 0.15093260, Train RMSE: 0.17672881, Train DA: 0.49315068, Val Loss: 0.03039444, Val MAE: 0.14975415, Val RMSE: 0.17433999, Val DA: 0.48387097\n",
      "Epoch [227/1000], Train Loss: 0.02017582, Train MAE: 0.10332284, Train RMSE: 0.14204161, Train DA: 0.50000000, Val Loss: 0.02307936, Val MAE: 0.11080355, Val RMSE: 0.15191893, Val DA: 0.46774194\n",
      "Epoch [228/1000], Train Loss: 0.01954894, Train MAE: 0.10603256, Train RMSE: 0.13981752, Train DA: 0.52739726, Val Loss: 0.02416179, Val MAE: 0.11926728, Val RMSE: 0.15544063, Val DA: 0.45161290\n",
      "Epoch [229/1000], Train Loss: 0.02725371, Train MAE: 0.13770720, Train RMSE: 0.16508697, Train DA: 0.52739726, Val Loss: 0.03280691, Val MAE: 0.14955375, Val RMSE: 0.18112677, Val DA: 0.45161290\n",
      "Epoch [230/1000], Train Loss: 0.02313913, Train MAE: 0.12355405, Train RMSE: 0.15211552, Train DA: 0.51369863, Val Loss: 0.02958056, Val MAE: 0.13963400, Val RMSE: 0.17198999, Val DA: 0.46774194\n",
      "Epoch [231/1000], Train Loss: 0.02123679, Train MAE: 0.11191046, Train RMSE: 0.14572847, Train DA: 0.48630137, Val Loss: 0.02565319, Val MAE: 0.12443713, Val RMSE: 0.16016613, Val DA: 0.50000000\n",
      "Epoch [232/1000], Train Loss: 0.02045368, Train MAE: 0.11334104, Train RMSE: 0.14301637, Train DA: 0.48630137, Val Loss: 0.02634691, Val MAE: 0.12820610, Val RMSE: 0.16231729, Val DA: 0.50000000\n",
      "Epoch [233/1000], Train Loss: 0.02803922, Train MAE: 0.13683824, Train RMSE: 0.16744915, Train DA: 0.50684932, Val Loss: 0.03908157, Val MAE: 0.16027373, Val RMSE: 0.19769059, Val DA: 0.50000000\n",
      "Epoch [234/1000], Train Loss: 0.02706409, Train MAE: 0.13211048, Train RMSE: 0.16451167, Train DA: 0.50000000, Val Loss: 0.03853761, Val MAE: 0.15804036, Val RMSE: 0.19630997, Val DA: 0.48387097\n",
      "Epoch [235/1000], Train Loss: 0.02268745, Train MAE: 0.11352607, Train RMSE: 0.15062353, Train DA: 0.46575342, Val Loss: 0.02144766, Val MAE: 0.11633327, Val RMSE: 0.14645021, Val DA: 0.48387097\n",
      "Epoch [236/1000], Train Loss: 0.01960183, Train MAE: 0.10802914, Train RMSE: 0.14000653, Train DA: 0.50684932, Val Loss: 0.03300160, Val MAE: 0.12921986, Val RMSE: 0.18166342, Val DA: 0.46774194\n",
      "Epoch [237/1000], Train Loss: 0.02477009, Train MAE: 0.12650381, Train RMSE: 0.15738517, Train DA: 0.49315068, Val Loss: 0.03708545, Val MAE: 0.15210921, Val RMSE: 0.19257584, Val DA: 0.40322581\n",
      "Epoch [238/1000], Train Loss: 0.02158394, Train MAE: 0.11659241, Train RMSE: 0.14691474, Train DA: 0.47945205, Val Loss: 0.02477436, Val MAE: 0.12734590, Val RMSE: 0.15739875, Val DA: 0.43548387\n",
      "Epoch [239/1000], Train Loss: 0.02141753, Train MAE: 0.11433873, Train RMSE: 0.14634730, Train DA: 0.46575342, Val Loss: 0.02863619, Val MAE: 0.12809208, Val RMSE: 0.16922233, Val DA: 0.45161290\n",
      "Epoch [240/1000], Train Loss: 0.02136851, Train MAE: 0.11488215, Train RMSE: 0.14617971, Train DA: 0.45890411, Val Loss: 0.02810573, Val MAE: 0.12444961, Val RMSE: 0.16764762, Val DA: 0.46774194\n",
      "Epoch [241/1000], Train Loss: 0.02273937, Train MAE: 0.11967687, Train RMSE: 0.15079579, Train DA: 0.48630137, Val Loss: 0.02787075, Val MAE: 0.12797305, Val RMSE: 0.16694535, Val DA: 0.46774194\n",
      "Epoch [242/1000], Train Loss: 0.02512535, Train MAE: 0.12786460, Train RMSE: 0.15850979, Train DA: 0.44520548, Val Loss: 0.03715494, Val MAE: 0.14189459, Val RMSE: 0.19275618, Val DA: 0.50000000\n",
      "Epoch [243/1000], Train Loss: 0.02280673, Train MAE: 0.12098189, Train RMSE: 0.15101896, Train DA: 0.46575342, Val Loss: 0.03458590, Val MAE: 0.13331099, Val RMSE: 0.18597285, Val DA: 0.53225806\n",
      "Epoch [244/1000], Train Loss: 0.02116447, Train MAE: 0.11083961, Train RMSE: 0.14548016, Train DA: 0.49315068, Val Loss: 0.02680458, Val MAE: 0.12303683, Val RMSE: 0.16372104, Val DA: 0.50000000\n",
      "Epoch [245/1000], Train Loss: 0.02227183, Train MAE: 0.11882267, Train RMSE: 0.14923748, Train DA: 0.46575342, Val Loss: 0.03133631, Val MAE: 0.12880777, Val RMSE: 0.17702065, Val DA: 0.43548387\n",
      "Epoch [246/1000], Train Loss: 0.02672476, Train MAE: 0.13306901, Train RMSE: 0.16347708, Train DA: 0.49315068, Val Loss: 0.04105487, Val MAE: 0.15089026, Val RMSE: 0.20262003, Val DA: 0.46774194\n",
      "Epoch [247/1000], Train Loss: 0.02107922, Train MAE: 0.11482060, Train RMSE: 0.14518687, Train DA: 0.51369863, Val Loss: 0.03640549, Val MAE: 0.13608015, Val RMSE: 0.19080225, Val DA: 0.50000000\n",
      "Epoch [248/1000], Train Loss: 0.02005642, Train MAE: 0.10886467, Train RMSE: 0.14162068, Train DA: 0.50684932, Val Loss: 0.03194425, Val MAE: 0.12914345, Val RMSE: 0.17872952, Val DA: 0.51612903\n",
      "Epoch [249/1000], Train Loss: 0.02195647, Train MAE: 0.11765110, Train RMSE: 0.14817715, Train DA: 0.51369863, Val Loss: 0.03421069, Val MAE: 0.13614966, Val RMSE: 0.18496130, Val DA: 0.45161290\n",
      "Epoch [250/1000], Train Loss: 0.02556457, Train MAE: 0.12625636, Train RMSE: 0.15988925, Train DA: 0.49315068, Val Loss: 0.03917343, Val MAE: 0.14525881, Val RMSE: 0.19792280, Val DA: 0.43548387\n",
      "Epoch [251/1000], Train Loss: 0.02340161, Train MAE: 0.11894518, Train RMSE: 0.15297584, Train DA: 0.49315068, Val Loss: 0.03824674, Val MAE: 0.14136492, Val RMSE: 0.19556774, Val DA: 0.41935484\n",
      "Epoch [252/1000], Train Loss: 0.02180629, Train MAE: 0.10893547, Train RMSE: 0.14766954, Train DA: 0.46575342, Val Loss: 0.02855225, Val MAE: 0.12263773, Val RMSE: 0.16897412, Val DA: 0.41935484\n",
      "Epoch [253/1000], Train Loss: 0.02327371, Train MAE: 0.11862919, Train RMSE: 0.15255725, Train DA: 0.50000000, Val Loss: 0.04071273, Val MAE: 0.14373873, Val RMSE: 0.20177394, Val DA: 0.41935484\n",
      "Epoch [254/1000], Train Loss: 0.02505555, Train MAE: 0.12428938, Train RMSE: 0.15828945, Train DA: 0.47260274, Val Loss: 0.02604780, Val MAE: 0.12588508, Val RMSE: 0.16139331, Val DA: 0.43548387\n",
      "Epoch [255/1000], Train Loss: 0.02149612, Train MAE: 0.10998177, Train RMSE: 0.14661555, Train DA: 0.47260274, Val Loss: 0.02291523, Val MAE: 0.11159546, Val RMSE: 0.15137778, Val DA: 0.45161290\n",
      "Epoch [256/1000], Train Loss: 0.02246550, Train MAE: 0.10775610, Train RMSE: 0.14988494, Train DA: 0.47945205, Val Loss: 0.02563399, Val MAE: 0.11403698, Val RMSE: 0.16010620, Val DA: 0.50000000\n",
      "Epoch [257/1000], Train Loss: 0.02115138, Train MAE: 0.11121697, Train RMSE: 0.14543514, Train DA: 0.48630137, Val Loss: 0.02760608, Val MAE: 0.11674520, Val RMSE: 0.16615078, Val DA: 0.45161290\n",
      "Epoch [258/1000], Train Loss: 0.03664319, Train MAE: 0.15446195, Train RMSE: 0.19142410, Train DA: 0.46575342, Val Loss: 0.03502503, Val MAE: 0.14182183, Val RMSE: 0.18714975, Val DA: 0.50000000\n",
      "Epoch [259/1000], Train Loss: 0.02137120, Train MAE: 0.10876875, Train RMSE: 0.14618893, Train DA: 0.54794521, Val Loss: 0.02512358, Val MAE: 0.11871678, Val RMSE: 0.15850419, Val DA: 0.51612903\n",
      "Epoch [260/1000], Train Loss: 0.02473048, Train MAE: 0.11242425, Train RMSE: 0.15725929, Train DA: 0.54109589, Val Loss: 0.01958402, Val MAE: 0.10218815, Val RMSE: 0.13994293, Val DA: 0.59677419\n",
      "Epoch [261/1000], Train Loss: 0.02456327, Train MAE: 0.12716880, Train RMSE: 0.15672673, Train DA: 0.53424658, Val Loss: 0.02276233, Val MAE: 0.12152688, Val RMSE: 0.15087190, Val DA: 0.53225806\n",
      "Epoch [262/1000], Train Loss: 0.01873321, Train MAE: 0.10265121, Train RMSE: 0.13686933, Train DA: 0.50000000, Val Loss: 0.01580462, Val MAE: 0.08954678, Val RMSE: 0.12571643, Val DA: 0.56451613\n",
      "Epoch [263/1000], Train Loss: 0.01869037, Train MAE: 0.10261401, Train RMSE: 0.13671272, Train DA: 0.50684932, Val Loss: 0.01820787, Val MAE: 0.10397525, Val RMSE: 0.13493654, Val DA: 0.54838710\n",
      "Epoch [264/1000], Train Loss: 0.02066118, Train MAE: 0.11156838, Train RMSE: 0.14373997, Train DA: 0.47260274, Val Loss: 0.02167958, Val MAE: 0.11740173, Val RMSE: 0.14723985, Val DA: 0.50000000\n",
      "Epoch [265/1000], Train Loss: 0.02241798, Train MAE: 0.11870584, Train RMSE: 0.14972635, Train DA: 0.47260274, Val Loss: 0.02456571, Val MAE: 0.12236678, Val RMSE: 0.15673453, Val DA: 0.54838710\n",
      "Epoch [266/1000], Train Loss: 0.02306629, Train MAE: 0.11981735, Train RMSE: 0.15187590, Train DA: 0.48630137, Val Loss: 0.03598231, Val MAE: 0.14115632, Val RMSE: 0.18969002, Val DA: 0.51612903\n",
      "Epoch [267/1000], Train Loss: 0.02144903, Train MAE: 0.11309490, Train RMSE: 0.14645486, Train DA: 0.49315068, Val Loss: 0.05423220, Val MAE: 0.16481954, Val RMSE: 0.23287806, Val DA: 0.50000000\n",
      "Epoch [268/1000], Train Loss: 0.02123459, Train MAE: 0.10825971, Train RMSE: 0.14572094, Train DA: 0.49315068, Val Loss: 0.03681881, Val MAE: 0.13882267, Val RMSE: 0.19188230, Val DA: 0.48387097\n",
      "Epoch [269/1000], Train Loss: 0.02812760, Train MAE: 0.13218097, Train RMSE: 0.16771285, Train DA: 0.50000000, Val Loss: 0.05380946, Val MAE: 0.16525519, Val RMSE: 0.23196867, Val DA: 0.48387097\n",
      "Epoch [270/1000], Train Loss: 0.02319043, Train MAE: 0.11465447, Train RMSE: 0.15228404, Train DA: 0.52739726, Val Loss: 0.05139275, Val MAE: 0.16979104, Val RMSE: 0.22669969, Val DA: 0.51612903\n",
      "Epoch [271/1000], Train Loss: 0.02177453, Train MAE: 0.10787413, Train RMSE: 0.14756197, Train DA: 0.52739726, Val Loss: 0.02544697, Val MAE: 0.12180237, Val RMSE: 0.15952106, Val DA: 0.50000000\n",
      "Epoch [272/1000], Train Loss: 0.02639332, Train MAE: 0.12755808, Train RMSE: 0.16246019, Train DA: 0.53424658, Val Loss: 0.06836496, Val MAE: 0.19286670, Val RMSE: 0.26146695, Val DA: 0.56451613\n",
      "Epoch [273/1000], Train Loss: 0.02147157, Train MAE: 0.10952083, Train RMSE: 0.14653181, Train DA: 0.54794521, Val Loss: 0.03421798, Val MAE: 0.13880102, Val RMSE: 0.18498105, Val DA: 0.50000000\n",
      "Epoch [274/1000], Train Loss: 0.02083347, Train MAE: 0.10358243, Train RMSE: 0.14433804, Train DA: 0.55479452, Val Loss: 0.01593360, Val MAE: 0.09038503, Val RMSE: 0.12622835, Val DA: 0.51612903\n",
      "Epoch [275/1000], Train Loss: 0.02368308, Train MAE: 0.12412467, Train RMSE: 0.15389308, Train DA: 0.56849315, Val Loss: 0.01893642, Val MAE: 0.10888055, Val RMSE: 0.13760968, Val DA: 0.53225806\n",
      "Epoch [276/1000], Train Loss: 0.01953017, Train MAE: 0.10196771, Train RMSE: 0.13975038, Train DA: 0.59589041, Val Loss: 0.01480606, Val MAE: 0.08650506, Val RMSE: 0.12168017, Val DA: 0.50000000\n",
      "Epoch [277/1000], Train Loss: 0.01956287, Train MAE: 0.10060025, Train RMSE: 0.13986732, Train DA: 0.57534247, Val Loss: 0.01511894, Val MAE: 0.08568677, Val RMSE: 0.12295908, Val DA: 0.46774194\n",
      "Epoch [278/1000], Train Loss: 0.02194759, Train MAE: 0.11691844, Train RMSE: 0.14814718, Train DA: 0.58904110, Val Loss: 0.01835997, Val MAE: 0.10647427, Val RMSE: 0.13549899, Val DA: 0.51612903\n",
      "Epoch [279/1000], Train Loss: 0.02093487, Train MAE: 0.11402566, Train RMSE: 0.14468887, Train DA: 0.58219178, Val Loss: 0.01870681, Val MAE: 0.10650600, Val RMSE: 0.13677284, Val DA: 0.53225806\n",
      "Epoch [280/1000], Train Loss: 0.01987083, Train MAE: 0.10958649, Train RMSE: 0.14096393, Train DA: 0.56849315, Val Loss: 0.01880816, Val MAE: 0.10581857, Val RMSE: 0.13714284, Val DA: 0.51612903\n",
      "Epoch [281/1000], Train Loss: 0.01918393, Train MAE: 0.10660864, Train RMSE: 0.13850607, Train DA: 0.56849315, Val Loss: 0.01997494, Val MAE: 0.10900520, Val RMSE: 0.14133272, Val DA: 0.51612903\n",
      "Epoch [282/1000], Train Loss: 0.01935072, Train MAE: 0.10683298, Train RMSE: 0.13910687, Train DA: 0.58219178, Val Loss: 0.02232365, Val MAE: 0.11551492, Val RMSE: 0.14941102, Val DA: 0.51612903\n",
      "Epoch [283/1000], Train Loss: 0.02011537, Train MAE: 0.10956465, Train RMSE: 0.14182866, Train DA: 0.60273973, Val Loss: 0.02435485, Val MAE: 0.11867698, Val RMSE: 0.15606041, Val DA: 0.48387097\n",
      "Epoch [284/1000], Train Loss: 0.02645697, Train MAE: 0.13067311, Train RMSE: 0.16265598, Train DA: 0.60273973, Val Loss: 0.03139927, Val MAE: 0.13461360, Val RMSE: 0.17719840, Val DA: 0.46774194\n",
      "Epoch [285/1000], Train Loss: 0.03006453, Train MAE: 0.14128728, Train RMSE: 0.17339125, Train DA: 0.60958904, Val Loss: 0.03813104, Val MAE: 0.15176567, Val RMSE: 0.19527173, Val DA: 0.46774194\n",
      "Epoch [286/1000], Train Loss: 0.02250684, Train MAE: 0.11262441, Train RMSE: 0.15002279, Train DA: 0.61643836, Val Loss: 0.03635313, Val MAE: 0.14580905, Val RMSE: 0.19066498, Val DA: 0.46774194\n",
      "Epoch [287/1000], Train Loss: 0.02794493, Train MAE: 0.12210537, Train RMSE: 0.16716737, Train DA: 0.60273973, Val Loss: 0.02528369, Val MAE: 0.11980543, Val RMSE: 0.15900846, Val DA: 0.46774194\n",
      "Epoch [288/1000], Train Loss: 0.01917552, Train MAE: 0.09838036, Train RMSE: 0.13847570, Train DA: 0.56849315, Val Loss: 0.01570202, Val MAE: 0.08570821, Val RMSE: 0.12530772, Val DA: 0.45161290\n",
      "Epoch [289/1000], Train Loss: 0.02928391, Train MAE: 0.14483458, Train RMSE: 0.17112543, Train DA: 0.58219178, Val Loss: 0.02451635, Val MAE: 0.13057227, Val RMSE: 0.15657699, Val DA: 0.45161290\n",
      "Epoch [290/1000], Train Loss: 0.01960046, Train MAE: 0.10995617, Train RMSE: 0.14000165, Train DA: 0.55479452, Val Loss: 0.01717236, Val MAE: 0.09817051, Val RMSE: 0.13104336, Val DA: 0.51612903\n",
      "Epoch [291/1000], Train Loss: 0.03544989, Train MAE: 0.14785905, Train RMSE: 0.18828140, Train DA: 0.47945205, Val Loss: 0.02846348, Val MAE: 0.13230745, Val RMSE: 0.16871125, Val DA: 0.56451613\n",
      "Epoch [292/1000], Train Loss: 0.01924697, Train MAE: 0.10061152, Train RMSE: 0.13873346, Train DA: 0.48630137, Val Loss: 0.01596359, Val MAE: 0.09102742, Val RMSE: 0.12634709, Val DA: 0.51612903\n",
      "Epoch [293/1000], Train Loss: 0.02178473, Train MAE: 0.11856574, Train RMSE: 0.14759649, Train DA: 0.56164384, Val Loss: 0.02290606, Val MAE: 0.11957830, Val RMSE: 0.15134750, Val DA: 0.50000000\n",
      "Epoch [294/1000], Train Loss: 0.02184637, Train MAE: 0.11664467, Train RMSE: 0.14780515, Train DA: 0.54794521, Val Loss: 0.02534225, Val MAE: 0.12747762, Val RMSE: 0.15919247, Val DA: 0.54838710\n",
      "Epoch [295/1000], Train Loss: 0.02298094, Train MAE: 0.11274060, Train RMSE: 0.15159464, Train DA: 0.61643836, Val Loss: 0.02706546, Val MAE: 0.13018881, Val RMSE: 0.16451584, Val DA: 0.53225806\n",
      "Epoch [296/1000], Train Loss: 0.02367720, Train MAE: 0.12441381, Train RMSE: 0.15387398, Train DA: 0.60958904, Val Loss: 0.02671923, Val MAE: 0.12595758, Val RMSE: 0.16346020, Val DA: 0.51612903\n",
      "Epoch [297/1000], Train Loss: 0.02213703, Train MAE: 0.11708681, Train RMSE: 0.14878517, Train DA: 0.57534247, Val Loss: 0.02483862, Val MAE: 0.11780077, Val RMSE: 0.15760271, Val DA: 0.53225806\n",
      "Epoch [298/1000], Train Loss: 0.02432117, Train MAE: 0.11777902, Train RMSE: 0.15595247, Train DA: 0.63698630, Val Loss: 0.03224183, Val MAE: 0.13938941, Val RMSE: 0.17956010, Val DA: 0.51612903\n",
      "Epoch [299/1000], Train Loss: 0.02359689, Train MAE: 0.11772592, Train RMSE: 0.15361278, Train DA: 0.60958904, Val Loss: 0.03154624, Val MAE: 0.13344485, Val RMSE: 0.17761260, Val DA: 0.56451613\n",
      "Epoch [300/1000], Train Loss: 0.02276514, Train MAE: 0.11780979, Train RMSE: 0.15088122, Train DA: 0.55479452, Val Loss: 0.02744390, Val MAE: 0.12720664, Val RMSE: 0.16566202, Val DA: 0.45161290\n",
      "Epoch [301/1000], Train Loss: 0.02239316, Train MAE: 0.11207061, Train RMSE: 0.14964344, Train DA: 0.58904110, Val Loss: 0.03281480, Val MAE: 0.13671695, Val RMSE: 0.18114856, Val DA: 0.43548387\n",
      "Epoch [302/1000], Train Loss: 0.02367133, Train MAE: 0.11497150, Train RMSE: 0.15385491, Train DA: 0.58219178, Val Loss: 0.04037645, Val MAE: 0.15003975, Val RMSE: 0.20093893, Val DA: 0.48387097\n",
      "Epoch [303/1000], Train Loss: 0.02290089, Train MAE: 0.11628947, Train RMSE: 0.15133041, Train DA: 0.60273973, Val Loss: 0.03461080, Val MAE: 0.13552198, Val RMSE: 0.18603978, Val DA: 0.48387097\n",
      "Epoch [304/1000], Train Loss: 0.02350654, Train MAE: 0.11862039, Train RMSE: 0.15331843, Train DA: 0.58904110, Val Loss: 0.03646404, Val MAE: 0.14054868, Val RMSE: 0.19095558, Val DA: 0.48387097\n",
      "Epoch [305/1000], Train Loss: 0.02341074, Train MAE: 0.11552022, Train RMSE: 0.15300567, Train DA: 0.58219178, Val Loss: 0.04539934, Val MAE: 0.15946540, Val RMSE: 0.21307121, Val DA: 0.46774194\n",
      "Epoch [306/1000], Train Loss: 0.02217821, Train MAE: 0.10821261, Train RMSE: 0.14892349, Train DA: 0.57534247, Val Loss: 0.03896061, Val MAE: 0.14708878, Val RMSE: 0.19738443, Val DA: 0.46774194\n",
      "Epoch [307/1000], Train Loss: 0.02226894, Train MAE: 0.11183100, Train RMSE: 0.14922781, Train DA: 0.58219178, Val Loss: 0.03900687, Val MAE: 0.14526321, Val RMSE: 0.19750157, Val DA: 0.46774194\n",
      "Epoch [308/1000], Train Loss: 0.02824391, Train MAE: 0.12766549, Train RMSE: 0.16805926, Train DA: 0.58904110, Val Loss: 0.05256142, Val MAE: 0.17009129, Val RMSE: 0.22926275, Val DA: 0.43548387\n",
      "Epoch [309/1000], Train Loss: 0.02371148, Train MAE: 0.11385439, Train RMSE: 0.15398534, Train DA: 0.58904110, Val Loss: 0.04744972, Val MAE: 0.16001563, Val RMSE: 0.21782957, Val DA: 0.46774194\n",
      "Epoch [310/1000], Train Loss: 0.02444950, Train MAE: 0.11242151, Train RMSE: 0.15636337, Train DA: 0.54109589, Val Loss: 0.03759303, Val MAE: 0.14535682, Val RMSE: 0.19388922, Val DA: 0.45161290\n",
      "Epoch [311/1000], Train Loss: 0.02012450, Train MAE: 0.10574742, Train RMSE: 0.14186086, Train DA: 0.55479452, Val Loss: 0.03678530, Val MAE: 0.13703419, Val RMSE: 0.19179495, Val DA: 0.41935484\n",
      "Epoch [312/1000], Train Loss: 0.02682192, Train MAE: 0.12644514, Train RMSE: 0.16377400, Train DA: 0.54109589, Val Loss: 0.05511372, Val MAE: 0.17059693, Val RMSE: 0.23476312, Val DA: 0.46774194\n",
      "Epoch [313/1000], Train Loss: 0.02151625, Train MAE: 0.10755098, Train RMSE: 0.14668417, Train DA: 0.57534247, Val Loss: 0.04186860, Val MAE: 0.15018487, Val RMSE: 0.20461819, Val DA: 0.45161290\n",
      "Epoch [314/1000], Train Loss: 0.02182902, Train MAE: 0.10686655, Train RMSE: 0.14774649, Train DA: 0.54109589, Val Loss: 0.03821693, Val MAE: 0.14654255, Val RMSE: 0.19549152, Val DA: 0.45161290\n",
      "Epoch [315/1000], Train Loss: 0.02547651, Train MAE: 0.12227275, Train RMSE: 0.15961364, Train DA: 0.54794521, Val Loss: 0.04941490, Val MAE: 0.16179271, Val RMSE: 0.22229463, Val DA: 0.43548387\n",
      "Epoch [316/1000], Train Loss: 0.02572813, Train MAE: 0.12242375, Train RMSE: 0.16039991, Train DA: 0.56164384, Val Loss: 0.05297288, Val MAE: 0.16732170, Val RMSE: 0.23015837, Val DA: 0.43548387\n",
      "Epoch [317/1000], Train Loss: 0.02341067, Train MAE: 0.11170957, Train RMSE: 0.15300547, Train DA: 0.55479452, Val Loss: 0.04906920, Val MAE: 0.16327439, Val RMSE: 0.22151570, Val DA: 0.45161290\n",
      "Epoch [318/1000], Train Loss: 0.02213461, Train MAE: 0.10759149, Train RMSE: 0.14877704, Train DA: 0.53424658, Val Loss: 0.03978261, Val MAE: 0.14550664, Val RMSE: 0.19945578, Val DA: 0.43548387\n",
      "Epoch [319/1000], Train Loss: 0.02546782, Train MAE: 0.12560089, Train RMSE: 0.15958641, Train DA: 0.57534247, Val Loss: 0.04248688, Val MAE: 0.14798583, Val RMSE: 0.20612347, Val DA: 0.48387097\n",
      "Epoch [320/1000], Train Loss: 0.02907399, Train MAE: 0.13131312, Train RMSE: 0.17051098, Train DA: 0.56164384, Val Loss: 0.06175919, Val MAE: 0.18039295, Val RMSE: 0.24851398, Val DA: 0.48387097\n",
      "Epoch [321/1000], Train Loss: 0.02515917, Train MAE: 0.11539898, Train RMSE: 0.15861642, Train DA: 0.56164384, Val Loss: 0.04385405, Val MAE: 0.15782632, Val RMSE: 0.20941359, Val DA: 0.46774194\n",
      "Epoch [322/1000], Train Loss: 0.02021265, Train MAE: 0.10157663, Train RMSE: 0.14217119, Train DA: 0.54109589, Val Loss: 0.02999810, Val MAE: 0.11896852, Val RMSE: 0.17319961, Val DA: 0.50000000\n",
      "Epoch [323/1000], Train Loss: 0.02831629, Train MAE: 0.13273653, Train RMSE: 0.16827445, Train DA: 0.56849315, Val Loss: 0.05892623, Val MAE: 0.17058997, Val RMSE: 0.24274726, Val DA: 0.43548387\n",
      "Epoch [324/1000], Train Loss: 0.02474223, Train MAE: 0.11928765, Train RMSE: 0.15729664, Train DA: 0.52739726, Val Loss: 0.05647659, Val MAE: 0.18302913, Val RMSE: 0.23764804, Val DA: 0.41935484\n",
      "Epoch [325/1000], Train Loss: 0.02056001, Train MAE: 0.10513863, Train RMSE: 0.14338762, Train DA: 0.54794521, Val Loss: 0.03275147, Val MAE: 0.13381259, Val RMSE: 0.18097368, Val DA: 0.45161290\n",
      "Epoch [326/1000], Train Loss: 0.02637123, Train MAE: 0.13241814, Train RMSE: 0.16239221, Train DA: 0.54794521, Val Loss: 0.03441722, Val MAE: 0.13989717, Val RMSE: 0.18551877, Val DA: 0.43548387\n",
      "Epoch [327/1000], Train Loss: 0.02426141, Train MAE: 0.11861809, Train RMSE: 0.15576075, Train DA: 0.54109589, Val Loss: 0.05894173, Val MAE: 0.17565008, Val RMSE: 0.24277918, Val DA: 0.45161290\n",
      "Epoch [328/1000], Train Loss: 0.02729147, Train MAE: 0.12226264, Train RMSE: 0.16520129, Train DA: 0.55479452, Val Loss: 0.04685831, Val MAE: 0.16878343, Val RMSE: 0.21646781, Val DA: 0.45161290\n",
      "Epoch [329/1000], Train Loss: 0.02008033, Train MAE: 0.10710028, Train RMSE: 0.14170508, Train DA: 0.56164384, Val Loss: 0.02174122, Val MAE: 0.10984252, Val RMSE: 0.14744903, Val DA: 0.45161290\n",
      "Epoch [330/1000], Train Loss: 0.02937876, Train MAE: 0.13941829, Train RMSE: 0.17140234, Train DA: 0.58219178, Val Loss: 0.03206298, Val MAE: 0.14013445, Val RMSE: 0.17906138, Val DA: 0.46774194\n",
      "Epoch [331/1000], Train Loss: 0.02372357, Train MAE: 0.11395907, Train RMSE: 0.15402457, Train DA: 0.54794521, Val Loss: 0.04210919, Val MAE: 0.16289932, Val RMSE: 0.20520525, Val DA: 0.45161290\n",
      "Epoch [332/1000], Train Loss: 0.02214259, Train MAE: 0.10654563, Train RMSE: 0.14880386, Train DA: 0.55479452, Val Loss: 0.03917164, Val MAE: 0.14832418, Val RMSE: 0.19791825, Val DA: 0.41935484\n",
      "Epoch [333/1000], Train Loss: 0.02360087, Train MAE: 0.11793786, Train RMSE: 0.15362574, Train DA: 0.56164384, Val Loss: 0.03984869, Val MAE: 0.14769678, Val RMSE: 0.19962135, Val DA: 0.45161290\n",
      "Epoch [334/1000], Train Loss: 0.02675352, Train MAE: 0.12577577, Train RMSE: 0.16356504, Train DA: 0.56164384, Val Loss: 0.04232018, Val MAE: 0.15493843, Val RMSE: 0.20571870, Val DA: 0.50000000\n",
      "Epoch [335/1000], Train Loss: 0.02466174, Train MAE: 0.11695745, Train RMSE: 0.15704057, Train DA: 0.55479452, Val Loss: 0.04142990, Val MAE: 0.15650108, Val RMSE: 0.20354337, Val DA: 0.43548387\n",
      "Epoch [336/1000], Train Loss: 0.02019226, Train MAE: 0.10472557, Train RMSE: 0.14209945, Train DA: 0.56849315, Val Loss: 0.03557234, Val MAE: 0.13679643, Val RMSE: 0.18860632, Val DA: 0.43548387\n",
      "Epoch [337/1000], Train Loss: 0.02440805, Train MAE: 0.12245768, Train RMSE: 0.15623076, Train DA: 0.55479452, Val Loss: 0.04795056, Val MAE: 0.16130920, Val RMSE: 0.21897617, Val DA: 0.48387097\n",
      "Epoch [338/1000], Train Loss: 0.02331090, Train MAE: 0.11388882, Train RMSE: 0.15267909, Train DA: 0.56849315, Val Loss: 0.05225309, Val MAE: 0.16719735, Val RMSE: 0.22858934, Val DA: 0.41935484\n",
      "Epoch [339/1000], Train Loss: 0.02530906, Train MAE: 0.11538429, Train RMSE: 0.15908821, Train DA: 0.53424658, Val Loss: 0.04358350, Val MAE: 0.15891738, Val RMSE: 0.20876661, Val DA: 0.45161290\n",
      "Epoch [340/1000], Train Loss: 0.01992641, Train MAE: 0.10374036, Train RMSE: 0.14116095, Train DA: 0.53424658, Val Loss: 0.03588618, Val MAE: 0.13134466, Val RMSE: 0.18943647, Val DA: 0.45161290\n",
      "Epoch [341/1000], Train Loss: 0.02372781, Train MAE: 0.11667114, Train RMSE: 0.15403834, Train DA: 0.56849315, Val Loss: 0.04848220, Val MAE: 0.15803818, Val RMSE: 0.22018676, Val DA: 0.43548387\n",
      "Epoch [342/1000], Train Loss: 0.02431768, Train MAE: 0.11581735, Train RMSE: 0.15594128, Train DA: 0.52739726, Val Loss: 0.05468721, Val MAE: 0.17881311, Val RMSE: 0.23385297, Val DA: 0.40322581\n",
      "Epoch [343/1000], Train Loss: 0.02124551, Train MAE: 0.10677744, Train RMSE: 0.14575839, Train DA: 0.58904110, Val Loss: 0.03901868, Val MAE: 0.14264497, Val RMSE: 0.19753148, Val DA: 0.51612903\n",
      "Epoch [344/1000], Train Loss: 0.02720133, Train MAE: 0.13109751, Train RMSE: 0.16492826, Train DA: 0.58219178, Val Loss: 0.04248705, Val MAE: 0.14765315, Val RMSE: 0.20612384, Val DA: 0.51612903\n",
      "Epoch [345/1000], Train Loss: 0.02208300, Train MAE: 0.10949206, Train RMSE: 0.14860348, Train DA: 0.58904110, Val Loss: 0.05012969, Val MAE: 0.16200432, Val RMSE: 0.22389661, Val DA: 0.50000000\n",
      "Epoch [346/1000], Train Loss: 0.02272307, Train MAE: 0.10813379, Train RMSE: 0.15074174, Train DA: 0.59589041, Val Loss: 0.05052507, Val MAE: 0.16581973, Val RMSE: 0.22477783, Val DA: 0.50000000\n",
      "Epoch [347/1000], Train Loss: 0.02109796, Train MAE: 0.10811260, Train RMSE: 0.14525135, Train DA: 0.55479452, Val Loss: 0.04058839, Val MAE: 0.14016473, Val RMSE: 0.20146562, Val DA: 0.48387097\n",
      "Epoch [348/1000], Train Loss: 0.02382995, Train MAE: 0.11815015, Train RMSE: 0.15436952, Train DA: 0.58904110, Val Loss: 0.04364427, Val MAE: 0.15138224, Val RMSE: 0.20891209, Val DA: 0.50000000\n",
      "Epoch [349/1000], Train Loss: 0.02310471, Train MAE: 0.11232243, Train RMSE: 0.15200233, Train DA: 0.57534247, Val Loss: 0.04742362, Val MAE: 0.16461891, Val RMSE: 0.21776965, Val DA: 0.46774194\n",
      "Epoch [350/1000], Train Loss: 0.02061395, Train MAE: 0.10569420, Train RMSE: 0.14357556, Train DA: 0.55479452, Val Loss: 0.04179565, Val MAE: 0.14664680, Val RMSE: 0.20443983, Val DA: 0.41935484\n",
      "Epoch [351/1000], Train Loss: 0.02359958, Train MAE: 0.11660463, Train RMSE: 0.15362154, Train DA: 0.57534247, Val Loss: 0.04444433, Val MAE: 0.15252306, Val RMSE: 0.21081823, Val DA: 0.43548387\n",
      "Epoch [352/1000], Train Loss: 0.02331828, Train MAE: 0.11422011, Train RMSE: 0.15270324, Train DA: 0.56164384, Val Loss: 0.04719335, Val MAE: 0.16058768, Val RMSE: 0.21724032, Val DA: 0.43548387\n",
      "Epoch [353/1000], Train Loss: 0.02219938, Train MAE: 0.10909297, Train RMSE: 0.14899455, Train DA: 0.55479452, Val Loss: 0.04591451, Val MAE: 0.15641738, Val RMSE: 0.21427670, Val DA: 0.41935484\n",
      "Epoch [354/1000], Train Loss: 0.02193513, Train MAE: 0.10995901, Train RMSE: 0.14810511, Train DA: 0.58219178, Val Loss: 0.04327306, Val MAE: 0.15166454, Val RMSE: 0.20802179, Val DA: 0.41935484\n",
      "Epoch [355/1000], Train Loss: 0.02367486, Train MAE: 0.11528334, Train RMSE: 0.15386637, Train DA: 0.57534247, Val Loss: 0.04790595, Val MAE: 0.16135202, Val RMSE: 0.21887426, Val DA: 0.43548387\n",
      "Epoch [356/1000], Train Loss: 0.02288128, Train MAE: 0.11088226, Train RMSE: 0.15126559, Train DA: 0.56849315, Val Loss: 0.04887229, Val MAE: 0.16312414, Val RMSE: 0.22107078, Val DA: 0.43548387\n",
      "Epoch [357/1000], Train Loss: 0.02135410, Train MAE: 0.10597297, Train RMSE: 0.14613044, Train DA: 0.56164384, Val Loss: 0.04547105, Val MAE: 0.15467922, Val RMSE: 0.21323942, Val DA: 0.40322581\n",
      "Epoch [358/1000], Train Loss: 0.02237903, Train MAE: 0.10962222, Train RMSE: 0.14959621, Train DA: 0.58219178, Val Loss: 0.04721342, Val MAE: 0.15959689, Val RMSE: 0.21728650, Val DA: 0.38709677\n",
      "Epoch [359/1000], Train Loss: 0.02413582, Train MAE: 0.11394586, Train RMSE: 0.15535708, Train DA: 0.58219178, Val Loss: 0.05111921, Val MAE: 0.16777031, Val RMSE: 0.22609557, Val DA: 0.40322581\n",
      "Epoch [360/1000], Train Loss: 0.02282462, Train MAE: 0.10952031, Train RMSE: 0.15107818, Train DA: 0.56849315, Val Loss: 0.05124601, Val MAE: 0.16790484, Val RMSE: 0.22637582, Val DA: 0.40322581\n",
      "Epoch [361/1000], Train Loss: 0.02143881, Train MAE: 0.10588332, Train RMSE: 0.14641999, Train DA: 0.58904110, Val Loss: 0.04910621, Val MAE: 0.16269781, Val RMSE: 0.22159921, Val DA: 0.41935484\n",
      "Epoch [362/1000], Train Loss: 0.02463875, Train MAE: 0.11799803, Train RMSE: 0.15696736, Train DA: 0.59589041, Val Loss: 0.05086421, Val MAE: 0.16776474, Val RMSE: 0.22553097, Val DA: 0.40322581\n",
      "Epoch [363/1000], Train Loss: 0.02637103, Train MAE: 0.12259029, Train RMSE: 0.16239162, Train DA: 0.58219178, Val Loss: 0.05215445, Val MAE: 0.17012770, Val RMSE: 0.22837348, Val DA: 0.40322581\n",
      "Epoch [364/1000], Train Loss: 0.02550628, Train MAE: 0.11726000, Train RMSE: 0.15970685, Train DA: 0.55479452, Val Loss: 0.04853948, Val MAE: 0.16349238, Val RMSE: 0.22031678, Val DA: 0.41935484\n",
      "Epoch [365/1000], Train Loss: 0.02181284, Train MAE: 0.10520016, Train RMSE: 0.14769171, Train DA: 0.60958904, Val Loss: 0.04222901, Val MAE: 0.15021433, Val RMSE: 0.20549698, Val DA: 0.40322581\n",
      "Epoch [366/1000], Train Loss: 0.02991711, Train MAE: 0.13787422, Train RMSE: 0.17296565, Train DA: 0.58904110, Val Loss: 0.05858496, Val MAE: 0.17575358, Val RMSE: 0.24204329, Val DA: 0.41935484\n",
      "Epoch [367/1000], Train Loss: 0.02592103, Train MAE: 0.12236856, Train RMSE: 0.16100009, Train DA: 0.57534247, Val Loss: 0.05958207, Val MAE: 0.17377147, Val RMSE: 0.24409437, Val DA: 0.41935484\n",
      "Epoch [368/1000], Train Loss: 0.03007851, Train MAE: 0.12991281, Train RMSE: 0.17343158, Train DA: 0.53424658, Val Loss: 0.04971262, Val MAE: 0.17279693, Val RMSE: 0.22296327, Val DA: 0.45161290\n",
      "Epoch [369/1000], Train Loss: 0.01941437, Train MAE: 0.10138969, Train RMSE: 0.13933545, Train DA: 0.53424658, Val Loss: 0.02708841, Val MAE: 0.11433069, Val RMSE: 0.16458558, Val DA: 0.46774194\n",
      "Epoch [370/1000], Train Loss: 0.02247514, Train MAE: 0.11647589, Train RMSE: 0.14991710, Train DA: 0.54109589, Val Loss: 0.05185057, Val MAE: 0.15488333, Val RMSE: 0.22770719, Val DA: 0.43548387\n",
      "Epoch [371/1000], Train Loss: 0.02546327, Train MAE: 0.11825201, Train RMSE: 0.15957214, Train DA: 0.54109589, Val Loss: 0.06851944, Val MAE: 0.19460973, Val RMSE: 0.26176217, Val DA: 0.41935484\n",
      "Epoch [372/1000], Train Loss: 0.02201115, Train MAE: 0.10659648, Train RMSE: 0.14836155, Train DA: 0.57534247, Val Loss: 0.04516406, Val MAE: 0.16104108, Val RMSE: 0.21251838, Val DA: 0.41935484\n",
      "Epoch [373/1000], Train Loss: 0.02501530, Train MAE: 0.12576795, Train RMSE: 0.15816228, Train DA: 0.56164384, Val Loss: 0.03499985, Val MAE: 0.13914897, Val RMSE: 0.18708248, Val DA: 0.43548387\n",
      "Epoch [374/1000], Train Loss: 0.02256701, Train MAE: 0.11188933, Train RMSE: 0.15022318, Train DA: 0.57534247, Val Loss: 0.04579832, Val MAE: 0.16057888, Val RMSE: 0.21400541, Val DA: 0.41935484\n",
      "Epoch [375/1000], Train Loss: 0.02911318, Train MAE: 0.12749779, Train RMSE: 0.17062585, Train DA: 0.54109589, Val Loss: 0.05743412, Val MAE: 0.18629038, Val RMSE: 0.23965415, Val DA: 0.46774194\n",
      "Epoch [376/1000], Train Loss: 0.02499217, Train MAE: 0.12733634, Train RMSE: 0.15808910, Train DA: 0.55479452, Val Loss: 0.03488788, Val MAE: 0.14232400, Val RMSE: 0.18678297, Val DA: 0.41935484\n",
      "Epoch [377/1000], Train Loss: 0.02253508, Train MAE: 0.11567319, Train RMSE: 0.15011688, Train DA: 0.50684932, Val Loss: 0.03310641, Val MAE: 0.13766310, Val RMSE: 0.18195167, Val DA: 0.45161290\n",
      "Epoch [378/1000], Train Loss: 0.02736550, Train MAE: 0.12445159, Train RMSE: 0.16542523, Train DA: 0.53424658, Val Loss: 0.03818977, Val MAE: 0.15240479, Val RMSE: 0.19542202, Val DA: 0.51612903\n",
      "Epoch [379/1000], Train Loss: 0.02140593, Train MAE: 0.10766059, Train RMSE: 0.14630766, Train DA: 0.58904110, Val Loss: 0.03232699, Val MAE: 0.12662022, Val RMSE: 0.17979708, Val DA: 0.51612903\n",
      "Epoch [380/1000], Train Loss: 0.02567611, Train MAE: 0.12967540, Train RMSE: 0.16023767, Train DA: 0.54794521, Val Loss: 0.03038166, Val MAE: 0.13643320, Val RMSE: 0.17430335, Val DA: 0.54838710\n",
      "Epoch [381/1000], Train Loss: 0.02094973, Train MAE: 0.10652866, Train RMSE: 0.14474019, Train DA: 0.52739726, Val Loss: 0.03459973, Val MAE: 0.13878918, Val RMSE: 0.18601002, Val DA: 0.54838710\n",
      "Epoch [382/1000], Train Loss: 0.02151095, Train MAE: 0.10569911, Train RMSE: 0.14666612, Train DA: 0.57534247, Val Loss: 0.03414420, Val MAE: 0.13763274, Val RMSE: 0.18478148, Val DA: 0.54838710\n",
      "Epoch [383/1000], Train Loss: 0.02102130, Train MAE: 0.11119840, Train RMSE: 0.14498726, Train DA: 0.58904110, Val Loss: 0.02760481, Val MAE: 0.12604524, Val RMSE: 0.16614693, Val DA: 0.51612903\n",
      "Epoch [384/1000], Train Loss: 0.02244744, Train MAE: 0.11415689, Train RMSE: 0.14982469, Train DA: 0.58219178, Val Loss: 0.03286615, Val MAE: 0.13737652, Val RMSE: 0.18129025, Val DA: 0.48387097\n",
      "Epoch [385/1000], Train Loss: 0.02194642, Train MAE: 0.10793918, Train RMSE: 0.14814326, Train DA: 0.56164384, Val Loss: 0.03930805, Val MAE: 0.14863996, Val RMSE: 0.19826260, Val DA: 0.41935484\n",
      "Epoch [386/1000], Train Loss: 0.02086702, Train MAE: 0.10549627, Train RMSE: 0.14445420, Train DA: 0.56849315, Val Loss: 0.03353225, Val MAE: 0.13764586, Val RMSE: 0.18311812, Val DA: 0.50000000\n",
      "Epoch [387/1000], Train Loss: 0.02207155, Train MAE: 0.11044570, Train RMSE: 0.14856496, Train DA: 0.56164384, Val Loss: 0.04059792, Val MAE: 0.14886580, Val RMSE: 0.20148925, Val DA: 0.46774194\n",
      "Epoch [388/1000], Train Loss: 0.02207614, Train MAE: 0.10912463, Train RMSE: 0.14858042, Train DA: 0.56164384, Val Loss: 0.04164298, Val MAE: 0.14944397, Val RMSE: 0.20406614, Val DA: 0.48387097\n",
      "Epoch [389/1000], Train Loss: 0.02170626, Train MAE: 0.10843082, Train RMSE: 0.14733045, Train DA: 0.56849315, Val Loss: 0.04145439, Val MAE: 0.14817466, Val RMSE: 0.20360352, Val DA: 0.48387097\n",
      "Epoch [390/1000], Train Loss: 0.02235001, Train MAE: 0.11061499, Train RMSE: 0.14949919, Train DA: 0.56164384, Val Loss: 0.04390257, Val MAE: 0.15262645, Val RMSE: 0.20952940, Val DA: 0.48387097\n",
      "Epoch [391/1000], Train Loss: 0.02311868, Train MAE: 0.11155146, Train RMSE: 0.15204827, Train DA: 0.56164384, Val Loss: 0.04561276, Val MAE: 0.15707950, Val RMSE: 0.21357144, Val DA: 0.48387097\n",
      "Epoch [392/1000], Train Loss: 0.02252393, Train MAE: 0.10859678, Train RMSE: 0.15007974, Train DA: 0.56849315, Val Loss: 0.04245841, Val MAE: 0.15097266, Val RMSE: 0.20605437, Val DA: 0.46774194\n",
      "Epoch [393/1000], Train Loss: 0.02273526, Train MAE: 0.10965942, Train RMSE: 0.15078217, Train DA: 0.56849315, Val Loss: 0.04219817, Val MAE: 0.15089650, Val RMSE: 0.20542192, Val DA: 0.50000000\n",
      "Epoch [394/1000], Train Loss: 0.02481414, Train MAE: 0.11709229, Train RMSE: 0.15752505, Train DA: 0.56849315, Val Loss: 0.04542939, Val MAE: 0.15854658, Val RMSE: 0.21314171, Val DA: 0.53225806\n",
      "Epoch [395/1000], Train Loss: 0.02406147, Train MAE: 0.11309552, Train RMSE: 0.15511759, Train DA: 0.54109589, Val Loss: 0.04524503, Val MAE: 0.15932788, Val RMSE: 0.21270877, Val DA: 0.50000000\n",
      "Epoch [396/1000], Train Loss: 0.02334789, Train MAE: 0.10926196, Train RMSE: 0.15280017, Train DA: 0.57534247, Val Loss: 0.04237041, Val MAE: 0.15238135, Val RMSE: 0.20584072, Val DA: 0.48387097\n",
      "Epoch [397/1000], Train Loss: 0.02354048, Train MAE: 0.11373907, Train RMSE: 0.15342908, Train DA: 0.56164384, Val Loss: 0.04217171, Val MAE: 0.15175381, Val RMSE: 0.20535752, Val DA: 0.50000000\n",
      "Epoch [398/1000], Train Loss: 0.02705693, Train MAE: 0.12673430, Train RMSE: 0.16448991, Train DA: 0.56164384, Val Loss: 0.04909009, Val MAE: 0.16586781, Val RMSE: 0.22156283, Val DA: 0.50000000\n",
      "Epoch [399/1000], Train Loss: 0.02579419, Train MAE: 0.11738096, Train RMSE: 0.16060568, Train DA: 0.54794521, Val Loss: 0.04619776, Val MAE: 0.16338591, Val RMSE: 0.21493663, Val DA: 0.43548387\n",
      "Epoch [400/1000], Train Loss: 0.02282872, Train MAE: 0.10719596, Train RMSE: 0.15109177, Train DA: 0.58904110, Val Loss: 0.03745263, Val MAE: 0.14185731, Val RMSE: 0.19352683, Val DA: 0.50000000\n",
      "Epoch [401/1000], Train Loss: 0.02647732, Train MAE: 0.12824567, Train RMSE: 0.16271852, Train DA: 0.58219178, Val Loss: 0.04296847, Val MAE: 0.15520428, Val RMSE: 0.20728840, Val DA: 0.45161290\n",
      "Epoch [402/1000], Train Loss: 0.02476653, Train MAE: 0.11711641, Train RMSE: 0.15737385, Train DA: 0.54794521, Val Loss: 0.04908638, Val MAE: 0.16337743, Val RMSE: 0.22155446, Val DA: 0.45161290\n",
      "Epoch [403/1000], Train Loss: 0.02583373, Train MAE: 0.11651949, Train RMSE: 0.16072875, Train DA: 0.56849315, Val Loss: 0.04119381, Val MAE: 0.15634836, Val RMSE: 0.20296258, Val DA: 0.43548387\n",
      "Epoch [404/1000], Train Loss: 0.02084167, Train MAE: 0.10607933, Train RMSE: 0.14436643, Train DA: 0.56849315, Val Loss: 0.03661349, Val MAE: 0.13983904, Val RMSE: 0.19134654, Val DA: 0.46774194\n",
      "Epoch [405/1000], Train Loss: 0.02498423, Train MAE: 0.12299497, Train RMSE: 0.15806402, Train DA: 0.56849315, Val Loss: 0.04543233, Val MAE: 0.16225353, Val RMSE: 0.21314862, Val DA: 0.46774194\n",
      "Epoch [406/1000], Train Loss: 0.02337487, Train MAE: 0.11194525, Train RMSE: 0.15288845, Train DA: 0.55479452, Val Loss: 0.04783110, Val MAE: 0.16433659, Val RMSE: 0.21870323, Val DA: 0.50000000\n",
      "Epoch [407/1000], Train Loss: 0.02245275, Train MAE: 0.10709845, Train RMSE: 0.14984241, Train DA: 0.57534247, Val Loss: 0.03296110, Val MAE: 0.13706131, Val RMSE: 0.18155192, Val DA: 0.46774194\n",
      "Epoch [408/1000], Train Loss: 0.02258892, Train MAE: 0.11197495, Train RMSE: 0.15029611, Train DA: 0.58904110, Val Loss: 0.03389332, Val MAE: 0.13941018, Val RMSE: 0.18410140, Val DA: 0.50000000\n",
      "Epoch [409/1000], Train Loss: 0.02458662, Train MAE: 0.11624520, Train RMSE: 0.15680122, Train DA: 0.54109589, Val Loss: 0.04746153, Val MAE: 0.16348581, Val RMSE: 0.21785669, Val DA: 0.50000000\n",
      "Epoch [410/1000], Train Loss: 0.02443780, Train MAE: 0.11255875, Train RMSE: 0.15632594, Train DA: 0.55479452, Val Loss: 0.04658481, Val MAE: 0.16436377, Val RMSE: 0.21583514, Val DA: 0.46774194\n",
      "Epoch [411/1000], Train Loss: 0.02201764, Train MAE: 0.10650504, Train RMSE: 0.14838342, Train DA: 0.55479452, Val Loss: 0.03630023, Val MAE: 0.14594774, Val RMSE: 0.19052619, Val DA: 0.48387097\n",
      "Epoch [412/1000], Train Loss: 0.02444498, Train MAE: 0.11816105, Train RMSE: 0.15634890, Train DA: 0.54109589, Val Loss: 0.04477744, Val MAE: 0.16096281, Val RMSE: 0.21160680, Val DA: 0.48387097\n",
      "Epoch [413/1000], Train Loss: 0.02415614, Train MAE: 0.11332886, Train RMSE: 0.15542246, Train DA: 0.54794521, Val Loss: 0.04738326, Val MAE: 0.16529526, Val RMSE: 0.21767698, Val DA: 0.51612903\n",
      "Epoch [414/1000], Train Loss: 0.02367848, Train MAE: 0.11021642, Train RMSE: 0.15387814, Train DA: 0.55479452, Val Loss: 0.04247430, Val MAE: 0.15565358, Val RMSE: 0.20609294, Val DA: 0.50000000\n",
      "Epoch [415/1000], Train Loss: 0.02321264, Train MAE: 0.11082919, Train RMSE: 0.15235695, Train DA: 0.55479452, Val Loss: 0.04195356, Val MAE: 0.15376966, Val RMSE: 0.20482570, Val DA: 0.50000000\n",
      "Epoch [416/1000], Train Loss: 0.02469666, Train MAE: 0.11580688, Train RMSE: 0.15715170, Train DA: 0.54109589, Val Loss: 0.04535908, Val MAE: 0.16212146, Val RMSE: 0.21297671, Val DA: 0.51612903\n",
      "Epoch [417/1000], Train Loss: 0.02421145, Train MAE: 0.11228231, Train RMSE: 0.15560029, Train DA: 0.54794521, Val Loss: 0.04515420, Val MAE: 0.16153570, Val RMSE: 0.21249521, Val DA: 0.50000000\n",
      "Epoch [418/1000], Train Loss: 0.02364265, Train MAE: 0.10992055, Train RMSE: 0.15376168, Train DA: 0.56164384, Val Loss: 0.04206614, Val MAE: 0.15547103, Val RMSE: 0.20510031, Val DA: 0.48387097\n",
      "Epoch [419/1000], Train Loss: 0.02415330, Train MAE: 0.11307709, Train RMSE: 0.15541333, Train DA: 0.55479452, Val Loss: 0.04383651, Val MAE: 0.15925530, Val RMSE: 0.20937170, Val DA: 0.50000000\n",
      "Epoch [420/1000], Train Loss: 0.02453913, Train MAE: 0.11386567, Train RMSE: 0.15664969, Train DA: 0.55479452, Val Loss: 0.04545197, Val MAE: 0.16244856, Val RMSE: 0.21319470, Val DA: 0.50000000\n",
      "Epoch [421/1000], Train Loss: 0.02421218, Train MAE: 0.11118453, Train RMSE: 0.15560263, Train DA: 0.55479452, Val Loss: 0.04364125, Val MAE: 0.15918934, Val RMSE: 0.20890486, Val DA: 0.50000000\n",
      "Epoch [422/1000], Train Loss: 0.02385773, Train MAE: 0.10994321, Train RMSE: 0.15445949, Train DA: 0.56849315, Val Loss: 0.04157778, Val MAE: 0.15627161, Val RMSE: 0.20390630, Val DA: 0.50000000\n",
      "Epoch [423/1000], Train Loss: 0.02481828, Train MAE: 0.11578432, Train RMSE: 0.15753821, Train DA: 0.56164384, Val Loss: 0.04504537, Val MAE: 0.16286694, Val RMSE: 0.21223895, Val DA: 0.46774194\n",
      "Epoch [424/1000], Train Loss: 0.02463914, Train MAE: 0.11348892, Train RMSE: 0.15696858, Train DA: 0.54794521, Val Loss: 0.04495188, Val MAE: 0.16156760, Val RMSE: 0.21201856, Val DA: 0.50000000\n",
      "Epoch [425/1000], Train Loss: 0.02467143, Train MAE: 0.11186153, Train RMSE: 0.15707141, Train DA: 0.56849315, Val Loss: 0.04352817, Val MAE: 0.15767287, Val RMSE: 0.20863408, Val DA: 0.46774194\n",
      "Epoch [426/1000], Train Loss: 0.02407428, Train MAE: 0.11019158, Train RMSE: 0.15515891, Train DA: 0.57534247, Val Loss: 0.04256598, Val MAE: 0.15697211, Val RMSE: 0.20631522, Val DA: 0.50000000\n",
      "Epoch [427/1000], Train Loss: 0.02536105, Train MAE: 0.11851317, Train RMSE: 0.15925153, Train DA: 0.55479452, Val Loss: 0.04487126, Val MAE: 0.16334216, Val RMSE: 0.21182837, Val DA: 0.46774194\n",
      "Epoch [428/1000], Train Loss: 0.02487270, Train MAE: 0.11430763, Train RMSE: 0.15771081, Train DA: 0.56164384, Val Loss: 0.04521749, Val MAE: 0.16231281, Val RMSE: 0.21264407, Val DA: 0.50000000\n",
      "Epoch [429/1000], Train Loss: 0.02566619, Train MAE: 0.11448484, Train RMSE: 0.16020672, Train DA: 0.57534247, Val Loss: 0.04371510, Val MAE: 0.15805399, Val RMSE: 0.20908158, Val DA: 0.46774194\n",
      "Epoch [430/1000], Train Loss: 0.02359854, Train MAE: 0.10957028, Train RMSE: 0.15361817, Train DA: 0.56849315, Val Loss: 0.04207246, Val MAE: 0.15414159, Val RMSE: 0.20511571, Val DA: 0.50000000\n",
      "Epoch [431/1000], Train Loss: 0.02652010, Train MAE: 0.12408131, Train RMSE: 0.16284993, Train DA: 0.54794521, Val Loss: 0.04590326, Val MAE: 0.16635881, Val RMSE: 0.21425046, Val DA: 0.50000000\n",
      "Epoch [432/1000], Train Loss: 0.02567368, Train MAE: 0.11843386, Train RMSE: 0.16023009, Train DA: 0.54109589, Val Loss: 0.04625608, Val MAE: 0.16478571, Val RMSE: 0.21507227, Val DA: 0.48387097\n",
      "Epoch [433/1000], Train Loss: 0.02700729, Train MAE: 0.11859463, Train RMSE: 0.16433896, Train DA: 0.57534247, Val Loss: 0.04082938, Val MAE: 0.15236036, Val RMSE: 0.20206279, Val DA: 0.43548387\n",
      "Epoch [434/1000], Train Loss: 0.02282130, Train MAE: 0.10799219, Train RMSE: 0.15106720, Train DA: 0.57534247, Val Loss: 0.04031631, Val MAE: 0.14688228, Val RMSE: 0.20078923, Val DA: 0.43548387\n",
      "Epoch [435/1000], Train Loss: 0.02617592, Train MAE: 0.12352916, Train RMSE: 0.16178975, Train DA: 0.54794521, Val Loss: 0.04540034, Val MAE: 0.16302006, Val RMSE: 0.21307355, Val DA: 0.46774194\n",
      "Epoch [436/1000], Train Loss: 0.02592855, Train MAE: 0.11756867, Train RMSE: 0.16102344, Train DA: 0.55479452, Val Loss: 0.04340943, Val MAE: 0.16558817, Val RMSE: 0.20834927, Val DA: 0.45161290\n",
      "Epoch [437/1000], Train Loss: 0.02713596, Train MAE: 0.11855206, Train RMSE: 0.16472997, Train DA: 0.56849315, Val Loss: 0.03882750, Val MAE: 0.15156838, Val RMSE: 0.19704697, Val DA: 0.46774194\n",
      "Epoch [438/1000], Train Loss: 0.02272587, Train MAE: 0.10949957, Train RMSE: 0.15075102, Train DA: 0.56849315, Val Loss: 0.03898914, Val MAE: 0.14631149, Val RMSE: 0.19745666, Val DA: 0.48387097\n",
      "Epoch [439/1000], Train Loss: 0.02635164, Train MAE: 0.12394595, Train RMSE: 0.16233188, Train DA: 0.54794521, Val Loss: 0.04604130, Val MAE: 0.16465744, Val RMSE: 0.21457236, Val DA: 0.46774194\n",
      "Epoch [440/1000], Train Loss: 0.02552906, Train MAE: 0.11623381, Train RMSE: 0.15977816, Train DA: 0.55479452, Val Loss: 0.04463648, Val MAE: 0.16718379, Val RMSE: 0.21127349, Val DA: 0.43548387\n",
      "Epoch [441/1000], Train Loss: 0.02654116, Train MAE: 0.11698359, Train RMSE: 0.16291459, Train DA: 0.57534247, Val Loss: 0.03850935, Val MAE: 0.15343012, Val RMSE: 0.19623800, Val DA: 0.48387097\n",
      "Epoch [442/1000], Train Loss: 0.02277678, Train MAE: 0.10960500, Train RMSE: 0.15091977, Train DA: 0.57534247, Val Loss: 0.03747382, Val MAE: 0.14426462, Val RMSE: 0.19358157, Val DA: 0.46774194\n",
      "Epoch [443/1000], Train Loss: 0.02672096, Train MAE: 0.12489436, Train RMSE: 0.16346547, Train DA: 0.55479452, Val Loss: 0.04553138, Val MAE: 0.16598968, Val RMSE: 0.21338083, Val DA: 0.50000000\n",
      "Epoch [444/1000], Train Loss: 0.02568743, Train MAE: 0.11645212, Train RMSE: 0.16027297, Train DA: 0.56849315, Val Loss: 0.04586378, Val MAE: 0.16915202, Val RMSE: 0.21415834, Val DA: 0.45161290\n",
      "Epoch [445/1000], Train Loss: 0.02571577, Train MAE: 0.11449933, Train RMSE: 0.16036135, Train DA: 0.58219178, Val Loss: 0.03819481, Val MAE: 0.15171997, Val RMSE: 0.19543491, Val DA: 0.48387097\n",
      "Epoch [446/1000], Train Loss: 0.02350247, Train MAE: 0.11199103, Train RMSE: 0.15330517, Train DA: 0.58219178, Val Loss: 0.04048055, Val MAE: 0.15228225, Val RMSE: 0.20119779, Val DA: 0.50000000\n",
      "Epoch [447/1000], Train Loss: 0.02591601, Train MAE: 0.12059054, Train RMSE: 0.16098450, Train DA: 0.55479452, Val Loss: 0.04478289, Val MAE: 0.16507919, Val RMSE: 0.21161968, Val DA: 0.51612903\n",
      "Epoch [448/1000], Train Loss: 0.02602551, Train MAE: 0.11628851, Train RMSE: 0.16132425, Train DA: 0.58219178, Val Loss: 0.04603392, Val MAE: 0.16731834, Val RMSE: 0.21455517, Val DA: 0.48387097\n",
      "Epoch [449/1000], Train Loss: 0.02551847, Train MAE: 0.11327327, Train RMSE: 0.15974501, Train DA: 0.58904110, Val Loss: 0.03960328, Val MAE: 0.15751868, Val RMSE: 0.19900574, Val DA: 0.46774194\n",
      "Epoch [450/1000], Train Loss: 0.02498551, Train MAE: 0.11566544, Train RMSE: 0.15806806, Train DA: 0.57534247, Val Loss: 0.04417498, Val MAE: 0.16276635, Val RMSE: 0.21017846, Val DA: 0.46774194\n",
      "Epoch [451/1000], Train Loss: 0.02480999, Train MAE: 0.11336987, Train RMSE: 0.15751189, Train DA: 0.57534247, Val Loss: 0.04381881, Val MAE: 0.16257721, Val RMSE: 0.20932944, Val DA: 0.48387097\n",
      "Epoch [452/1000], Train Loss: 0.02518811, Train MAE: 0.11268657, Train RMSE: 0.15870763, Train DA: 0.59589041, Val Loss: 0.04343908, Val MAE: 0.16234361, Val RMSE: 0.20842044, Val DA: 0.48387097\n",
      "Epoch [453/1000], Train Loss: 0.02542665, Train MAE: 0.11303233, Train RMSE: 0.15945736, Train DA: 0.58904110, Val Loss: 0.04379301, Val MAE: 0.16407207, Val RMSE: 0.20926780, Val DA: 0.48387097\n",
      "Epoch [454/1000], Train Loss: 0.02573436, Train MAE: 0.11550158, Train RMSE: 0.16041933, Train DA: 0.58219178, Val Loss: 0.04598784, Val MAE: 0.16743617, Val RMSE: 0.21444774, Val DA: 0.46774194\n",
      "Epoch [455/1000], Train Loss: 0.02560821, Train MAE: 0.11390074, Train RMSE: 0.16002569, Train DA: 0.58219178, Val Loss: 0.04545905, Val MAE: 0.16576914, Val RMSE: 0.21321128, Val DA: 0.48387097\n",
      "Epoch [456/1000], Train Loss: 0.02554734, Train MAE: 0.11323374, Train RMSE: 0.15983535, Train DA: 0.58219178, Val Loss: 0.04531693, Val MAE: 0.16490898, Val RMSE: 0.21287775, Val DA: 0.48387097\n",
      "Epoch [457/1000], Train Loss: 0.02560985, Train MAE: 0.11328784, Train RMSE: 0.16003077, Train DA: 0.57534247, Val Loss: 0.04597381, Val MAE: 0.16641596, Val RMSE: 0.21441506, Val DA: 0.48387097\n",
      "Epoch [458/1000], Train Loss: 0.02592070, Train MAE: 0.11441951, Train RMSE: 0.16099909, Train DA: 0.58219178, Val Loss: 0.04653737, Val MAE: 0.16884424, Val RMSE: 0.21572523, Val DA: 0.48387097\n",
      "Epoch [459/1000], Train Loss: 0.02600211, Train MAE: 0.11465039, Train RMSE: 0.16125168, Train DA: 0.57534247, Val Loss: 0.04624565, Val MAE: 0.16823989, Val RMSE: 0.21504803, Val DA: 0.48387097\n",
      "Epoch [460/1000], Train Loss: 0.02595390, Train MAE: 0.11446191, Train RMSE: 0.16110215, Train DA: 0.58219178, Val Loss: 0.04589464, Val MAE: 0.16658300, Val RMSE: 0.21423036, Val DA: 0.48387097\n",
      "Epoch [461/1000], Train Loss: 0.02604511, Train MAE: 0.11409032, Train RMSE: 0.16138498, Train DA: 0.58219178, Val Loss: 0.04595993, Val MAE: 0.16702718, Val RMSE: 0.21438266, Val DA: 0.48387097\n",
      "Epoch [462/1000], Train Loss: 0.02606098, Train MAE: 0.11500819, Train RMSE: 0.16143413, Train DA: 0.58219178, Val Loss: 0.04602654, Val MAE: 0.16868907, Val RMSE: 0.21453796, Val DA: 0.48387097\n",
      "Epoch [463/1000], Train Loss: 0.02664721, Train MAE: 0.11787180, Train RMSE: 0.16323975, Train DA: 0.58219178, Val Loss: 0.04683606, Val MAE: 0.17010672, Val RMSE: 0.21641642, Val DA: 0.48387097\n",
      "Epoch [464/1000], Train Loss: 0.02642368, Train MAE: 0.11616602, Train RMSE: 0.16255362, Train DA: 0.57534247, Val Loss: 0.04629588, Val MAE: 0.16703092, Val RMSE: 0.21516478, Val DA: 0.48387097\n",
      "Epoch [465/1000], Train Loss: 0.02654672, Train MAE: 0.11498425, Train RMSE: 0.16293165, Train DA: 0.57534247, Val Loss: 0.04545670, Val MAE: 0.16538814, Val RMSE: 0.21320575, Val DA: 0.46774194\n",
      "Epoch [466/1000], Train Loss: 0.02605175, Train MAE: 0.11615655, Train RMSE: 0.16140553, Train DA: 0.56849315, Val Loss: 0.04488577, Val MAE: 0.16637211, Val RMSE: 0.21186262, Val DA: 0.46774194\n",
      "Epoch [467/1000], Train Loss: 0.02830184, Train MAE: 0.12665291, Train RMSE: 0.16823149, Train DA: 0.57534247, Val Loss: 0.04769215, Val MAE: 0.17251933, Val RMSE: 0.21838534, Val DA: 0.48387097\n",
      "Epoch [468/1000], Train Loss: 0.02789259, Train MAE: 0.12437723, Train RMSE: 0.16701075, Train DA: 0.56164384, Val Loss: 0.04717941, Val MAE: 0.16757990, Val RMSE: 0.21720822, Val DA: 0.46774194\n",
      "Epoch [469/1000], Train Loss: 0.02891612, Train MAE: 0.12194095, Train RMSE: 0.17004742, Train DA: 0.58904110, Val Loss: 0.04154365, Val MAE: 0.15693222, Val RMSE: 0.20382261, Val DA: 0.46774194\n",
      "Epoch [470/1000], Train Loss: 0.02334831, Train MAE: 0.11144596, Train RMSE: 0.15280151, Train DA: 0.54109589, Val Loss: 0.03709990, Val MAE: 0.14704339, Val RMSE: 0.19261335, Val DA: 0.45161290\n",
      "Epoch [471/1000], Train Loss: 0.03417315, Train MAE: 0.15244709, Train RMSE: 0.18485981, Train DA: 0.56849315, Val Loss: 0.04751757, Val MAE: 0.16921714, Val RMSE: 0.21798524, Val DA: 0.48387097\n",
      "Epoch [472/1000], Train Loss: 0.02639763, Train MAE: 0.12199075, Train RMSE: 0.16247348, Train DA: 0.56164384, Val Loss: 0.04532567, Val MAE: 0.16420756, Val RMSE: 0.21289827, Val DA: 0.45161290\n",
      "Epoch [473/1000], Train Loss: 0.02944347, Train MAE: 0.12459753, Train RMSE: 0.17159101, Train DA: 0.55479452, Val Loss: 0.03508446, Val MAE: 0.14291126, Val RMSE: 0.18730848, Val DA: 0.48387097\n",
      "Epoch [474/1000], Train Loss: 0.02236770, Train MAE: 0.11066795, Train RMSE: 0.14955837, Train DA: 0.57534247, Val Loss: 0.02613729, Val MAE: 0.12248591, Val RMSE: 0.16167030, Val DA: 0.51612903\n",
      "Epoch [475/1000], Train Loss: 0.02994957, Train MAE: 0.13479505, Train RMSE: 0.17305943, Train DA: 0.57534247, Val Loss: 0.04344872, Val MAE: 0.16338630, Val RMSE: 0.20844358, Val DA: 0.45161290\n",
      "Epoch [476/1000], Train Loss: 0.02550182, Train MAE: 0.11665404, Train RMSE: 0.15969290, Train DA: 0.56849315, Val Loss: 0.03586451, Val MAE: 0.15142235, Val RMSE: 0.18937929, Val DA: 0.48387097\n",
      "Epoch [477/1000], Train Loss: 0.02446789, Train MAE: 0.11128886, Train RMSE: 0.15642217, Train DA: 0.54109589, Val Loss: 0.03389980, Val MAE: 0.14570849, Val RMSE: 0.18411896, Val DA: 0.45161290\n",
      "Epoch [478/1000], Train Loss: 0.02686686, Train MAE: 0.12548803, Train RMSE: 0.16391115, Train DA: 0.54794521, Val Loss: 0.04878119, Val MAE: 0.16781700, Val RMSE: 0.22086462, Val DA: 0.46774194\n",
      "Epoch [479/1000], Train Loss: 0.02465363, Train MAE: 0.11580936, Train RMSE: 0.15701474, Train DA: 0.56164384, Val Loss: 0.04808941, Val MAE: 0.16903503, Val RMSE: 0.21929298, Val DA: 0.50000000\n",
      "Epoch [480/1000], Train Loss: 0.02379993, Train MAE: 0.10955394, Train RMSE: 0.15427229, Train DA: 0.58219178, Val Loss: 0.03609921, Val MAE: 0.14568101, Val RMSE: 0.18999791, Val DA: 0.51612903\n",
      "Epoch [481/1000], Train Loss: 0.02375691, Train MAE: 0.11006131, Train RMSE: 0.15413278, Train DA: 0.58219178, Val Loss: 0.03859473, Val MAE: 0.14890619, Val RMSE: 0.19645542, Val DA: 0.48387097\n",
      "Epoch [482/1000], Train Loss: 0.02835215, Train MAE: 0.12445061, Train RMSE: 0.16838098, Train DA: 0.56849315, Val Loss: 0.04699305, Val MAE: 0.17208883, Val RMSE: 0.21677881, Val DA: 0.50000000\n",
      "Epoch [483/1000], Train Loss: 0.02469250, Train MAE: 0.11320830, Train RMSE: 0.15713847, Train DA: 0.56164384, Val Loss: 0.03984695, Val MAE: 0.15780714, Val RMSE: 0.19961701, Val DA: 0.50000000\n",
      "Epoch [484/1000], Train Loss: 0.02369189, Train MAE: 0.10789688, Train RMSE: 0.15392171, Train DA: 0.57534247, Val Loss: 0.03517898, Val MAE: 0.14531796, Val RMSE: 0.18756062, Val DA: 0.53225806\n",
      "Epoch [485/1000], Train Loss: 0.02451582, Train MAE: 0.11346702, Train RMSE: 0.15657529, Train DA: 0.56164384, Val Loss: 0.04165951, Val MAE: 0.15973110, Val RMSE: 0.20410664, Val DA: 0.53225806\n",
      "Epoch [486/1000], Train Loss: 0.02504342, Train MAE: 0.11380651, Train RMSE: 0.15825114, Train DA: 0.56164384, Val Loss: 0.04232869, Val MAE: 0.16324306, Val RMSE: 0.20573938, Val DA: 0.51612903\n",
      "Epoch [487/1000], Train Loss: 0.02499409, Train MAE: 0.11180974, Train RMSE: 0.15809518, Train DA: 0.55479452, Val Loss: 0.04029646, Val MAE: 0.15857658, Val RMSE: 0.20073977, Val DA: 0.51612903\n",
      "Epoch [488/1000], Train Loss: 0.02500093, Train MAE: 0.11160940, Train RMSE: 0.15811682, Train DA: 0.56849315, Val Loss: 0.04103652, Val MAE: 0.16056064, Val RMSE: 0.20257473, Val DA: 0.51612903\n",
      "Epoch [489/1000], Train Loss: 0.02587852, Train MAE: 0.11613890, Train RMSE: 0.16086800, Train DA: 0.55479452, Val Loss: 0.04230804, Val MAE: 0.16440609, Val RMSE: 0.20568918, Val DA: 0.51612903\n",
      "Epoch [490/1000], Train Loss: 0.02567940, Train MAE: 0.11346488, Train RMSE: 0.16024794, Train DA: 0.55479452, Val Loss: 0.04098631, Val MAE: 0.16059190, Val RMSE: 0.20245078, Val DA: 0.51612903\n",
      "Epoch [491/1000], Train Loss: 0.02577237, Train MAE: 0.11268512, Train RMSE: 0.16053775, Train DA: 0.54794521, Val Loss: 0.04102756, Val MAE: 0.16133855, Val RMSE: 0.20255260, Val DA: 0.51612903\n",
      "Epoch [492/1000], Train Loss: 0.02616738, Train MAE: 0.11534332, Train RMSE: 0.16176336, Train DA: 0.55479452, Val Loss: 0.04275841, Val MAE: 0.16551490, Val RMSE: 0.20678107, Val DA: 0.50000000\n",
      "Epoch [493/1000], Train Loss: 0.02600575, Train MAE: 0.11347961, Train RMSE: 0.16126299, Train DA: 0.53424658, Val Loss: 0.04152729, Val MAE: 0.16350381, Val RMSE: 0.20378247, Val DA: 0.51612903\n",
      "Epoch [494/1000], Train Loss: 0.02616170, Train MAE: 0.11337070, Train RMSE: 0.16174580, Train DA: 0.54794521, Val Loss: 0.04100899, Val MAE: 0.16194819, Val RMSE: 0.20250677, Val DA: 0.51612903\n",
      "Epoch [495/1000], Train Loss: 0.02629192, Train MAE: 0.11378743, Train RMSE: 0.16214782, Train DA: 0.54109589, Val Loss: 0.04149298, Val MAE: 0.16324598, Val RMSE: 0.20369823, Val DA: 0.50000000\n",
      "Epoch [496/1000], Train Loss: 0.02633751, Train MAE: 0.11415846, Train RMSE: 0.16228834, Train DA: 0.54794521, Val Loss: 0.04147433, Val MAE: 0.16409092, Val RMSE: 0.20365249, Val DA: 0.50000000\n",
      "Epoch [497/1000], Train Loss: 0.02635028, Train MAE: 0.11440412, Train RMSE: 0.16232771, Train DA: 0.53424658, Val Loss: 0.04151532, Val MAE: 0.16415246, Val RMSE: 0.20375308, Val DA: 0.51612903\n",
      "Epoch [498/1000], Train Loss: 0.02653947, Train MAE: 0.11442509, Train RMSE: 0.16290939, Train DA: 0.55479452, Val Loss: 0.04172332, Val MAE: 0.16423996, Val RMSE: 0.20426288, Val DA: 0.51612903\n",
      "Epoch [499/1000], Train Loss: 0.02643991, Train MAE: 0.11416005, Train RMSE: 0.16260353, Train DA: 0.56164384, Val Loss: 0.04163430, Val MAE: 0.16420332, Val RMSE: 0.20404483, Val DA: 0.51612903\n",
      "Epoch [500/1000], Train Loss: 0.02644291, Train MAE: 0.11449018, Train RMSE: 0.16261275, Train DA: 0.56164384, Val Loss: 0.04144915, Val MAE: 0.16451226, Val RMSE: 0.20359065, Val DA: 0.51612903\n",
      "Epoch [501/1000], Train Loss: 0.02656802, Train MAE: 0.11509741, Train RMSE: 0.16299699, Train DA: 0.55479452, Val Loss: 0.04161866, Val MAE: 0.16492970, Val RMSE: 0.20400651, Val DA: 0.51612903\n",
      "Epoch [502/1000], Train Loss: 0.02675425, Train MAE: 0.11494604, Train RMSE: 0.16356726, Train DA: 0.56849315, Val Loss: 0.04175194, Val MAE: 0.16470748, Val RMSE: 0.20433293, Val DA: 0.51612903\n",
      "Epoch [503/1000], Train Loss: 0.02674597, Train MAE: 0.11495639, Train RMSE: 0.16354196, Train DA: 0.56849315, Val Loss: 0.04174307, Val MAE: 0.16520105, Val RMSE: 0.20431121, Val DA: 0.51612903\n",
      "Epoch [504/1000], Train Loss: 0.02666192, Train MAE: 0.11514646, Train RMSE: 0.16328476, Train DA: 0.56849315, Val Loss: 0.04160363, Val MAE: 0.16586815, Val RMSE: 0.20396969, Val DA: 0.51612903\n",
      "Epoch [505/1000], Train Loss: 0.02676048, Train MAE: 0.11663162, Train RMSE: 0.16358632, Train DA: 0.56164384, Val Loss: 0.04141002, Val MAE: 0.16539246, Val RMSE: 0.20349452, Val DA: 0.51612903\n",
      "Epoch [506/1000], Train Loss: 0.02693530, Train MAE: 0.11569751, Train RMSE: 0.16411977, Train DA: 0.58219178, Val Loss: 0.04147301, Val MAE: 0.16409270, Val RMSE: 0.20364924, Val DA: 0.51612903\n",
      "Epoch [507/1000], Train Loss: 0.02732486, Train MAE: 0.11644496, Train RMSE: 0.16530235, Train DA: 0.57534247, Val Loss: 0.04177968, Val MAE: 0.16454509, Val RMSE: 0.20440078, Val DA: 0.51612903\n",
      "Epoch [508/1000], Train Loss: 0.02650306, Train MAE: 0.11495528, Train RMSE: 0.16279760, Train DA: 0.56164384, Val Loss: 0.04062797, Val MAE: 0.16394189, Val RMSE: 0.20156382, Val DA: 0.50000000\n",
      "Epoch [509/1000], Train Loss: 0.02779719, Train MAE: 0.12364669, Train RMSE: 0.16672489, Train DA: 0.55479452, Val Loss: 0.04124364, Val MAE: 0.16539140, Val RMSE: 0.20308530, Val DA: 0.51612903\n",
      "Epoch [510/1000], Train Loss: 0.02678759, Train MAE: 0.11704965, Train RMSE: 0.16366915, Train DA: 0.55479452, Val Loss: 0.04078629, Val MAE: 0.16205458, Val RMSE: 0.20195615, Val DA: 0.50000000\n",
      "Epoch [511/1000], Train Loss: 0.02969902, Train MAE: 0.12379494, Train RMSE: 0.17233405, Train DA: 0.56849315, Val Loss: 0.04225215, Val MAE: 0.16218339, Val RMSE: 0.20555328, Val DA: 0.51612903\n",
      "Epoch [512/1000], Train Loss: 0.02464862, Train MAE: 0.11059259, Train RMSE: 0.15699880, Train DA: 0.58219178, Val Loss: 0.03591490, Val MAE: 0.15073332, Val RMSE: 0.18951227, Val DA: 0.50000000\n",
      "Epoch [513/1000], Train Loss: 0.03006837, Train MAE: 0.13491940, Train RMSE: 0.17340234, Train DA: 0.54109589, Val Loss: 0.04298973, Val MAE: 0.16651905, Val RMSE: 0.20733963, Val DA: 0.48387097\n",
      "Epoch [514/1000], Train Loss: 0.02689467, Train MAE: 0.12081325, Train RMSE: 0.16399595, Train DA: 0.54794521, Val Loss: 0.03929909, Val MAE: 0.15761538, Val RMSE: 0.19823997, Val DA: 0.51612903\n",
      "Epoch [515/1000], Train Loss: 0.03005404, Train MAE: 0.12506358, Train RMSE: 0.17336102, Train DA: 0.57534247, Val Loss: 0.04032495, Val MAE: 0.15520707, Val RMSE: 0.20081073, Val DA: 0.48387097\n",
      "Epoch [516/1000], Train Loss: 0.02442147, Train MAE: 0.10951567, Train RMSE: 0.15627369, Train DA: 0.54794521, Val Loss: 0.03862181, Val MAE: 0.15382499, Val RMSE: 0.19652434, Val DA: 0.48387097\n",
      "Epoch [517/1000], Train Loss: 0.03013871, Train MAE: 0.13213736, Train RMSE: 0.17360502, Train DA: 0.58219178, Val Loss: 0.04779477, Val MAE: 0.17831296, Val RMSE: 0.21862017, Val DA: 0.48387097\n",
      "Epoch [518/1000], Train Loss: 0.02661215, Train MAE: 0.11704982, Train RMSE: 0.16313231, Train DA: 0.55479452, Val Loss: 0.04037948, Val MAE: 0.15981016, Val RMSE: 0.20094647, Val DA: 0.46774194\n",
      "Epoch [519/1000], Train Loss: 0.02692156, Train MAE: 0.11395425, Train RMSE: 0.16407789, Train DA: 0.52739726, Val Loss: 0.03915472, Val MAE: 0.15550140, Val RMSE: 0.19787551, Val DA: 0.51612903\n",
      "Epoch [520/1000], Train Loss: 0.02889048, Train MAE: 0.12431757, Train RMSE: 0.16997202, Train DA: 0.56164384, Val Loss: 0.04482403, Val MAE: 0.16979674, Val RMSE: 0.21171688, Val DA: 0.54838710\n",
      "Epoch [521/1000], Train Loss: 0.02791115, Train MAE: 0.12143035, Train RMSE: 0.16706629, Train DA: 0.54109589, Val Loss: 0.04040445, Val MAE: 0.16178864, Val RMSE: 0.20100859, Val DA: 0.53225806\n",
      "Epoch [522/1000], Train Loss: 0.02738676, Train MAE: 0.11839955, Train RMSE: 0.16548945, Train DA: 0.54109589, Val Loss: 0.04047002, Val MAE: 0.16074392, Val RMSE: 0.20117162, Val DA: 0.51612903\n",
      "Epoch [523/1000], Train Loss: 0.02701723, Train MAE: 0.11468609, Train RMSE: 0.16436920, Train DA: 0.54794521, Val Loss: 0.04327750, Val MAE: 0.16638246, Val RMSE: 0.20803246, Val DA: 0.51612903\n",
      "Epoch [524/1000], Train Loss: 0.02718723, Train MAE: 0.12065963, Train RMSE: 0.16488551, Train DA: 0.55479452, Val Loss: 0.04341867, Val MAE: 0.16951072, Val RMSE: 0.20837149, Val DA: 0.51612903\n",
      "Epoch [525/1000], Train Loss: 0.02613635, Train MAE: 0.11668559, Train RMSE: 0.16166739, Train DA: 0.56164384, Val Loss: 0.04026018, Val MAE: 0.16342475, Val RMSE: 0.20064940, Val DA: 0.51612903\n",
      "Epoch [526/1000], Train Loss: 0.02665631, Train MAE: 0.11336608, Train RMSE: 0.16326763, Train DA: 0.56164384, Val Loss: 0.04151995, Val MAE: 0.16366775, Val RMSE: 0.20376447, Val DA: 0.50000000\n",
      "Epoch [527/1000], Train Loss: 0.02593533, Train MAE: 0.11257488, Train RMSE: 0.16104449, Train DA: 0.58219178, Val Loss: 0.04335282, Val MAE: 0.16894850, Val RMSE: 0.20821340, Val DA: 0.48387097\n",
      "Epoch [528/1000], Train Loss: 0.02662835, Train MAE: 0.11771233, Train RMSE: 0.16318196, Train DA: 0.56164384, Val Loss: 0.04141483, Val MAE: 0.16410190, Val RMSE: 0.20350632, Val DA: 0.50000000\n",
      "Epoch [529/1000], Train Loss: 0.02647122, Train MAE: 0.11592440, Train RMSE: 0.16269980, Train DA: 0.56164384, Val Loss: 0.03979509, Val MAE: 0.15863982, Val RMSE: 0.19948705, Val DA: 0.51612903\n",
      "Epoch [530/1000], Train Loss: 0.02791778, Train MAE: 0.11591176, Train RMSE: 0.16708614, Train DA: 0.54794521, Val Loss: 0.04093617, Val MAE: 0.16251886, Val RMSE: 0.20232689, Val DA: 0.50000000\n",
      "Epoch [531/1000], Train Loss: 0.02719602, Train MAE: 0.11781157, Train RMSE: 0.16491215, Train DA: 0.56164384, Val Loss: 0.04411424, Val MAE: 0.17062679, Val RMSE: 0.21003389, Val DA: 0.50000000\n",
      "Epoch [532/1000], Train Loss: 0.02755638, Train MAE: 0.11937119, Train RMSE: 0.16600114, Train DA: 0.56164384, Val Loss: 0.04165095, Val MAE: 0.16598840, Val RMSE: 0.20408565, Val DA: 0.50000000\n",
      "Epoch [533/1000], Train Loss: 0.02687240, Train MAE: 0.11438141, Train RMSE: 0.16392803, Train DA: 0.53424658, Val Loss: 0.04004895, Val MAE: 0.16201058, Val RMSE: 0.20012234, Val DA: 0.51612903\n",
      "Epoch [534/1000], Train Loss: 0.02683842, Train MAE: 0.11191120, Train RMSE: 0.16382433, Train DA: 0.54794521, Val Loss: 0.04327384, Val MAE: 0.16977535, Val RMSE: 0.20802365, Val DA: 0.51612903\n",
      "Epoch [535/1000], Train Loss: 0.02707992, Train MAE: 0.11798575, Train RMSE: 0.16455978, Train DA: 0.55479452, Val Loss: 0.04426683, Val MAE: 0.17171334, Val RMSE: 0.21039686, Val DA: 0.50000000\n",
      "Epoch [536/1000], Train Loss: 0.02633643, Train MAE: 0.11597945, Train RMSE: 0.16228503, Train DA: 0.55479452, Val Loss: 0.03987501, Val MAE: 0.16159096, Val RMSE: 0.19968727, Val DA: 0.51612903\n",
      "Epoch [537/1000], Train Loss: 0.02721690, Train MAE: 0.11407890, Train RMSE: 0.16497545, Train DA: 0.54109589, Val Loss: 0.04027539, Val MAE: 0.16187276, Val RMSE: 0.20068727, Val DA: 0.50000000\n",
      "Epoch [538/1000], Train Loss: 0.02676015, Train MAE: 0.11513602, Train RMSE: 0.16358529, Train DA: 0.54794521, Val Loss: 0.04432964, Val MAE: 0.16994788, Val RMSE: 0.21054606, Val DA: 0.51612903\n",
      "Epoch [539/1000], Train Loss: 0.02698981, Train MAE: 0.11865894, Train RMSE: 0.16428575, Train DA: 0.54794521, Val Loss: 0.04301968, Val MAE: 0.17000051, Val RMSE: 0.20741189, Val DA: 0.51612903\n",
      "Epoch [540/1000], Train Loss: 0.02632657, Train MAE: 0.11583775, Train RMSE: 0.16225465, Train DA: 0.54109589, Val Loss: 0.04014970, Val MAE: 0.16138278, Val RMSE: 0.20037389, Val DA: 0.50000000\n",
      "Epoch [541/1000], Train Loss: 0.02832299, Train MAE: 0.11685576, Train RMSE: 0.16829437, Train DA: 0.56849315, Val Loss: 0.04076573, Val MAE: 0.16148822, Val RMSE: 0.20190525, Val DA: 0.51612903\n",
      "Epoch [542/1000], Train Loss: 0.02727739, Train MAE: 0.11897636, Train RMSE: 0.16515867, Train DA: 0.56164384, Val Loss: 0.04526952, Val MAE: 0.17407967, Val RMSE: 0.21276636, Val DA: 0.51612903\n",
      "Epoch [543/1000], Train Loss: 0.02821502, Train MAE: 0.12223076, Train RMSE: 0.16797327, Train DA: 0.55479452, Val Loss: 0.04492720, Val MAE: 0.17622596, Val RMSE: 0.21196038, Val DA: 0.51612903\n",
      "Epoch [544/1000], Train Loss: 0.02682554, Train MAE: 0.11443085, Train RMSE: 0.16378504, Train DA: 0.54109589, Val Loss: 0.04046825, Val MAE: 0.16303775, Val RMSE: 0.20116721, Val DA: 0.53225806\n",
      "Epoch [545/1000], Train Loss: 0.02667032, Train MAE: 0.11259271, Train RMSE: 0.16331050, Train DA: 0.56849315, Val Loss: 0.04317611, Val MAE: 0.16879576, Val RMSE: 0.20778863, Val DA: 0.51612903\n",
      "Epoch [546/1000], Train Loss: 0.02723297, Train MAE: 0.11968753, Train RMSE: 0.16502413, Train DA: 0.56164384, Val Loss: 0.04773902, Val MAE: 0.17886838, Val RMSE: 0.21849260, Val DA: 0.50000000\n",
      "Epoch [547/1000], Train Loss: 0.02613176, Train MAE: 0.11430137, Train RMSE: 0.16165322, Train DA: 0.55479452, Val Loss: 0.04343371, Val MAE: 0.17023687, Val RMSE: 0.20840755, Val DA: 0.51612903\n",
      "Epoch [548/1000], Train Loss: 0.02676779, Train MAE: 0.11298709, Train RMSE: 0.16360864, Train DA: 0.55479452, Val Loss: 0.04195964, Val MAE: 0.16504635, Val RMSE: 0.20484054, Val DA: 0.56451613\n",
      "Epoch [549/1000], Train Loss: 0.02697558, Train MAE: 0.11635035, Train RMSE: 0.16424243, Train DA: 0.54794521, Val Loss: 0.04435942, Val MAE: 0.17073135, Val RMSE: 0.21061677, Val DA: 0.50000000\n",
      "Epoch [550/1000], Train Loss: 0.02694411, Train MAE: 0.11860999, Train RMSE: 0.16414662, Train DA: 0.54794521, Val Loss: 0.04525981, Val MAE: 0.17495908, Val RMSE: 0.21274352, Val DA: 0.51612903\n",
      "Epoch [551/1000], Train Loss: 0.02632765, Train MAE: 0.11425649, Train RMSE: 0.16225800, Train DA: 0.55479452, Val Loss: 0.04017501, Val MAE: 0.16098171, Val RMSE: 0.20043705, Val DA: 0.54838710\n",
      "Epoch [552/1000], Train Loss: 0.02705109, Train MAE: 0.11329093, Train RMSE: 0.16447215, Train DA: 0.56164384, Val Loss: 0.04045844, Val MAE: 0.16120464, Val RMSE: 0.20114285, Val DA: 0.54838710\n",
      "Epoch [553/1000], Train Loss: 0.02726511, Train MAE: 0.11955369, Train RMSE: 0.16512151, Train DA: 0.55479452, Val Loss: 0.04697393, Val MAE: 0.17565833, Val RMSE: 0.21673469, Val DA: 0.53225806\n",
      "Epoch [554/1000], Train Loss: 0.02659906, Train MAE: 0.11598089, Train RMSE: 0.16309220, Train DA: 0.54794521, Val Loss: 0.04412326, Val MAE: 0.17237216, Val RMSE: 0.21005537, Val DA: 0.53225806\n",
      "Epoch [555/1000], Train Loss: 0.02674179, Train MAE: 0.11522267, Train RMSE: 0.16352916, Train DA: 0.54794521, Val Loss: 0.04134824, Val MAE: 0.16429070, Val RMSE: 0.20334268, Val DA: 0.54838710\n",
      "Epoch [556/1000], Train Loss: 0.02596316, Train MAE: 0.11015185, Train RMSE: 0.16113089, Train DA: 0.56849315, Val Loss: 0.04186264, Val MAE: 0.16518484, Val RMSE: 0.20460363, Val DA: 0.54838710\n",
      "Epoch [557/1000], Train Loss: 0.02794149, Train MAE: 0.12411224, Train RMSE: 0.16715708, Train DA: 0.57534247, Val Loss: 0.04885349, Val MAE: 0.17906696, Val RMSE: 0.22102825, Val DA: 0.50000000\n",
      "Epoch [558/1000], Train Loss: 0.02599955, Train MAE: 0.11255770, Train RMSE: 0.16124377, Train DA: 0.55479452, Val Loss: 0.04393264, Val MAE: 0.17117813, Val RMSE: 0.20960115, Val DA: 0.51612903\n",
      "Epoch [559/1000], Train Loss: 0.02693537, Train MAE: 0.11566265, Train RMSE: 0.16411996, Train DA: 0.56849315, Val Loss: 0.04024256, Val MAE: 0.16039139, Val RMSE: 0.20060550, Val DA: 0.54838710\n",
      "Epoch [560/1000], Train Loss: 0.02531645, Train MAE: 0.10870550, Train RMSE: 0.15911144, Train DA: 0.56164384, Val Loss: 0.04242305, Val MAE: 0.16628300, Val RMSE: 0.20596854, Val DA: 0.51612903\n",
      "Epoch [561/1000], Train Loss: 0.02731827, Train MAE: 0.12375732, Train RMSE: 0.16528240, Train DA: 0.57534247, Val Loss: 0.04619225, Val MAE: 0.17622678, Val RMSE: 0.21492383, Val DA: 0.53225806\n",
      "Epoch [562/1000], Train Loss: 0.02725871, Train MAE: 0.11974963, Train RMSE: 0.16510214, Train DA: 0.54794521, Val Loss: 0.04247232, Val MAE: 0.16626765, Val RMSE: 0.20608814, Val DA: 0.53225806\n",
      "Epoch [563/1000], Train Loss: 0.02894158, Train MAE: 0.11925849, Train RMSE: 0.17012227, Train DA: 0.56164384, Val Loss: 0.03919189, Val MAE: 0.15479867, Val RMSE: 0.19796942, Val DA: 0.58064516\n",
      "Epoch [564/1000], Train Loss: 0.02596168, Train MAE: 0.11636778, Train RMSE: 0.16112629, Train DA: 0.55479452, Val Loss: 0.04396343, Val MAE: 0.16802797, Val RMSE: 0.20967457, Val DA: 0.50000000\n",
      "Epoch [565/1000], Train Loss: 0.03010353, Train MAE: 0.12966499, Train RMSE: 0.17350368, Train DA: 0.58219178, Val Loss: 0.05153717, Val MAE: 0.18548955, Val RMSE: 0.22701800, Val DA: 0.50000000\n",
      "Epoch [566/1000], Train Loss: 0.02816267, Train MAE: 0.11766046, Train RMSE: 0.16781737, Train DA: 0.54794521, Val Loss: 0.03954952, Val MAE: 0.15683416, Val RMSE: 0.19887060, Val DA: 0.56451613\n",
      "Epoch [567/1000], Train Loss: 0.02670367, Train MAE: 0.11130704, Train RMSE: 0.16341259, Train DA: 0.56164384, Val Loss: 0.03865524, Val MAE: 0.15525575, Val RMSE: 0.19660933, Val DA: 0.56451613\n",
      "Epoch [568/1000], Train Loss: 0.02791269, Train MAE: 0.12438762, Train RMSE: 0.16707091, Train DA: 0.54794521, Val Loss: 0.05346059, Val MAE: 0.18771684, Val RMSE: 0.23121548, Val DA: 0.45161290\n",
      "Epoch [569/1000], Train Loss: 0.02556850, Train MAE: 0.11340740, Train RMSE: 0.15990154, Train DA: 0.56164384, Val Loss: 0.04117513, Val MAE: 0.16730192, Val RMSE: 0.20291656, Val DA: 0.48387097\n",
      "Epoch [570/1000], Train Loss: 0.02754768, Train MAE: 0.11370223, Train RMSE: 0.16597496, Train DA: 0.55479452, Val Loss: 0.04226511, Val MAE: 0.16490975, Val RMSE: 0.20558478, Val DA: 0.53225806\n",
      "Epoch [571/1000], Train Loss: 0.02617971, Train MAE: 0.11343939, Train RMSE: 0.16180144, Train DA: 0.56849315, Val Loss: 0.04417101, Val MAE: 0.17150135, Val RMSE: 0.21016902, Val DA: 0.56451613\n",
      "Epoch [572/1000], Train Loss: 0.02528424, Train MAE: 0.11062156, Train RMSE: 0.15901020, Train DA: 0.56849315, Val Loss: 0.04370246, Val MAE: 0.17099518, Val RMSE: 0.20905136, Val DA: 0.53225806\n",
      "Epoch [573/1000], Train Loss: 0.02568447, Train MAE: 0.11018436, Train RMSE: 0.16026378, Train DA: 0.58219178, Val Loss: 0.04237281, Val MAE: 0.16676301, Val RMSE: 0.20584655, Val DA: 0.54838710\n",
      "Epoch [574/1000], Train Loss: 0.02574585, Train MAE: 0.11060803, Train RMSE: 0.16045515, Train DA: 0.58219178, Val Loss: 0.04434910, Val MAE: 0.17182253, Val RMSE: 0.21059227, Val DA: 0.53225806\n",
      "Epoch [575/1000], Train Loss: 0.02566357, Train MAE: 0.11084656, Train RMSE: 0.16019852, Train DA: 0.57534247, Val Loss: 0.04511349, Val MAE: 0.17378251, Val RMSE: 0.21239936, Val DA: 0.54838710\n",
      "Epoch [576/1000], Train Loss: 0.02552705, Train MAE: 0.10901203, Train RMSE: 0.15977186, Train DA: 0.57534247, Val Loss: 0.04371170, Val MAE: 0.17034763, Val RMSE: 0.20907344, Val DA: 0.56451613\n",
      "Epoch [577/1000], Train Loss: 0.02572937, Train MAE: 0.10902905, Train RMSE: 0.16040379, Train DA: 0.56849315, Val Loss: 0.04508448, Val MAE: 0.17327033, Val RMSE: 0.21233106, Val DA: 0.56451613\n",
      "Epoch [578/1000], Train Loss: 0.02594098, Train MAE: 0.11081076, Train RMSE: 0.16106203, Train DA: 0.56849315, Val Loss: 0.04623058, Val MAE: 0.17640184, Val RMSE: 0.21501295, Val DA: 0.54838710\n",
      "Epoch [579/1000], Train Loss: 0.02573732, Train MAE: 0.10945865, Train RMSE: 0.16042854, Train DA: 0.58219178, Val Loss: 0.04494290, Val MAE: 0.17408755, Val RMSE: 0.21199737, Val DA: 0.53225806\n",
      "Epoch [580/1000], Train Loss: 0.02567268, Train MAE: 0.10889722, Train RMSE: 0.16022697, Train DA: 0.57534247, Val Loss: 0.04449450, Val MAE: 0.17250048, Val RMSE: 0.21093720, Val DA: 0.53225806\n",
      "Epoch [581/1000], Train Loss: 0.02579817, Train MAE: 0.10903056, Train RMSE: 0.16061807, Train DA: 0.57534247, Val Loss: 0.04486805, Val MAE: 0.17329030, Val RMSE: 0.21182080, Val DA: 0.54838710\n",
      "Epoch [582/1000], Train Loss: 0.02598126, Train MAE: 0.11056756, Train RMSE: 0.16118704, Train DA: 0.56849315, Val Loss: 0.04537544, Val MAE: 0.17503200, Val RMSE: 0.21301512, Val DA: 0.54838710\n",
      "Epoch [583/1000], Train Loss: 0.02583228, Train MAE: 0.10938289, Train RMSE: 0.16072424, Train DA: 0.56849315, Val Loss: 0.04448064, Val MAE: 0.17318229, Val RMSE: 0.21090436, Val DA: 0.54838710\n",
      "Epoch [584/1000], Train Loss: 0.02577062, Train MAE: 0.10891825, Train RMSE: 0.16053230, Train DA: 0.57534247, Val Loss: 0.04406553, Val MAE: 0.17174491, Val RMSE: 0.20991790, Val DA: 0.56451613\n",
      "Epoch [585/1000], Train Loss: 0.02596670, Train MAE: 0.10952076, Train RMSE: 0.16114187, Train DA: 0.56849315, Val Loss: 0.04494020, Val MAE: 0.17373210, Val RMSE: 0.21199104, Val DA: 0.54838710\n",
      "Epoch [586/1000], Train Loss: 0.02609602, Train MAE: 0.11069637, Train RMSE: 0.16154264, Train DA: 0.56849315, Val Loss: 0.04522856, Val MAE: 0.17472531, Val RMSE: 0.21267004, Val DA: 0.53225806\n",
      "Epoch [587/1000], Train Loss: 0.02586349, Train MAE: 0.10933135, Train RMSE: 0.16082130, Train DA: 0.56849315, Val Loss: 0.04427439, Val MAE: 0.17254731, Val RMSE: 0.21041478, Val DA: 0.54838710\n",
      "Epoch [588/1000], Train Loss: 0.02586621, Train MAE: 0.10915253, Train RMSE: 0.16082978, Train DA: 0.56849315, Val Loss: 0.04401650, Val MAE: 0.17143758, Val RMSE: 0.20980109, Val DA: 0.56451613\n",
      "Epoch [589/1000], Train Loss: 0.02612274, Train MAE: 0.10984576, Train RMSE: 0.16162530, Train DA: 0.56164384, Val Loss: 0.04469008, Val MAE: 0.17308272, Val RMSE: 0.21140029, Val DA: 0.56451613\n",
      "Epoch [590/1000], Train Loss: 0.02626642, Train MAE: 0.11141052, Train RMSE: 0.16206917, Train DA: 0.55479452, Val Loss: 0.04511994, Val MAE: 0.17453082, Val RMSE: 0.21241455, Val DA: 0.53225806\n",
      "Epoch [591/1000], Train Loss: 0.02598233, Train MAE: 0.10974072, Train RMSE: 0.16119038, Train DA: 0.56164384, Val Loss: 0.04424276, Val MAE: 0.17259203, Val RMSE: 0.21033962, Val DA: 0.54838710\n",
      "Epoch [592/1000], Train Loss: 0.02594527, Train MAE: 0.10963991, Train RMSE: 0.16107534, Train DA: 0.56849315, Val Loss: 0.04362499, Val MAE: 0.17046207, Val RMSE: 0.20886597, Val DA: 0.56451613\n",
      "Epoch [593/1000], Train Loss: 0.02623554, Train MAE: 0.11000513, Train RMSE: 0.16197388, Train DA: 0.55479452, Val Loss: 0.04429518, Val MAE: 0.17204696, Val RMSE: 0.21046419, Val DA: 0.56451613\n",
      "Epoch [594/1000], Train Loss: 0.02647706, Train MAE: 0.11239658, Train RMSE: 0.16271773, Train DA: 0.54794521, Val Loss: 0.04520995, Val MAE: 0.17472905, Val RMSE: 0.21262631, Val DA: 0.53225806\n",
      "Epoch [595/1000], Train Loss: 0.02618833, Train MAE: 0.11091413, Train RMSE: 0.16182809, Train DA: 0.55479452, Val Loss: 0.04451224, Val MAE: 0.17327712, Val RMSE: 0.21097925, Val DA: 0.53225806\n",
      "Epoch [596/1000], Train Loss: 0.02600190, Train MAE: 0.11008880, Train RMSE: 0.16125105, Train DA: 0.56849315, Val Loss: 0.04335767, Val MAE: 0.16976231, Val RMSE: 0.20822504, Val DA: 0.54838710\n",
      "Epoch [597/1000], Train Loss: 0.02635259, Train MAE: 0.11041351, Train RMSE: 0.16233481, Train DA: 0.55479452, Val Loss: 0.04382430, Val MAE: 0.17055683, Val RMSE: 0.20934254, Val DA: 0.54838710\n",
      "Epoch [598/1000], Train Loss: 0.02658805, Train MAE: 0.11291012, Train RMSE: 0.16305844, Train DA: 0.54109589, Val Loss: 0.04499052, Val MAE: 0.17427456, Val RMSE: 0.21210971, Val DA: 0.53225806\n",
      "Epoch [599/1000], Train Loss: 0.02644981, Train MAE: 0.11302476, Train RMSE: 0.16263397, Train DA: 0.55479452, Val Loss: 0.04489061, Val MAE: 0.17435576, Val RMSE: 0.21187404, Val DA: 0.51612903\n",
      "Epoch [600/1000], Train Loss: 0.02600400, Train MAE: 0.11027510, Train RMSE: 0.16125755, Train DA: 0.56164384, Val Loss: 0.04318476, Val MAE: 0.16927713, Val RMSE: 0.20780945, Val DA: 0.56451613\n",
      "Epoch [601/1000], Train Loss: 0.02651924, Train MAE: 0.11161897, Train RMSE: 0.16284730, Train DA: 0.55479452, Val Loss: 0.04334529, Val MAE: 0.16823800, Val RMSE: 0.20819531, Val DA: 0.56451613\n",
      "Epoch [602/1000], Train Loss: 0.02687735, Train MAE: 0.11283323, Train RMSE: 0.16394313, Train DA: 0.54109589, Val Loss: 0.04497034, Val MAE: 0.17322335, Val RMSE: 0.21206212, Val DA: 0.53225806\n",
      "Epoch [603/1000], Train Loss: 0.02722011, Train MAE: 0.11722542, Train RMSE: 0.16498518, Train DA: 0.54794521, Val Loss: 0.04608444, Val MAE: 0.17728029, Val RMSE: 0.21467288, Val DA: 0.53225806\n",
      "Epoch [604/1000], Train Loss: 0.02623354, Train MAE: 0.11143950, Train RMSE: 0.16196772, Train DA: 0.55479452, Val Loss: 0.04375712, Val MAE: 0.17176662, Val RMSE: 0.20918204, Val DA: 0.53225806\n",
      "Epoch [605/1000], Train Loss: 0.02675057, Train MAE: 0.11363383, Train RMSE: 0.16355601, Train DA: 0.56849315, Val Loss: 0.04266861, Val MAE: 0.16640782, Val RMSE: 0.20656382, Val DA: 0.56451613\n",
      "Epoch [606/1000], Train Loss: 0.02669980, Train MAE: 0.11166362, Train RMSE: 0.16340072, Train DA: 0.56164384, Val Loss: 0.04317228, Val MAE: 0.16812301, Val RMSE: 0.20777944, Val DA: 0.58064516\n",
      "Epoch [607/1000], Train Loss: 0.02759212, Train MAE: 0.12053591, Train RMSE: 0.16610876, Train DA: 0.57534247, Val Loss: 0.04681290, Val MAE: 0.17961246, Val RMSE: 0.21636288, Val DA: 0.54838710\n",
      "Epoch [608/1000], Train Loss: 0.02688611, Train MAE: 0.11764072, Train RMSE: 0.16396983, Train DA: 0.56849315, Val Loss: 0.04540618, Val MAE: 0.17616872, Val RMSE: 0.21308726, Val DA: 0.50000000\n",
      "Epoch [609/1000], Train Loss: 0.02612948, Train MAE: 0.11169395, Train RMSE: 0.16164616, Train DA: 0.55479452, Val Loss: 0.04052849, Val MAE: 0.16121212, Val RMSE: 0.20131688, Val DA: 0.53225806\n",
      "Epoch [610/1000], Train Loss: 0.02732196, Train MAE: 0.11401027, Train RMSE: 0.16529356, Train DA: 0.52739726, Val Loss: 0.04138862, Val MAE: 0.15892228, Val RMSE: 0.20344193, Val DA: 0.58064516\n",
      "Epoch [611/1000], Train Loss: 0.02670272, Train MAE: 0.11573440, Train RMSE: 0.16340968, Train DA: 0.55479452, Val Loss: 0.04485469, Val MAE: 0.17175588, Val RMSE: 0.21178927, Val DA: 0.54838710\n",
      "Epoch [612/1000], Train Loss: 0.02869425, Train MAE: 0.12576881, Train RMSE: 0.16939379, Train DA: 0.55479452, Val Loss: 0.04973311, Val MAE: 0.18552405, Val RMSE: 0.22300923, Val DA: 0.51612903\n",
      "Epoch [613/1000], Train Loss: 0.02541381, Train MAE: 0.11036971, Train RMSE: 0.15941711, Train DA: 0.56164384, Val Loss: 0.04074362, Val MAE: 0.16340795, Val RMSE: 0.20185050, Val DA: 0.50000000\n",
      "Epoch [614/1000], Train Loss: 0.02741405, Train MAE: 0.11748681, Train RMSE: 0.16557188, Train DA: 0.56164384, Val Loss: 0.03998503, Val MAE: 0.15682946, Val RMSE: 0.19996257, Val DA: 0.51612903\n",
      "Epoch [615/1000], Train Loss: 0.02683354, Train MAE: 0.11718942, Train RMSE: 0.16380945, Train DA: 0.55479452, Val Loss: 0.04051483, Val MAE: 0.15770745, Val RMSE: 0.20128298, Val DA: 0.54838710\n",
      "Epoch [616/1000], Train Loss: 0.02871933, Train MAE: 0.12784001, Train RMSE: 0.16946778, Train DA: 0.55479452, Val Loss: 0.04948832, Val MAE: 0.18265031, Val RMSE: 0.22245970, Val DA: 0.51612903\n",
      "Epoch [617/1000], Train Loss: 0.02397309, Train MAE: 0.10621450, Train RMSE: 0.15483245, Train DA: 0.56849315, Val Loss: 0.03717608, Val MAE: 0.15551174, Val RMSE: 0.19281101, Val DA: 0.53225806\n",
      "Epoch [618/1000], Train Loss: 0.02845697, Train MAE: 0.12430735, Train RMSE: 0.16869195, Train DA: 0.58219178, Val Loss: 0.03708797, Val MAE: 0.15663417, Val RMSE: 0.19258235, Val DA: 0.59677419\n",
      "Epoch [619/1000], Train Loss: 0.02683537, Train MAE: 0.12496328, Train RMSE: 0.16381505, Train DA: 0.49315068, Val Loss: 0.03369332, Val MAE: 0.15466832, Val RMSE: 0.18355742, Val DA: 0.51612903\n",
      "Epoch [620/1000], Train Loss: 0.02231864, Train MAE: 0.11288053, Train RMSE: 0.14939426, Train DA: 0.56849315, Val Loss: 0.02743770, Val MAE: 0.13254337, Val RMSE: 0.16564329, Val DA: 0.56451613\n",
      "Epoch [621/1000], Train Loss: 0.03265962, Train MAE: 0.13482079, Train RMSE: 0.18071973, Train DA: 0.57534247, Val Loss: 0.03468693, Val MAE: 0.15655471, Val RMSE: 0.18624425, Val DA: 0.48387097\n",
      "Epoch [622/1000], Train Loss: 0.02513004, Train MAE: 0.11227520, Train RMSE: 0.15852459, Train DA: 0.56164384, Val Loss: 0.02350543, Val MAE: 0.11619308, Val RMSE: 0.15331481, Val DA: 0.50000000\n",
      "Epoch [623/1000], Train Loss: 0.02389395, Train MAE: 0.12159109, Train RMSE: 0.15457669, Train DA: 0.46575342, Val Loss: 0.02460481, Val MAE: 0.12127288, Val RMSE: 0.15685922, Val DA: 0.50000000\n",
      "Epoch [624/1000], Train Loss: 0.02275935, Train MAE: 0.11095243, Train RMSE: 0.15086202, Train DA: 0.53424658, Val Loss: 0.02718749, Val MAE: 0.12165610, Val RMSE: 0.16488628, Val DA: 0.46774194\n",
      "Epoch [625/1000], Train Loss: 0.02147605, Train MAE: 0.10341248, Train RMSE: 0.14654709, Train DA: 0.50000000, Val Loss: 0.01587833, Val MAE: 0.09111110, Val RMSE: 0.12600926, Val DA: 0.48387097\n",
      "Epoch [626/1000], Train Loss: 0.02211597, Train MAE: 0.10903528, Train RMSE: 0.14871438, Train DA: 0.52054795, Val Loss: 0.01734282, Val MAE: 0.10221428, Val RMSE: 0.13169214, Val DA: 0.51612903\n",
      "Epoch [627/1000], Train Loss: 0.02900282, Train MAE: 0.14258425, Train RMSE: 0.17030214, Train DA: 0.50000000, Val Loss: 0.02464317, Val MAE: 0.12918262, Val RMSE: 0.15698144, Val DA: 0.48387097\n",
      "Epoch [628/1000], Train Loss: 0.02085500, Train MAE: 0.10767425, Train RMSE: 0.14441262, Train DA: 0.48630137, Val Loss: 0.01807194, Val MAE: 0.10200299, Val RMSE: 0.13443190, Val DA: 0.41935484\n",
      "Epoch [629/1000], Train Loss: 0.03006473, Train MAE: 0.12730730, Train RMSE: 0.17339183, Train DA: 0.50000000, Val Loss: 0.02541788, Val MAE: 0.11682421, Val RMSE: 0.15942983, Val DA: 0.45161290\n",
      "Epoch [630/1000], Train Loss: 0.02142820, Train MAE: 0.10571711, Train RMSE: 0.14638375, Train DA: 0.52739726, Val Loss: 0.02280982, Val MAE: 0.11342896, Val RMSE: 0.15102920, Val DA: 0.46774194\n",
      "Epoch [631/1000], Train Loss: 0.02878998, Train MAE: 0.14104328, Train RMSE: 0.16967611, Train DA: 0.54109589, Val Loss: 0.03787443, Val MAE: 0.14439467, Val RMSE: 0.19461355, Val DA: 0.45161290\n",
      "Epoch [632/1000], Train Loss: 0.02382782, Train MAE: 0.11204296, Train RMSE: 0.15436262, Train DA: 0.47945205, Val Loss: 0.03641704, Val MAE: 0.13553949, Val RMSE: 0.19083251, Val DA: 0.43548387\n",
      "Epoch [633/1000], Train Loss: 0.02717511, Train MAE: 0.11910199, Train RMSE: 0.16484873, Train DA: 0.48630137, Val Loss: 0.03979514, Val MAE: 0.14506106, Val RMSE: 0.19948718, Val DA: 0.43548387\n",
      "Epoch [634/1000], Train Loss: 0.02097216, Train MAE: 0.10250591, Train RMSE: 0.14481768, Train DA: 0.50000000, Val Loss: 0.03567636, Val MAE: 0.13476320, Val RMSE: 0.18888187, Val DA: 0.40322581\n",
      "Epoch [635/1000], Train Loss: 0.02634912, Train MAE: 0.12298223, Train RMSE: 0.16232413, Train DA: 0.54109589, Val Loss: 0.03080524, Val MAE: 0.13453527, Val RMSE: 0.17551421, Val DA: 0.45161290\n",
      "Epoch [636/1000], Train Loss: 0.02153244, Train MAE: 0.11125075, Train RMSE: 0.14673936, Train DA: 0.47945205, Val Loss: 0.02690516, Val MAE: 0.12798551, Val RMSE: 0.16402793, Val DA: 0.45161290\n",
      "Epoch [637/1000], Train Loss: 0.02570834, Train MAE: 0.11280122, Train RMSE: 0.16033822, Train DA: 0.50684932, Val Loss: 0.03250433, Val MAE: 0.13855818, Val RMSE: 0.18028957, Val DA: 0.41935484\n",
      "Epoch [638/1000], Train Loss: 0.02345924, Train MAE: 0.10567261, Train RMSE: 0.15316410, Train DA: 0.52739726, Val Loss: 0.03119562, Val MAE: 0.13592045, Val RMSE: 0.17662282, Val DA: 0.43548387\n",
      "Epoch [639/1000], Train Loss: 0.02569154, Train MAE: 0.13291802, Train RMSE: 0.16028582, Train DA: 0.51369863, Val Loss: 0.03389448, Val MAE: 0.14633542, Val RMSE: 0.18410455, Val DA: 0.41935484\n",
      "Epoch [640/1000], Train Loss: 0.02073401, Train MAE: 0.10337316, Train RMSE: 0.14399309, Train DA: 0.53424658, Val Loss: 0.03583027, Val MAE: 0.13149664, Val RMSE: 0.18928884, Val DA: 0.51612903\n",
      "Epoch [641/1000], Train Loss: 0.02651776, Train MAE: 0.11692000, Train RMSE: 0.16284275, Train DA: 0.49315068, Val Loss: 0.03792741, Val MAE: 0.14720418, Val RMSE: 0.19474961, Val DA: 0.50000000\n",
      "Epoch [642/1000], Train Loss: 0.02275992, Train MAE: 0.11386628, Train RMSE: 0.15086393, Train DA: 0.50000000, Val Loss: 0.03557129, Val MAE: 0.13419153, Val RMSE: 0.18860351, Val DA: 0.48387097\n",
      "Epoch [643/1000], Train Loss: 0.02956014, Train MAE: 0.13557535, Train RMSE: 0.17193063, Train DA: 0.50000000, Val Loss: 0.04671245, Val MAE: 0.15930685, Val RMSE: 0.21613064, Val DA: 0.48387097\n",
      "Epoch [644/1000], Train Loss: 0.02449573, Train MAE: 0.11022003, Train RMSE: 0.15651113, Train DA: 0.53424658, Val Loss: 0.04193620, Val MAE: 0.15303472, Val RMSE: 0.20478331, Val DA: 0.43548387\n",
      "Epoch [645/1000], Train Loss: 0.02246914, Train MAE: 0.10418105, Train RMSE: 0.14989711, Train DA: 0.51369863, Val Loss: 0.03608771, Val MAE: 0.13756117, Val RMSE: 0.18996768, Val DA: 0.43548387\n",
      "Epoch [646/1000], Train Loss: 0.02521737, Train MAE: 0.12055828, Train RMSE: 0.15879978, Train DA: 0.48630137, Val Loss: 0.04230047, Val MAE: 0.14772047, Val RMSE: 0.20567076, Val DA: 0.43548387\n",
      "Epoch [647/1000], Train Loss: 0.02554436, Train MAE: 0.11162466, Train RMSE: 0.15982603, Train DA: 0.50000000, Val Loss: 0.04406076, Val MAE: 0.15071578, Val RMSE: 0.20990656, Val DA: 0.45161290\n",
      "Epoch [648/1000], Train Loss: 0.02387236, Train MAE: 0.10770840, Train RMSE: 0.15450682, Train DA: 0.47945205, Val Loss: 0.03622618, Val MAE: 0.14061102, Val RMSE: 0.19033179, Val DA: 0.43548387\n",
      "Epoch [649/1000], Train Loss: 0.02503045, Train MAE: 0.11430831, Train RMSE: 0.15821014, Train DA: 0.50684932, Val Loss: 0.04120268, Val MAE: 0.14380237, Val RMSE: 0.20298444, Val DA: 0.46774194\n",
      "Epoch [650/1000], Train Loss: 0.02402612, Train MAE: 0.11086551, Train RMSE: 0.15500362, Train DA: 0.51369863, Val Loss: 0.04188957, Val MAE: 0.14874545, Val RMSE: 0.20466943, Val DA: 0.46774194\n",
      "Epoch [651/1000], Train Loss: 0.02356781, Train MAE: 0.10568456, Train RMSE: 0.15351813, Train DA: 0.49315068, Val Loss: 0.04159013, Val MAE: 0.15248474, Val RMSE: 0.20393658, Val DA: 0.41935484\n",
      "Epoch [652/1000], Train Loss: 0.02261880, Train MAE: 0.10327400, Train RMSE: 0.15039548, Train DA: 0.50684932, Val Loss: 0.04084618, Val MAE: 0.14608140, Val RMSE: 0.20210439, Val DA: 0.43548387\n",
      "Epoch [653/1000], Train Loss: 0.02419198, Train MAE: 0.11050964, Train RMSE: 0.15553771, Train DA: 0.52739726, Val Loss: 0.04329445, Val MAE: 0.14875427, Val RMSE: 0.20807320, Val DA: 0.43548387\n",
      "Epoch [654/1000], Train Loss: 0.02300341, Train MAE: 0.10372518, Train RMSE: 0.15166877, Train DA: 0.50684932, Val Loss: 0.04050126, Val MAE: 0.14437930, Val RMSE: 0.20124924, Val DA: 0.41935484\n",
      "Epoch [655/1000], Train Loss: 0.02414723, Train MAE: 0.10553553, Train RMSE: 0.15539381, Train DA: 0.51369863, Val Loss: 0.04097595, Val MAE: 0.15050228, Val RMSE: 0.20242517, Val DA: 0.43548387\n",
      "Epoch [656/1000], Train Loss: 0.02420819, Train MAE: 0.10791078, Train RMSE: 0.15558980, Train DA: 0.52054795, Val Loss: 0.04131187, Val MAE: 0.14738289, Val RMSE: 0.20325321, Val DA: 0.43548387\n",
      "Epoch [657/1000], Train Loss: 0.02623274, Train MAE: 0.11630125, Train RMSE: 0.16196525, Train DA: 0.52054795, Val Loss: 0.04431099, Val MAE: 0.15283981, Val RMSE: 0.21050175, Val DA: 0.45161290\n",
      "Epoch [658/1000], Train Loss: 0.02494364, Train MAE: 0.10674685, Train RMSE: 0.15793554, Train DA: 0.53424658, Val Loss: 0.04234260, Val MAE: 0.15220711, Val RMSE: 0.20577316, Val DA: 0.46774194\n",
      "Epoch [659/1000], Train Loss: 0.02406612, Train MAE: 0.10483193, Train RMSE: 0.15513259, Train DA: 0.52739726, Val Loss: 0.04049852, Val MAE: 0.14831062, Val RMSE: 0.20124245, Val DA: 0.43548387\n",
      "Epoch [660/1000], Train Loss: 0.02651636, Train MAE: 0.11527143, Train RMSE: 0.16283847, Train DA: 0.57534247, Val Loss: 0.04163072, Val MAE: 0.14914331, Val RMSE: 0.20403606, Val DA: 0.46774194\n",
      "Epoch [661/1000], Train Loss: 0.02705555, Train MAE: 0.11598832, Train RMSE: 0.16448571, Train DA: 0.54109589, Val Loss: 0.04340807, Val MAE: 0.15359122, Val RMSE: 0.20834605, Val DA: 0.45161290\n",
      "Epoch [662/1000], Train Loss: 0.02676085, Train MAE: 0.11060897, Train RMSE: 0.16358744, Train DA: 0.52054795, Val Loss: 0.04358410, Val MAE: 0.15544316, Val RMSE: 0.20876805, Val DA: 0.43548387\n",
      "Epoch [663/1000], Train Loss: 0.02475516, Train MAE: 0.10652767, Train RMSE: 0.15733774, Train DA: 0.54794521, Val Loss: 0.03981275, Val MAE: 0.14551735, Val RMSE: 0.19953133, Val DA: 0.45161290\n",
      "Epoch [664/1000], Train Loss: 0.02901169, Train MAE: 0.12404088, Train RMSE: 0.17032817, Train DA: 0.56164384, Val Loss: 0.04386373, Val MAE: 0.15521128, Val RMSE: 0.20943670, Val DA: 0.46774194\n",
      "Epoch [665/1000], Train Loss: 0.02775271, Train MAE: 0.11319499, Train RMSE: 0.16659144, Train DA: 0.54109589, Val Loss: 0.04612963, Val MAE: 0.16236354, Val RMSE: 0.21477810, Val DA: 0.45161290\n",
      "Epoch [666/1000], Train Loss: 0.02702267, Train MAE: 0.11164382, Train RMSE: 0.16438574, Train DA: 0.55479452, Val Loss: 0.04375812, Val MAE: 0.15867278, Val RMSE: 0.20918444, Val DA: 0.45161290\n",
      "Epoch [667/1000], Train Loss: 0.02526300, Train MAE: 0.10847414, Train RMSE: 0.15894340, Train DA: 0.56164384, Val Loss: 0.04241963, Val MAE: 0.15282539, Val RMSE: 0.20596024, Val DA: 0.43548387\n",
      "Epoch [668/1000], Train Loss: 0.02954870, Train MAE: 0.12634021, Train RMSE: 0.17189735, Train DA: 0.56849315, Val Loss: 0.04701754, Val MAE: 0.16272813, Val RMSE: 0.21683528, Val DA: 0.43548387\n",
      "Epoch [669/1000], Train Loss: 0.02595379, Train MAE: 0.11046487, Train RMSE: 0.16110182, Train DA: 0.53424658, Val Loss: 0.04346116, Val MAE: 0.15420474, Val RMSE: 0.20847344, Val DA: 0.46774194\n",
      "Epoch [670/1000], Train Loss: 0.02779863, Train MAE: 0.11334532, Train RMSE: 0.16672920, Train DA: 0.52054795, Val Loss: 0.04416236, Val MAE: 0.15706031, Val RMSE: 0.21014841, Val DA: 0.46774194\n",
      "Epoch [671/1000], Train Loss: 0.02549024, Train MAE: 0.11004857, Train RMSE: 0.15965661, Train DA: 0.54794521, Val Loss: 0.04433813, Val MAE: 0.15578952, Val RMSE: 0.21056619, Val DA: 0.45161290\n",
      "Epoch [672/1000], Train Loss: 0.03162702, Train MAE: 0.13184369, Train RMSE: 0.17783988, Train DA: 0.55479452, Val Loss: 0.05182892, Val MAE: 0.17423674, Val RMSE: 0.22765969, Val DA: 0.45161290\n",
      "Epoch [673/1000], Train Loss: 0.02783673, Train MAE: 0.11402202, Train RMSE: 0.16684343, Train DA: 0.53424658, Val Loss: 0.04670352, Val MAE: 0.16831386, Val RMSE: 0.21610996, Val DA: 0.41935484\n",
      "Epoch [674/1000], Train Loss: 0.02666542, Train MAE: 0.11019202, Train RMSE: 0.16329551, Train DA: 0.54109589, Val Loss: 0.04594986, Val MAE: 0.16518193, Val RMSE: 0.21435918, Val DA: 0.37096774\n",
      "Epoch [675/1000], Train Loss: 0.02767222, Train MAE: 0.11613258, Train RMSE: 0.16634969, Train DA: 0.54794521, Val Loss: 0.04512521, Val MAE: 0.15869559, Val RMSE: 0.21242695, Val DA: 0.43548387\n",
      "Epoch [676/1000], Train Loss: 0.03012109, Train MAE: 0.12463841, Train RMSE: 0.17355426, Train DA: 0.54109589, Val Loss: 0.04726018, Val MAE: 0.16608907, Val RMSE: 0.21739407, Val DA: 0.45161290\n",
      "Epoch [677/1000], Train Loss: 0.02696977, Train MAE: 0.11121786, Train RMSE: 0.16422476, Train DA: 0.54109589, Val Loss: 0.04530971, Val MAE: 0.16489406, Val RMSE: 0.21286079, Val DA: 0.41935484\n",
      "Epoch [678/1000], Train Loss: 0.02604583, Train MAE: 0.10986044, Train RMSE: 0.16138722, Train DA: 0.52739726, Val Loss: 0.05126949, Val MAE: 0.17367347, Val RMSE: 0.22642766, Val DA: 0.45161290\n",
      "Epoch [679/1000], Train Loss: 0.03002160, Train MAE: 0.12584338, Train RMSE: 0.17326742, Train DA: 0.54794521, Val Loss: 0.04747847, Val MAE: 0.16860457, Val RMSE: 0.21789557, Val DA: 0.45161290\n",
      "Epoch [680/1000], Train Loss: 0.02802186, Train MAE: 0.11451154, Train RMSE: 0.16739732, Train DA: 0.54109589, Val Loss: 0.04209213, Val MAE: 0.15746151, Val RMSE: 0.20516367, Val DA: 0.45161290\n",
      "Epoch [681/1000], Train Loss: 0.02896653, Train MAE: 0.11573335, Train RMSE: 0.17019558, Train DA: 0.53424658, Val Loss: 0.04882386, Val MAE: 0.17154841, Val RMSE: 0.22096123, Val DA: 0.43548387\n",
      "Epoch [682/1000], Train Loss: 0.02831077, Train MAE: 0.11880815, Train RMSE: 0.16825803, Train DA: 0.53424658, Val Loss: 0.04994724, Val MAE: 0.17122146, Val RMSE: 0.22348878, Val DA: 0.43548387\n",
      "Epoch [683/1000], Train Loss: 0.03002924, Train MAE: 0.12430710, Train RMSE: 0.17328946, Train DA: 0.54109589, Val Loss: 0.04799649, Val MAE: 0.16833213, Val RMSE: 0.21908101, Val DA: 0.43548387\n",
      "Epoch [684/1000], Train Loss: 0.02970591, Train MAE: 0.11956400, Train RMSE: 0.17235404, Train DA: 0.54794521, Val Loss: 0.04648335, Val MAE: 0.16419278, Val RMSE: 0.21559998, Val DA: 0.43548387\n",
      "Epoch [685/1000], Train Loss: 0.02751362, Train MAE: 0.11249096, Train RMSE: 0.16587231, Train DA: 0.53424658, Val Loss: 0.04473942, Val MAE: 0.16001950, Val RMSE: 0.21151695, Val DA: 0.43548387\n",
      "Epoch [686/1000], Train Loss: 0.03081625, Train MAE: 0.12702903, Train RMSE: 0.17554559, Train DA: 0.53424658, Val Loss: 0.05134689, Val MAE: 0.17496368, Val RMSE: 0.22659852, Val DA: 0.41935484\n",
      "Epoch [687/1000], Train Loss: 0.02966817, Train MAE: 0.12108591, Train RMSE: 0.17224449, Train DA: 0.53424658, Val Loss: 0.04820281, Val MAE: 0.16921198, Val RMSE: 0.21955137, Val DA: 0.41935484\n",
      "Epoch [688/1000], Train Loss: 0.02970471, Train MAE: 0.11888897, Train RMSE: 0.17235056, Train DA: 0.54109589, Val Loss: 0.04831745, Val MAE: 0.16712989, Val RMSE: 0.21981232, Val DA: 0.43548387\n",
      "Epoch [689/1000], Train Loss: 0.02956403, Train MAE: 0.11820178, Train RMSE: 0.17194195, Train DA: 0.53424658, Val Loss: 0.05126398, Val MAE: 0.17198317, Val RMSE: 0.22641549, Val DA: 0.43548387\n",
      "Epoch [690/1000], Train Loss: 0.03021906, Train MAE: 0.12627223, Train RMSE: 0.17383631, Train DA: 0.54109589, Val Loss: 0.04981453, Val MAE: 0.17062600, Val RMSE: 0.22319166, Val DA: 0.41935484\n",
      "Epoch [691/1000], Train Loss: 0.02946085, Train MAE: 0.11483900, Train RMSE: 0.17164163, Train DA: 0.54794521, Val Loss: 0.05058147, Val MAE: 0.17172064, Val RMSE: 0.22490326, Val DA: 0.43548387\n",
      "Epoch [692/1000], Train Loss: 0.02994016, Train MAE: 0.11756796, Train RMSE: 0.17303227, Train DA: 0.54794521, Val Loss: 0.04985700, Val MAE: 0.17188843, Val RMSE: 0.22328682, Val DA: 0.43548387\n",
      "Epoch [693/1000], Train Loss: 0.02980881, Train MAE: 0.12179651, Train RMSE: 0.17265229, Train DA: 0.53424658, Val Loss: 0.04777879, Val MAE: 0.16765130, Val RMSE: 0.21858358, Val DA: 0.41935484\n",
      "Epoch [694/1000], Train Loss: 0.02923369, Train MAE: 0.11593460, Train RMSE: 0.17097862, Train DA: 0.54109589, Val Loss: 0.04854247, Val MAE: 0.16747451, Val RMSE: 0.22032355, Val DA: 0.43548387\n",
      "Epoch [695/1000], Train Loss: 0.02953453, Train MAE: 0.11476617, Train RMSE: 0.17185612, Train DA: 0.55479452, Val Loss: 0.05049269, Val MAE: 0.17026159, Val RMSE: 0.22470579, Val DA: 0.41935484\n",
      "Epoch [696/1000], Train Loss: 0.03008983, Train MAE: 0.11888818, Train RMSE: 0.17346419, Train DA: 0.54794521, Val Loss: 0.05082659, Val MAE: 0.17273280, Val RMSE: 0.22544754, Val DA: 0.41935484\n",
      "Epoch [697/1000], Train Loss: 0.03055903, Train MAE: 0.12038416, Train RMSE: 0.17481141, Train DA: 0.54109589, Val Loss: 0.05075456, Val MAE: 0.17306720, Val RMSE: 0.22528774, Val DA: 0.43548387\n",
      "Epoch [698/1000], Train Loss: 0.02975677, Train MAE: 0.11628967, Train RMSE: 0.17250153, Train DA: 0.54109589, Val Loss: 0.05041165, Val MAE: 0.17124023, Val RMSE: 0.22452541, Val DA: 0.41935484\n",
      "Epoch [699/1000], Train Loss: 0.02956781, Train MAE: 0.11516733, Train RMSE: 0.17195293, Train DA: 0.54794521, Val Loss: 0.04943472, Val MAE: 0.16956536, Val RMSE: 0.22233920, Val DA: 0.41935484\n",
      "Epoch [700/1000], Train Loss: 0.03024101, Train MAE: 0.12099856, Train RMSE: 0.17389943, Train DA: 0.54794521, Val Loss: 0.04986469, Val MAE: 0.17151017, Val RMSE: 0.22330403, Val DA: 0.41935484\n",
      "Epoch [701/1000], Train Loss: 0.03093550, Train MAE: 0.12221166, Train RMSE: 0.17588492, Train DA: 0.54109589, Val Loss: 0.04992795, Val MAE: 0.17142054, Val RMSE: 0.22344565, Val DA: 0.41935484\n",
      "Epoch [702/1000], Train Loss: 0.03101686, Train MAE: 0.12036704, Train RMSE: 0.17611605, Train DA: 0.54109589, Val Loss: 0.04985619, Val MAE: 0.17064530, Val RMSE: 0.22328499, Val DA: 0.43548387\n",
      "Epoch [703/1000], Train Loss: 0.02941101, Train MAE: 0.11534391, Train RMSE: 0.17149638, Train DA: 0.54794521, Val Loss: 0.04936577, Val MAE: 0.16795273, Val RMSE: 0.22218408, Val DA: 0.41935484\n",
      "Epoch [704/1000], Train Loss: 0.03085157, Train MAE: 0.12547141, Train RMSE: 0.17564617, Train DA: 0.53424658, Val Loss: 0.05137727, Val MAE: 0.17340948, Val RMSE: 0.22666556, Val DA: 0.41935484\n",
      "Epoch [705/1000], Train Loss: 0.03173706, Train MAE: 0.12756975, Train RMSE: 0.17814897, Train DA: 0.54109589, Val Loss: 0.05065680, Val MAE: 0.17256288, Val RMSE: 0.22507066, Val DA: 0.41935484\n",
      "Epoch [706/1000], Train Loss: 0.03148793, Train MAE: 0.12241065, Train RMSE: 0.17744839, Train DA: 0.54794521, Val Loss: 0.04891211, Val MAE: 0.16849998, Val RMSE: 0.22116081, Val DA: 0.43548387\n",
      "Epoch [707/1000], Train Loss: 0.02957704, Train MAE: 0.11671881, Train RMSE: 0.17197976, Train DA: 0.54794521, Val Loss: 0.04688488, Val MAE: 0.16208239, Val RMSE: 0.21652918, Val DA: 0.43548387\n",
      "Epoch [708/1000], Train Loss: 0.03279675, Train MAE: 0.13170922, Train RMSE: 0.18109873, Train DA: 0.53424658, Val Loss: 0.05398584, Val MAE: 0.17889456, Val RMSE: 0.23234853, Val DA: 0.41935484\n",
      "Epoch [709/1000], Train Loss: 0.03301906, Train MAE: 0.12869212, Train RMSE: 0.18171147, Train DA: 0.54109589, Val Loss: 0.05436869, Val MAE: 0.18111832, Val RMSE: 0.23317096, Val DA: 0.40322581\n",
      "Epoch [710/1000], Train Loss: 0.03300865, Train MAE: 0.12603788, Train RMSE: 0.18168284, Train DA: 0.54794521, Val Loss: 0.05295405, Val MAE: 0.17728804, Val RMSE: 0.23011747, Val DA: 0.41935484\n",
      "Epoch [711/1000], Train Loss: 0.02965897, Train MAE: 0.11672757, Train RMSE: 0.17221779, Train DA: 0.56164384, Val Loss: 0.05079036, Val MAE: 0.17087258, Val RMSE: 0.22536716, Val DA: 0.40322581\n",
      "Epoch [712/1000], Train Loss: 0.03391964, Train MAE: 0.13446759, Train RMSE: 0.18417285, Train DA: 0.52054795, Val Loss: 0.05691589, Val MAE: 0.18457875, Val RMSE: 0.23857053, Val DA: 0.41935484\n",
      "Epoch [713/1000], Train Loss: 0.03103615, Train MAE: 0.11967927, Train RMSE: 0.17617080, Train DA: 0.54794521, Val Loss: 0.04883096, Val MAE: 0.17024572, Val RMSE: 0.22097729, Val DA: 0.43548387\n",
      "Epoch [714/1000], Train Loss: 0.03234616, Train MAE: 0.12312667, Train RMSE: 0.17985038, Train DA: 0.54794521, Val Loss: 0.05128944, Val MAE: 0.17460531, Val RMSE: 0.22647171, Val DA: 0.46774194\n",
      "Epoch [715/1000], Train Loss: 0.03071395, Train MAE: 0.12032426, Train RMSE: 0.17525397, Train DA: 0.56164384, Val Loss: 0.05179140, Val MAE: 0.17170188, Val RMSE: 0.22757722, Val DA: 0.45161290\n",
      "Epoch [716/1000], Train Loss: 0.03588407, Train MAE: 0.13736154, Train RMSE: 0.18943091, Train DA: 0.54109589, Val Loss: 0.05582721, Val MAE: 0.18360256, Val RMSE: 0.23627782, Val DA: 0.43548387\n",
      "Epoch [717/1000], Train Loss: 0.03013721, Train MAE: 0.11630052, Train RMSE: 0.17360073, Train DA: 0.55479452, Val Loss: 0.04688903, Val MAE: 0.16513218, Val RMSE: 0.21653874, Val DA: 0.41935484\n",
      "Epoch [718/1000], Train Loss: 0.02961642, Train MAE: 0.11376117, Train RMSE: 0.17209423, Train DA: 0.55479452, Val Loss: 0.04876951, Val MAE: 0.16393857, Val RMSE: 0.22083819, Val DA: 0.40322581\n",
      "Epoch [719/1000], Train Loss: 0.03298267, Train MAE: 0.12943694, Train RMSE: 0.18161131, Train DA: 0.56164384, Val Loss: 0.04757950, Val MAE: 0.16573097, Val RMSE: 0.21812727, Val DA: 0.41935484\n",
      "Epoch [720/1000], Train Loss: 0.03157396, Train MAE: 0.12254519, Train RMSE: 0.17769064, Train DA: 0.53424658, Val Loss: 0.04596478, Val MAE: 0.16414931, Val RMSE: 0.21439397, Val DA: 0.43548387\n",
      "Epoch [721/1000], Train Loss: 0.03130189, Train MAE: 0.11949168, Train RMSE: 0.17692339, Train DA: 0.55479452, Val Loss: 0.05142136, Val MAE: 0.17019010, Val RMSE: 0.22676279, Val DA: 0.41935484\n",
      "Epoch [722/1000], Train Loss: 0.03135595, Train MAE: 0.12467598, Train RMSE: 0.17707610, Train DA: 0.54109589, Val Loss: 0.04784770, Val MAE: 0.16598520, Val RMSE: 0.21874116, Val DA: 0.45161290\n",
      "Epoch [723/1000], Train Loss: 0.03307334, Train MAE: 0.12790616, Train RMSE: 0.18186076, Train DA: 0.54794521, Val Loss: 0.04669160, Val MAE: 0.16696681, Val RMSE: 0.21608238, Val DA: 0.45161290\n",
      "Epoch [724/1000], Train Loss: 0.03139628, Train MAE: 0.12025863, Train RMSE: 0.17718996, Train DA: 0.54109589, Val Loss: 0.04803830, Val MAE: 0.16737729, Val RMSE: 0.21917643, Val DA: 0.45161290\n",
      "Epoch [725/1000], Train Loss: 0.03014592, Train MAE: 0.11511903, Train RMSE: 0.17362580, Train DA: 0.56849315, Val Loss: 0.04907027, Val MAE: 0.16665681, Val RMSE: 0.22151810, Val DA: 0.41935484\n",
      "Epoch [726/1000], Train Loss: 0.03206232, Train MAE: 0.12578326, Train RMSE: 0.17905954, Train DA: 0.54794521, Val Loss: 0.04802126, Val MAE: 0.16748692, Val RMSE: 0.21913755, Val DA: 0.43548387\n",
      "Epoch [727/1000], Train Loss: 0.03144732, Train MAE: 0.12077015, Train RMSE: 0.17733392, Train DA: 0.54794521, Val Loss: 0.04549539, Val MAE: 0.16344619, Val RMSE: 0.21329647, Val DA: 0.41935484\n",
      "Epoch [728/1000], Train Loss: 0.03059305, Train MAE: 0.11727071, Train RMSE: 0.17490870, Train DA: 0.55479452, Val Loss: 0.04771552, Val MAE: 0.16578712, Val RMSE: 0.21843883, Val DA: 0.41935484\n",
      "Epoch [729/1000], Train Loss: 0.03111148, Train MAE: 0.11987714, Train RMSE: 0.17638446, Train DA: 0.56164384, Val Loss: 0.05026675, Val MAE: 0.17011030, Val RMSE: 0.22420248, Val DA: 0.43548387\n",
      "Epoch [730/1000], Train Loss: 0.03157429, Train MAE: 0.12357224, Train RMSE: 0.17769156, Train DA: 0.54794521, Val Loss: 0.04765112, Val MAE: 0.16763715, Val RMSE: 0.21829136, Val DA: 0.41935484\n",
      "Epoch [731/1000], Train Loss: 0.03106599, Train MAE: 0.11888167, Train RMSE: 0.17625545, Train DA: 0.54109589, Val Loss: 0.04692816, Val MAE: 0.16566005, Val RMSE: 0.21662907, Val DA: 0.41935484\n",
      "Epoch [732/1000], Train Loss: 0.03039881, Train MAE: 0.11469453, Train RMSE: 0.17435254, Train DA: 0.56164384, Val Loss: 0.04872919, Val MAE: 0.16739777, Val RMSE: 0.22074687, Val DA: 0.43548387\n",
      "Epoch [733/1000], Train Loss: 0.03139606, Train MAE: 0.12256549, Train RMSE: 0.17718934, Train DA: 0.56164384, Val Loss: 0.04991288, Val MAE: 0.17068049, Val RMSE: 0.22341192, Val DA: 0.41935484\n",
      "Epoch [734/1000], Train Loss: 0.03080423, Train MAE: 0.11795790, Train RMSE: 0.17551133, Train DA: 0.56164384, Val Loss: 0.04726165, Val MAE: 0.16666304, Val RMSE: 0.21739747, Val DA: 0.41935484\n",
      "Epoch [735/1000], Train Loss: 0.03062950, Train MAE: 0.11686148, Train RMSE: 0.17501284, Train DA: 0.56164384, Val Loss: 0.04749415, Val MAE: 0.16590260, Val RMSE: 0.21793154, Val DA: 0.43548387\n",
      "Epoch [736/1000], Train Loss: 0.03103077, Train MAE: 0.11728656, Train RMSE: 0.17615551, Train DA: 0.54794521, Val Loss: 0.04949966, Val MAE: 0.16869541, Val RMSE: 0.22248520, Val DA: 0.43548387\n",
      "Epoch [737/1000], Train Loss: 0.03133115, Train MAE: 0.12112579, Train RMSE: 0.17700608, Train DA: 0.56164384, Val Loss: 0.04880847, Val MAE: 0.16926554, Val RMSE: 0.22092640, Val DA: 0.41935484\n",
      "Epoch [738/1000], Train Loss: 0.03075007, Train MAE: 0.11710713, Train RMSE: 0.17535698, Train DA: 0.56164384, Val Loss: 0.04762866, Val MAE: 0.16740695, Val RMSE: 0.21823990, Val DA: 0.43548387\n",
      "Epoch [739/1000], Train Loss: 0.03079745, Train MAE: 0.11593721, Train RMSE: 0.17549202, Train DA: 0.55479452, Val Loss: 0.04931941, Val MAE: 0.16912135, Val RMSE: 0.22207975, Val DA: 0.43548387\n",
      "Epoch [740/1000], Train Loss: 0.03147095, Train MAE: 0.12096095, Train RMSE: 0.17740054, Train DA: 0.55479452, Val Loss: 0.04958355, Val MAE: 0.16992711, Val RMSE: 0.22267361, Val DA: 0.43548387\n",
      "Epoch [741/1000], Train Loss: 0.03137800, Train MAE: 0.11937267, Train RMSE: 0.17713837, Train DA: 0.56164384, Val Loss: 0.04813882, Val MAE: 0.16818680, Val RMSE: 0.21940561, Val DA: 0.43548387\n",
      "Epoch [742/1000], Train Loss: 0.03105025, Train MAE: 0.11764190, Train RMSE: 0.17621081, Train DA: 0.56164384, Val Loss: 0.04826521, Val MAE: 0.16793159, Val RMSE: 0.21969344, Val DA: 0.43548387\n",
      "Epoch [743/1000], Train Loss: 0.03103944, Train MAE: 0.11639481, Train RMSE: 0.17618014, Train DA: 0.55479452, Val Loss: 0.04976546, Val MAE: 0.16979444, Val RMSE: 0.22308174, Val DA: 0.43548387\n",
      "Epoch [744/1000], Train Loss: 0.03171848, Train MAE: 0.12211343, Train RMSE: 0.17809683, Train DA: 0.55479452, Val Loss: 0.04932144, Val MAE: 0.17058289, Val RMSE: 0.22208431, Val DA: 0.45161290\n",
      "Epoch [745/1000], Train Loss: 0.03125132, Train MAE: 0.11816163, Train RMSE: 0.17678042, Train DA: 0.56164384, Val Loss: 0.04779570, Val MAE: 0.16767648, Val RMSE: 0.21862230, Val DA: 0.45161290\n",
      "Epoch [746/1000], Train Loss: 0.03111832, Train MAE: 0.11670135, Train RMSE: 0.17640387, Train DA: 0.55479452, Val Loss: 0.04929347, Val MAE: 0.16946922, Val RMSE: 0.22202134, Val DA: 0.43548387\n",
      "Epoch [747/1000], Train Loss: 0.03152903, Train MAE: 0.11840580, Train RMSE: 0.17756416, Train DA: 0.56164384, Val Loss: 0.04976368, Val MAE: 0.17021923, Val RMSE: 0.22307773, Val DA: 0.43548387\n",
      "Epoch [748/1000], Train Loss: 0.03164740, Train MAE: 0.12083871, Train RMSE: 0.17789716, Train DA: 0.56164384, Val Loss: 0.04831175, Val MAE: 0.16884598, Val RMSE: 0.21979932, Val DA: 0.43548387\n",
      "Epoch [749/1000], Train Loss: 0.03119738, Train MAE: 0.11790007, Train RMSE: 0.17662780, Train DA: 0.56164384, Val Loss: 0.04799494, Val MAE: 0.16796058, Val RMSE: 0.21907747, Val DA: 0.45161290\n",
      "Epoch [750/1000], Train Loss: 0.03123047, Train MAE: 0.11611427, Train RMSE: 0.17672144, Train DA: 0.55479452, Val Loss: 0.04965758, Val MAE: 0.17028944, Val RMSE: 0.22283980, Val DA: 0.43548387\n",
      "Epoch [751/1000], Train Loss: 0.03159328, Train MAE: 0.11980180, Train RMSE: 0.17774498, Train DA: 0.56164384, Val Loss: 0.04930672, Val MAE: 0.16996689, Val RMSE: 0.22205119, Val DA: 0.43548387\n",
      "Epoch [752/1000], Train Loss: 0.03155440, Train MAE: 0.11960845, Train RMSE: 0.17763558, Train DA: 0.56164384, Val Loss: 0.04797222, Val MAE: 0.16792892, Val RMSE: 0.21902561, Val DA: 0.45161290\n",
      "Epoch [753/1000], Train Loss: 0.03137196, Train MAE: 0.11766014, Train RMSE: 0.17712131, Train DA: 0.55479452, Val Loss: 0.04859401, Val MAE: 0.16864115, Val RMSE: 0.22044048, Val DA: 0.45161290\n",
      "Epoch [754/1000], Train Loss: 0.03134044, Train MAE: 0.11617117, Train RMSE: 0.17703231, Train DA: 0.55479452, Val Loss: 0.04947475, Val MAE: 0.17003322, Val RMSE: 0.22242923, Val DA: 0.43548387\n",
      "Epoch [755/1000], Train Loss: 0.03179428, Train MAE: 0.12117543, Train RMSE: 0.17830950, Train DA: 0.56164384, Val Loss: 0.04911002, Val MAE: 0.16999128, Val RMSE: 0.22160780, Val DA: 0.45161290\n",
      "Epoch [756/1000], Train Loss: 0.03152855, Train MAE: 0.11908805, Train RMSE: 0.17756282, Train DA: 0.55479452, Val Loss: 0.04803240, Val MAE: 0.16785447, Val RMSE: 0.21916296, Val DA: 0.46774194\n",
      "Epoch [757/1000], Train Loss: 0.03138473, Train MAE: 0.11762511, Train RMSE: 0.17715737, Train DA: 0.55479452, Val Loss: 0.04846175, Val MAE: 0.16838081, Val RMSE: 0.22014029, Val DA: 0.45161290\n",
      "Epoch [758/1000], Train Loss: 0.03148293, Train MAE: 0.11666007, Train RMSE: 0.17743428, Train DA: 0.55479452, Val Loss: 0.04941633, Val MAE: 0.16990113, Val RMSE: 0.22229783, Val DA: 0.43548387\n",
      "Epoch [759/1000], Train Loss: 0.03201671, Train MAE: 0.12288687, Train RMSE: 0.17893216, Train DA: 0.54794521, Val Loss: 0.04903604, Val MAE: 0.17015435, Val RMSE: 0.22144082, Val DA: 0.46774194\n",
      "Epoch [760/1000], Train Loss: 0.03142841, Train MAE: 0.11874685, Train RMSE: 0.17728060, Train DA: 0.55479452, Val Loss: 0.04775850, Val MAE: 0.16720037, Val RMSE: 0.21853718, Val DA: 0.46774194\n",
      "Epoch [761/1000], Train Loss: 0.03157853, Train MAE: 0.11845808, Train RMSE: 0.17770347, Train DA: 0.55479452, Val Loss: 0.04866300, Val MAE: 0.16914950, Val RMSE: 0.22059692, Val DA: 0.45161290\n",
      "Epoch [762/1000], Train Loss: 0.03147871, Train MAE: 0.11704107, Train RMSE: 0.17742242, Train DA: 0.54794521, Val Loss: 0.04924213, Val MAE: 0.16979320, Val RMSE: 0.22190568, Val DA: 0.43548387\n",
      "Epoch [763/1000], Train Loss: 0.03223136, Train MAE: 0.12501743, Train RMSE: 0.17953093, Train DA: 0.54109589, Val Loss: 0.04895632, Val MAE: 0.17054525, Val RMSE: 0.22126076, Val DA: 0.45161290\n",
      "Epoch [764/1000], Train Loss: 0.03126598, Train MAE: 0.11812647, Train RMSE: 0.17682190, Train DA: 0.54794521, Val Loss: 0.04749809, Val MAE: 0.16668998, Val RMSE: 0.21794055, Val DA: 0.46774194\n",
      "Epoch [765/1000], Train Loss: 0.03173569, Train MAE: 0.11907876, Train RMSE: 0.17814516, Train DA: 0.55479452, Val Loss: 0.04887564, Val MAE: 0.16960120, Val RMSE: 0.22107837, Val DA: 0.45161290\n",
      "Epoch [766/1000], Train Loss: 0.03161120, Train MAE: 0.11784151, Train RMSE: 0.17779537, Train DA: 0.56164384, Val Loss: 0.04919848, Val MAE: 0.16969143, Val RMSE: 0.22180732, Val DA: 0.45161290\n",
      "Epoch [767/1000], Train Loss: 0.03262391, Train MAE: 0.12744373, Train RMSE: 0.18062089, Train DA: 0.54109589, Val Loss: 0.04938920, Val MAE: 0.17166261, Val RMSE: 0.22223681, Val DA: 0.45161290\n",
      "Epoch [768/1000], Train Loss: 0.03128779, Train MAE: 0.11842896, Train RMSE: 0.17688356, Train DA: 0.54794521, Val Loss: 0.04723661, Val MAE: 0.16681047, Val RMSE: 0.21733984, Val DA: 0.45161290\n",
      "Epoch [769/1000], Train Loss: 0.03220235, Train MAE: 0.12058616, Train RMSE: 0.17945011, Train DA: 0.56164384, Val Loss: 0.04951757, Val MAE: 0.17119181, Val RMSE: 0.22252543, Val DA: 0.45161290\n",
      "Epoch [770/1000], Train Loss: 0.03160583, Train MAE: 0.11896095, Train RMSE: 0.17778029, Train DA: 0.55479452, Val Loss: 0.04876838, Val MAE: 0.16913776, Val RMSE: 0.22083564, Val DA: 0.45161290\n",
      "Epoch [771/1000], Train Loss: 0.03308235, Train MAE: 0.12911765, Train RMSE: 0.18188554, Train DA: 0.54109589, Val Loss: 0.04921948, Val MAE: 0.17177142, Val RMSE: 0.22185461, Val DA: 0.45161290\n",
      "Epoch [772/1000], Train Loss: 0.03132018, Train MAE: 0.11865886, Train RMSE: 0.17697507, Train DA: 0.54109589, Val Loss: 0.04746525, Val MAE: 0.16740480, Val RMSE: 0.21786521, Val DA: 0.46774194\n",
      "Epoch [773/1000], Train Loss: 0.03208869, Train MAE: 0.11973885, Train RMSE: 0.17913316, Train DA: 0.56849315, Val Loss: 0.04999020, Val MAE: 0.17196088, Val RMSE: 0.22358491, Val DA: 0.46774194\n",
      "Epoch [774/1000], Train Loss: 0.03212816, Train MAE: 0.12292976, Train RMSE: 0.17924331, Train DA: 0.54794521, Val Loss: 0.04850020, Val MAE: 0.16948141, Val RMSE: 0.22022758, Val DA: 0.43548387\n",
      "Epoch [775/1000], Train Loss: 0.03273201, Train MAE: 0.12809670, Train RMSE: 0.18091990, Train DA: 0.54794521, Val Loss: 0.04761368, Val MAE: 0.16917469, Val RMSE: 0.21820560, Val DA: 0.45161290\n",
      "Epoch [776/1000], Train Loss: 0.03200945, Train MAE: 0.11933829, Train RMSE: 0.17891185, Train DA: 0.55479452, Val Loss: 0.04976942, Val MAE: 0.17239366, Val RMSE: 0.22309062, Val DA: 0.46774194\n",
      "Epoch [777/1000], Train Loss: 0.03194805, Train MAE: 0.11893994, Train RMSE: 0.17874017, Train DA: 0.56164384, Val Loss: 0.04945441, Val MAE: 0.17234756, Val RMSE: 0.22238348, Val DA: 0.45161290\n",
      "Epoch [778/1000], Train Loss: 0.03267907, Train MAE: 0.12647553, Train RMSE: 0.18077353, Train DA: 0.56164384, Val Loss: 0.04932719, Val MAE: 0.17204240, Val RMSE: 0.22209726, Val DA: 0.43548387\n",
      "Epoch [779/1000], Train Loss: 0.03283008, Train MAE: 0.12501737, Train RMSE: 0.18119073, Train DA: 0.55479452, Val Loss: 0.04629773, Val MAE: 0.16604601, Val RMSE: 0.21516909, Val DA: 0.45161290\n",
      "Epoch [780/1000], Train Loss: 0.03189545, Train MAE: 0.11860426, Train RMSE: 0.17859298, Train DA: 0.56164384, Val Loss: 0.04861160, Val MAE: 0.16959460, Val RMSE: 0.22048038, Val DA: 0.46774194\n",
      "Epoch [781/1000], Train Loss: 0.03335442, Train MAE: 0.12086269, Train RMSE: 0.18263192, Train DA: 0.57534247, Val Loss: 0.05807859, Val MAE: 0.18532136, Val RMSE: 0.24099499, Val DA: 0.46774194\n",
      "Epoch [782/1000], Train Loss: 0.03417278, Train MAE: 0.13374284, Train RMSE: 0.18485881, Train DA: 0.56164384, Val Loss: 0.04832784, Val MAE: 0.17254715, Val RMSE: 0.21983592, Val DA: 0.45161290\n",
      "Epoch [783/1000], Train Loss: 0.03202879, Train MAE: 0.12332141, Train RMSE: 0.17896588, Train DA: 0.54794521, Val Loss: 0.04338609, Val MAE: 0.15963027, Val RMSE: 0.20829327, Val DA: 0.45161290\n",
      "Epoch [784/1000], Train Loss: 0.03174379, Train MAE: 0.11866260, Train RMSE: 0.17816786, Train DA: 0.56164384, Val Loss: 0.05062534, Val MAE: 0.17309397, Val RMSE: 0.22500077, Val DA: 0.48387097\n",
      "Epoch [785/1000], Train Loss: 0.03367913, Train MAE: 0.12731951, Train RMSE: 0.18351874, Train DA: 0.57534247, Val Loss: 0.05696698, Val MAE: 0.18430457, Val RMSE: 0.23867758, Val DA: 0.48387097\n",
      "Epoch [786/1000], Train Loss: 0.03336262, Train MAE: 0.12943247, Train RMSE: 0.18265438, Train DA: 0.55479452, Val Loss: 0.04626628, Val MAE: 0.16811547, Val RMSE: 0.21509597, Val DA: 0.45161290\n",
      "Epoch [787/1000], Train Loss: 0.03242267, Train MAE: 0.12320944, Train RMSE: 0.18006296, Train DA: 0.53424658, Val Loss: 0.04178977, Val MAE: 0.15664504, Val RMSE: 0.20442547, Val DA: 0.48387097\n",
      "Epoch [788/1000], Train Loss: 0.03196077, Train MAE: 0.11806306, Train RMSE: 0.17877577, Train DA: 0.57534247, Val Loss: 0.04644506, Val MAE: 0.16632880, Val RMSE: 0.21551116, Val DA: 0.45161290\n",
      "Epoch [789/1000], Train Loss: 0.03389820, Train MAE: 0.13169515, Train RMSE: 0.18411463, Train DA: 0.56849315, Val Loss: 0.04863429, Val MAE: 0.17440911, Val RMSE: 0.22053185, Val DA: 0.43548387\n",
      "Epoch [790/1000], Train Loss: 0.03147520, Train MAE: 0.12163657, Train RMSE: 0.17741252, Train DA: 0.56164384, Val Loss: 0.04464033, Val MAE: 0.16427788, Val RMSE: 0.21128258, Val DA: 0.40322581\n",
      "Epoch [791/1000], Train Loss: 0.03083782, Train MAE: 0.11605023, Train RMSE: 0.17560700, Train DA: 0.55479452, Val Loss: 0.04270495, Val MAE: 0.15592745, Val RMSE: 0.20665178, Val DA: 0.45161290\n",
      "Epoch [792/1000], Train Loss: 0.03268248, Train MAE: 0.12298698, Train RMSE: 0.18078296, Train DA: 0.56164384, Val Loss: 0.04760810, Val MAE: 0.16699106, Val RMSE: 0.21819277, Val DA: 0.41935484\n",
      "Epoch [793/1000], Train Loss: 0.03296887, Train MAE: 0.12629749, Train RMSE: 0.18157332, Train DA: 0.56849315, Val Loss: 0.04846096, Val MAE: 0.17073798, Val RMSE: 0.22013849, Val DA: 0.46774194\n",
      "Epoch [794/1000], Train Loss: 0.03170213, Train MAE: 0.12183876, Train RMSE: 0.17805091, Train DA: 0.55479452, Val Loss: 0.04490270, Val MAE: 0.16601230, Val RMSE: 0.21190259, Val DA: 0.43548387\n",
      "Epoch [795/1000], Train Loss: 0.03136863, Train MAE: 0.11827768, Train RMSE: 0.17711191, Train DA: 0.54794521, Val Loss: 0.04426618, Val MAE: 0.16239358, Val RMSE: 0.21039529, Val DA: 0.45161290\n",
      "Epoch [796/1000], Train Loss: 0.03503821, Train MAE: 0.13447508, Train RMSE: 0.18718496, Train DA: 0.54109589, Val Loss: 0.05026441, Val MAE: 0.17611402, Val RMSE: 0.22419725, Val DA: 0.46774194\n",
      "Epoch [797/1000], Train Loss: 0.03216297, Train MAE: 0.12355867, Train RMSE: 0.17934038, Train DA: 0.56849315, Val Loss: 0.04514176, Val MAE: 0.16562387, Val RMSE: 0.21246590, Val DA: 0.40322581\n",
      "Epoch [798/1000], Train Loss: 0.03273680, Train MAE: 0.12395327, Train RMSE: 0.18093315, Train DA: 0.56164384, Val Loss: 0.04495625, Val MAE: 0.16559578, Val RMSE: 0.21202891, Val DA: 0.45161290\n",
      "Epoch [799/1000], Train Loss: 0.03464829, Train MAE: 0.12681034, Train RMSE: 0.18614052, Train DA: 0.56164384, Val Loss: 0.05119127, Val MAE: 0.17355117, Val RMSE: 0.22625488, Val DA: 0.45161290\n",
      "Epoch [800/1000], Train Loss: 0.03431763, Train MAE: 0.13023227, Train RMSE: 0.18525018, Train DA: 0.55479452, Val Loss: 0.04693922, Val MAE: 0.17062894, Val RMSE: 0.21665461, Val DA: 0.43548387\n",
      "Epoch [801/1000], Train Loss: 0.03067321, Train MAE: 0.11841816, Train RMSE: 0.17513770, Train DA: 0.56849315, Val Loss: 0.03971597, Val MAE: 0.15521872, Val RMSE: 0.19928865, Val DA: 0.41935484\n",
      "Epoch [802/1000], Train Loss: 0.03720693, Train MAE: 0.12897828, Train RMSE: 0.19289097, Train DA: 0.56164384, Val Loss: 0.05121323, Val MAE: 0.17251831, Val RMSE: 0.22630341, Val DA: 0.41935484\n",
      "Epoch [803/1000], Train Loss: 0.03739153, Train MAE: 0.13817085, Train RMSE: 0.19336890, Train DA: 0.54109589, Val Loss: 0.04578667, Val MAE: 0.16296378, Val RMSE: 0.21397820, Val DA: 0.45161290\n",
      "Epoch [804/1000], Train Loss: 0.02913075, Train MAE: 0.11922413, Train RMSE: 0.17067733, Train DA: 0.52739726, Val Loss: 0.03920806, Val MAE: 0.15083024, Val RMSE: 0.19801025, Val DA: 0.41935484\n",
      "Epoch [805/1000], Train Loss: 0.02947649, Train MAE: 0.11610105, Train RMSE: 0.17168719, Train DA: 0.56849315, Val Loss: 0.03809665, Val MAE: 0.15253472, Val RMSE: 0.19518363, Val DA: 0.45161290\n",
      "Epoch [806/1000], Train Loss: 0.03614306, Train MAE: 0.13640234, Train RMSE: 0.19011328, Train DA: 0.52739726, Val Loss: 0.04357041, Val MAE: 0.16414420, Val RMSE: 0.20873526, Val DA: 0.53225806\n",
      "Epoch [807/1000], Train Loss: 0.03179177, Train MAE: 0.12795162, Train RMSE: 0.17830248, Train DA: 0.51369863, Val Loss: 0.03887854, Val MAE: 0.15595421, Val RMSE: 0.19717640, Val DA: 0.48387097\n",
      "Epoch [808/1000], Train Loss: 0.03126911, Train MAE: 0.12228870, Train RMSE: 0.17683072, Train DA: 0.54109589, Val Loss: 0.03903667, Val MAE: 0.15089144, Val RMSE: 0.19757700, Val DA: 0.48387097\n",
      "Epoch [809/1000], Train Loss: 0.02966038, Train MAE: 0.11893838, Train RMSE: 0.17222190, Train DA: 0.54109589, Val Loss: 0.03916372, Val MAE: 0.15028913, Val RMSE: 0.19789825, Val DA: 0.51612903\n",
      "Epoch [810/1000], Train Loss: 0.03333177, Train MAE: 0.13730900, Train RMSE: 0.18256991, Train DA: 0.51369863, Val Loss: 0.04669461, Val MAE: 0.16635323, Val RMSE: 0.21608935, Val DA: 0.46774194\n",
      "Epoch [811/1000], Train Loss: 0.03349840, Train MAE: 0.12683845, Train RMSE: 0.18302567, Train DA: 0.54794521, Val Loss: 0.04432292, Val MAE: 0.15736449, Val RMSE: 0.21053009, Val DA: 0.53225806\n",
      "Epoch [812/1000], Train Loss: 0.02729521, Train MAE: 0.11665010, Train RMSE: 0.16521262, Train DA: 0.57534247, Val Loss: 0.06024285, Val MAE: 0.18159595, Val RMSE: 0.24544419, Val DA: 0.53225806\n",
      "Epoch [813/1000], Train Loss: 0.03180340, Train MAE: 0.13306259, Train RMSE: 0.17833506, Train DA: 0.57534247, Val Loss: 0.06631865, Val MAE: 0.19347322, Val RMSE: 0.25752407, Val DA: 0.59677419\n",
      "Epoch [814/1000], Train Loss: 0.04126525, Train MAE: 0.15130505, Train RMSE: 0.20313850, Train DA: 0.53424658, Val Loss: 0.06072605, Val MAE: 0.18202807, Val RMSE: 0.24642658, Val DA: 0.58064516\n",
      "Epoch [815/1000], Train Loss: 0.02819671, Train MAE: 0.11973061, Train RMSE: 0.16791879, Train DA: 0.55479452, Val Loss: 0.02274547, Val MAE: 0.11307805, Val RMSE: 0.15081602, Val DA: 0.56451613\n",
      "Epoch [816/1000], Train Loss: 0.02666215, Train MAE: 0.11338051, Train RMSE: 0.16328549, Train DA: 0.60273973, Val Loss: 0.04921246, Val MAE: 0.14998493, Val RMSE: 0.22183882, Val DA: 0.51612903\n",
      "Epoch [817/1000], Train Loss: 0.03639910, Train MAE: 0.15084171, Train RMSE: 0.19078547, Train DA: 0.54794521, Val Loss: 0.04863720, Val MAE: 0.17638807, Val RMSE: 0.22053844, Val DA: 0.58064516\n",
      "Epoch [818/1000], Train Loss: 0.02428319, Train MAE: 0.12594257, Train RMSE: 0.15583064, Train DA: 0.65068493, Val Loss: 0.02158106, Val MAE: 0.11884444, Val RMSE: 0.14690493, Val DA: 0.56451613\n",
      "Epoch [819/1000], Train Loss: 0.02471515, Train MAE: 0.11290578, Train RMSE: 0.15721053, Train DA: 0.58219178, Val Loss: 0.01783588, Val MAE: 0.10191955, Val RMSE: 0.13355103, Val DA: 0.54838710\n",
      "Epoch [820/1000], Train Loss: 0.02197947, Train MAE: 0.11340901, Train RMSE: 0.14825475, Train DA: 0.50684932, Val Loss: 0.01302662, Val MAE: 0.08585536, Val RMSE: 0.11413422, Val DA: 0.48387097\n",
      "Epoch [821/1000], Train Loss: 0.02459270, Train MAE: 0.12691078, Train RMSE: 0.15682060, Train DA: 0.52739726, Val Loss: 0.01837504, Val MAE: 0.10834876, Val RMSE: 0.13555457, Val DA: 0.50000000\n",
      "Epoch [822/1000], Train Loss: 0.02067657, Train MAE: 0.10542521, Train RMSE: 0.14379349, Train DA: 0.54794521, Val Loss: 0.01490988, Val MAE: 0.09110036, Val RMSE: 0.12210602, Val DA: 0.56451613\n",
      "Epoch [823/1000], Train Loss: 0.02004376, Train MAE: 0.10249410, Train RMSE: 0.14157598, Train DA: 0.52739726, Val Loss: 0.01401808, Val MAE: 0.08376703, Val RMSE: 0.11839796, Val DA: 0.58064516\n",
      "Epoch [824/1000], Train Loss: 0.01977850, Train MAE: 0.10319959, Train RMSE: 0.14063607, Train DA: 0.58219178, Val Loss: 0.01431695, Val MAE: 0.08625139, Val RMSE: 0.11965344, Val DA: 0.51612903\n",
      "Epoch [825/1000], Train Loss: 0.02019433, Train MAE: 0.10079786, Train RMSE: 0.14210676, Train DA: 0.57534247, Val Loss: 0.01420295, Val MAE: 0.08189998, Val RMSE: 0.11917613, Val DA: 0.51612903\n",
      "Epoch [826/1000], Train Loss: 0.01948727, Train MAE: 0.10144525, Train RMSE: 0.13959682, Train DA: 0.58904110, Val Loss: 0.01412946, Val MAE: 0.08466455, Val RMSE: 0.11886740, Val DA: 0.56451613\n",
      "Epoch [827/1000], Train Loss: 0.01975493, Train MAE: 0.10066423, Train RMSE: 0.14055222, Train DA: 0.56164384, Val Loss: 0.01400082, Val MAE: 0.08307234, Val RMSE: 0.11832504, Val DA: 0.59677419\n",
      "Epoch [828/1000], Train Loss: 0.01948375, Train MAE: 0.10103232, Train RMSE: 0.13958418, Train DA: 0.58219178, Val Loss: 0.01400977, Val MAE: 0.08420417, Val RMSE: 0.11836289, Val DA: 0.61290323\n",
      "Epoch [829/1000], Train Loss: 0.01951007, Train MAE: 0.10044852, Train RMSE: 0.13967846, Train DA: 0.57534247, Val Loss: 0.01405817, Val MAE: 0.08407443, Val RMSE: 0.11856717, Val DA: 0.56451613\n",
      "Epoch [830/1000], Train Loss: 0.01990340, Train MAE: 0.10208245, Train RMSE: 0.14107941, Train DA: 0.56849315, Val Loss: 0.01403254, Val MAE: 0.08472093, Val RMSE: 0.11845902, Val DA: 0.59677419\n",
      "Epoch [831/1000], Train Loss: 0.01965244, Train MAE: 0.10097144, Train RMSE: 0.14018714, Train DA: 0.54794521, Val Loss: 0.01409741, Val MAE: 0.08498409, Val RMSE: 0.11873253, Val DA: 0.58064516\n",
      "Epoch [832/1000], Train Loss: 0.01937231, Train MAE: 0.10061392, Train RMSE: 0.13918446, Train DA: 0.54794521, Val Loss: 0.01408267, Val MAE: 0.08652689, Val RMSE: 0.11867043, Val DA: 0.58064516\n",
      "Epoch [833/1000], Train Loss: 0.01895393, Train MAE: 0.10153522, Train RMSE: 0.13767327, Train DA: 0.56164384, Val Loss: 0.01476017, Val MAE: 0.09105791, Val RMSE: 0.12149145, Val DA: 0.53225806\n",
      "Epoch [834/1000], Train Loss: 0.02116416, Train MAE: 0.11060650, Train RMSE: 0.14547905, Train DA: 0.56849315, Val Loss: 0.01705124, Val MAE: 0.10190719, Val RMSE: 0.13058043, Val DA: 0.51612903\n",
      "Epoch [835/1000], Train Loss: 0.02504865, Train MAE: 0.13008352, Train RMSE: 0.15826766, Train DA: 0.55479452, Val Loss: 0.02213551, Val MAE: 0.12292914, Val RMSE: 0.14878006, Val DA: 0.51612903\n",
      "Epoch [836/1000], Train Loss: 0.02025567, Train MAE: 0.10781184, Train RMSE: 0.14232242, Train DA: 0.51369863, Val Loss: 0.01652940, Val MAE: 0.10074042, Val RMSE: 0.12856671, Val DA: 0.46774194\n",
      "Epoch [837/1000], Train Loss: 0.02479139, Train MAE: 0.11993466, Train RMSE: 0.15745284, Train DA: 0.57534247, Val Loss: 0.01881439, Val MAE: 0.10501025, Val RMSE: 0.13716558, Val DA: 0.58064516\n",
      "Epoch [838/1000], Train Loss: 0.01981954, Train MAE: 0.10216205, Train RMSE: 0.14078191, Train DA: 0.50684932, Val Loss: 0.01571412, Val MAE: 0.08943592, Val RMSE: 0.12535597, Val DA: 0.53225806\n",
      "Epoch [839/1000], Train Loss: 0.02020299, Train MAE: 0.10730654, Train RMSE: 0.14213723, Train DA: 0.52739726, Val Loss: 0.01545296, Val MAE: 0.09329640, Val RMSE: 0.12430993, Val DA: 0.56451613\n",
      "Epoch [840/1000], Train Loss: 0.02044539, Train MAE: 0.10195068, Train RMSE: 0.14298737, Train DA: 0.58219178, Val Loss: 0.01537970, Val MAE: 0.08926567, Val RMSE: 0.12401493, Val DA: 0.51612903\n",
      "Epoch [841/1000], Train Loss: 0.02113801, Train MAE: 0.10549308, Train RMSE: 0.14538917, Train DA: 0.54794521, Val Loss: 0.01570251, Val MAE: 0.09523244, Val RMSE: 0.12530965, Val DA: 0.54838710\n",
      "Epoch [842/1000], Train Loss: 0.02183499, Train MAE: 0.10801105, Train RMSE: 0.14776668, Train DA: 0.56164384, Val Loss: 0.01704476, Val MAE: 0.10291345, Val RMSE: 0.13055557, Val DA: 0.48387097\n",
      "Epoch [843/1000], Train Loss: 0.02438278, Train MAE: 0.11715929, Train RMSE: 0.15614988, Train DA: 0.54109589, Val Loss: 0.01895256, Val MAE: 0.11226558, Val RMSE: 0.13766828, Val DA: 0.50000000\n",
      "Epoch [844/1000], Train Loss: 0.02444157, Train MAE: 0.12618148, Train RMSE: 0.15633801, Train DA: 0.48630137, Val Loss: 0.02030929, Val MAE: 0.11176173, Val RMSE: 0.14251067, Val DA: 0.46774194\n",
      "Epoch [845/1000], Train Loss: 0.02015794, Train MAE: 0.10661155, Train RMSE: 0.14197868, Train DA: 0.54109589, Val Loss: 0.01722394, Val MAE: 0.10638884, Val RMSE: 0.13124001, Val DA: 0.48387097\n",
      "Epoch [846/1000], Train Loss: 0.02300989, Train MAE: 0.11304369, Train RMSE: 0.15169013, Train DA: 0.57534247, Val Loss: 0.01770802, Val MAE: 0.10611133, Val RMSE: 0.13307148, Val DA: 0.50000000\n",
      "Epoch [847/1000], Train Loss: 0.02145746, Train MAE: 0.11109828, Train RMSE: 0.14648364, Train DA: 0.50684932, Val Loss: 0.01987943, Val MAE: 0.10905117, Val RMSE: 0.14099443, Val DA: 0.46774194\n",
      "Epoch [848/1000], Train Loss: 0.02188791, Train MAE: 0.11158203, Train RMSE: 0.14794563, Train DA: 0.49315068, Val Loss: 0.02011979, Val MAE: 0.11137556, Val RMSE: 0.14184423, Val DA: 0.46774194\n",
      "Epoch [849/1000], Train Loss: 0.02165554, Train MAE: 0.10821854, Train RMSE: 0.14715822, Train DA: 0.47260274, Val Loss: 0.01736666, Val MAE: 0.10249199, Val RMSE: 0.13178264, Val DA: 0.48387097\n",
      "Epoch [850/1000], Train Loss: 0.02393992, Train MAE: 0.11614357, Train RMSE: 0.15472528, Train DA: 0.47260274, Val Loss: 0.02057745, Val MAE: 0.11363567, Val RMSE: 0.14344844, Val DA: 0.50000000\n",
      "Epoch [851/1000], Train Loss: 0.02920057, Train MAE: 0.13712017, Train RMSE: 0.17088175, Train DA: 0.49315068, Val Loss: 0.02807528, Val MAE: 0.13396326, Val RMSE: 0.16755678, Val DA: 0.51612903\n",
      "Epoch [852/1000], Train Loss: 0.02348692, Train MAE: 0.11253667, Train RMSE: 0.15325443, Train DA: 0.54794521, Val Loss: 0.02286959, Val MAE: 0.11649948, Val RMSE: 0.15122697, Val DA: 0.48387097\n",
      "Epoch [853/1000], Train Loss: 0.02496243, Train MAE: 0.11358566, Train RMSE: 0.15799502, Train DA: 0.54109589, Val Loss: 0.01864477, Val MAE: 0.10637248, Val RMSE: 0.13654585, Val DA: 0.46774194\n",
      "Epoch [854/1000], Train Loss: 0.02244587, Train MAE: 0.11435702, Train RMSE: 0.14981946, Train DA: 0.52054795, Val Loss: 0.02247902, Val MAE: 0.11279462, Val RMSE: 0.14993007, Val DA: 0.50000000\n",
      "Epoch [855/1000], Train Loss: 0.02295342, Train MAE: 0.11652400, Train RMSE: 0.15150385, Train DA: 0.54109589, Val Loss: 0.02490284, Val MAE: 0.12488674, Val RMSE: 0.15780632, Val DA: 0.48387097\n",
      "Epoch [856/1000], Train Loss: 0.02368581, Train MAE: 0.11675430, Train RMSE: 0.15390192, Train DA: 0.52739726, Val Loss: 0.01963695, Val MAE: 0.11072914, Val RMSE: 0.14013192, Val DA: 0.58064516\n",
      "Epoch [857/1000], Train Loss: 0.02073287, Train MAE: 0.10877717, Train RMSE: 0.14398913, Train DA: 0.53424658, Val Loss: 0.01738992, Val MAE: 0.10303769, Val RMSE: 0.13187084, Val DA: 0.50000000\n",
      "Epoch [858/1000], Train Loss: 0.01993225, Train MAE: 0.10743550, Train RMSE: 0.14118160, Train DA: 0.50000000, Val Loss: 0.01860806, Val MAE: 0.10869830, Val RMSE: 0.13641137, Val DA: 0.48387097\n",
      "Epoch [859/1000], Train Loss: 0.02365476, Train MAE: 0.12163320, Train RMSE: 0.15380102, Train DA: 0.50000000, Val Loss: 0.02772521, Val MAE: 0.13057539, Val RMSE: 0.16650888, Val DA: 0.51612903\n",
      "Epoch [860/1000], Train Loss: 0.02327910, Train MAE: 0.11342347, Train RMSE: 0.15257490, Train DA: 0.52054795, Val Loss: 0.02225129, Val MAE: 0.11965782, Val RMSE: 0.14916867, Val DA: 0.50000000\n",
      "Epoch [861/1000], Train Loss: 0.02087958, Train MAE: 0.11133080, Train RMSE: 0.14449766, Train DA: 0.50684932, Val Loss: 0.01753371, Val MAE: 0.10409132, Val RMSE: 0.13241491, Val DA: 0.54838710\n",
      "Epoch [862/1000], Train Loss: 0.02265929, Train MAE: 0.11502042, Train RMSE: 0.15053001, Train DA: 0.50684932, Val Loss: 0.02236035, Val MAE: 0.11748572, Val RMSE: 0.14953378, Val DA: 0.50000000\n",
      "Epoch [863/1000], Train Loss: 0.02200610, Train MAE: 0.11130617, Train RMSE: 0.14834452, Train DA: 0.48630137, Val Loss: 0.01933540, Val MAE: 0.10790234, Val RMSE: 0.13905180, Val DA: 0.51612903\n",
      "Epoch [864/1000], Train Loss: 0.02150781, Train MAE: 0.10868625, Train RMSE: 0.14665540, Train DA: 0.51369863, Val Loss: 0.02039503, Val MAE: 0.11513549, Val RMSE: 0.14281118, Val DA: 0.48387097\n",
      "Epoch [865/1000], Train Loss: 0.02172867, Train MAE: 0.11303905, Train RMSE: 0.14740649, Train DA: 0.52054795, Val Loss: 0.02077778, Val MAE: 0.11863062, Val RMSE: 0.14414501, Val DA: 0.50000000\n",
      "Epoch [866/1000], Train Loss: 0.02672995, Train MAE: 0.12502578, Train RMSE: 0.16349299, Train DA: 0.49315068, Val Loss: 0.02778003, Val MAE: 0.13602094, Val RMSE: 0.16667341, Val DA: 0.48387097\n",
      "Epoch [867/1000], Train Loss: 0.02142676, Train MAE: 0.10780786, Train RMSE: 0.14637882, Train DA: 0.56849315, Val Loss: 0.02163878, Val MAE: 0.12084201, Val RMSE: 0.14710125, Val DA: 0.48387097\n",
      "Epoch [868/1000], Train Loss: 0.02260173, Train MAE: 0.11153183, Train RMSE: 0.15033871, Train DA: 0.50000000, Val Loss: 0.01905237, Val MAE: 0.10854413, Val RMSE: 0.13803031, Val DA: 0.50000000\n",
      "Epoch [869/1000], Train Loss: 0.02004398, Train MAE: 0.10485208, Train RMSE: 0.14157677, Train DA: 0.50000000, Val Loss: 0.01948120, Val MAE: 0.11206643, Val RMSE: 0.13957506, Val DA: 0.46774194\n",
      "Epoch [870/1000], Train Loss: 0.02827798, Train MAE: 0.13113339, Train RMSE: 0.16816057, Train DA: 0.48630137, Val Loss: 0.03162980, Val MAE: 0.14172892, Val RMSE: 0.17784770, Val DA: 0.51612903\n",
      "Epoch [871/1000], Train Loss: 0.02090439, Train MAE: 0.10705154, Train RMSE: 0.14458349, Train DA: 0.51369863, Val Loss: 0.01984856, Val MAE: 0.11606846, Val RMSE: 0.14088492, Val DA: 0.48387097\n",
      "Epoch [872/1000], Train Loss: 0.02244567, Train MAE: 0.11132739, Train RMSE: 0.14981879, Train DA: 0.51369863, Val Loss: 0.02267326, Val MAE: 0.11877810, Val RMSE: 0.15057643, Val DA: 0.53225806\n",
      "Epoch [873/1000], Train Loss: 0.02062269, Train MAE: 0.10598860, Train RMSE: 0.14360602, Train DA: 0.52739726, Val Loss: 0.02195642, Val MAE: 0.12180913, Val RMSE: 0.14817697, Val DA: 0.46774194\n",
      "Epoch [874/1000], Train Loss: 0.02526226, Train MAE: 0.12049698, Train RMSE: 0.15894108, Train DA: 0.52739726, Val Loss: 0.02582796, Val MAE: 0.13023491, Val RMSE: 0.16071080, Val DA: 0.51612903\n",
      "Epoch [875/1000], Train Loss: 0.02512495, Train MAE: 0.11767425, Train RMSE: 0.15850851, Train DA: 0.48630137, Val Loss: 0.02269771, Val MAE: 0.11921842, Val RMSE: 0.15065758, Val DA: 0.53225806\n",
      "Epoch [876/1000], Train Loss: 0.02160026, Train MAE: 0.10571154, Train RMSE: 0.14697027, Train DA: 0.54109589, Val Loss: 0.01991285, Val MAE: 0.10798022, Val RMSE: 0.14111289, Val DA: 0.45161290\n",
      "Epoch [877/1000], Train Loss: 0.02525102, Train MAE: 0.12084834, Train RMSE: 0.15890570, Train DA: 0.52054795, Val Loss: 0.02539527, Val MAE: 0.12641330, Val RMSE: 0.15935893, Val DA: 0.53225806\n",
      "Epoch [878/1000], Train Loss: 0.02806563, Train MAE: 0.12506776, Train RMSE: 0.16752799, Train DA: 0.53424658, Val Loss: 0.02759845, Val MAE: 0.13301781, Val RMSE: 0.16612780, Val DA: 0.51612903\n",
      "Epoch [879/1000], Train Loss: 0.02583000, Train MAE: 0.12180781, Train RMSE: 0.16071714, Train DA: 0.56849315, Val Loss: 0.02157404, Val MAE: 0.11547381, Val RMSE: 0.14688103, Val DA: 0.54838710\n",
      "Epoch [880/1000], Train Loss: 0.02015017, Train MAE: 0.10542729, Train RMSE: 0.14195129, Train DA: 0.56164384, Val Loss: 0.01499619, Val MAE: 0.09262381, Val RMSE: 0.12245892, Val DA: 0.53225806\n",
      "Epoch [881/1000], Train Loss: 0.02217683, Train MAE: 0.11177473, Train RMSE: 0.14891885, Train DA: 0.53424658, Val Loss: 0.01794993, Val MAE: 0.10598782, Val RMSE: 0.13397735, Val DA: 0.54838710\n",
      "Epoch [882/1000], Train Loss: 0.02306432, Train MAE: 0.11368552, Train RMSE: 0.15186940, Train DA: 0.56849315, Val Loss: 0.02056414, Val MAE: 0.11134323, Val RMSE: 0.14340201, Val DA: 0.54838710\n",
      "Epoch [883/1000], Train Loss: 0.02758321, Train MAE: 0.12242718, Train RMSE: 0.16608195, Train DA: 0.54109589, Val Loss: 0.02450371, Val MAE: 0.12447275, Val RMSE: 0.15653659, Val DA: 0.58064516\n",
      "Epoch [884/1000], Train Loss: 0.02236310, Train MAE: 0.10926502, Train RMSE: 0.14954299, Train DA: 0.52054795, Val Loss: 0.01904842, Val MAE: 0.10966313, Val RMSE: 0.13801602, Val DA: 0.62903226\n",
      "Epoch [885/1000], Train Loss: 0.02640527, Train MAE: 0.11862738, Train RMSE: 0.16249700, Train DA: 0.54109589, Val Loss: 0.02609362, Val MAE: 0.12912901, Val RMSE: 0.16153520, Val DA: 0.58064516\n",
      "Epoch [886/1000], Train Loss: 0.02643569, Train MAE: 0.12252489, Train RMSE: 0.16259058, Train DA: 0.52739726, Val Loss: 0.02647963, Val MAE: 0.13223182, Val RMSE: 0.16272566, Val DA: 0.56451613\n",
      "Epoch [887/1000], Train Loss: 0.02658353, Train MAE: 0.11904225, Train RMSE: 0.16304456, Train DA: 0.57534247, Val Loss: 0.02448134, Val MAE: 0.12379135, Val RMSE: 0.15646514, Val DA: 0.54838710\n",
      "Epoch [888/1000], Train Loss: 0.02200213, Train MAE: 0.10866699, Train RMSE: 0.14833115, Train DA: 0.54794521, Val Loss: 0.01770593, Val MAE: 0.10123952, Val RMSE: 0.13306364, Val DA: 0.48387097\n",
      "Epoch [889/1000], Train Loss: 0.02508601, Train MAE: 0.12234840, Train RMSE: 0.15838565, Train DA: 0.53424658, Val Loss: 0.02223899, Val MAE: 0.11994220, Val RMSE: 0.14912741, Val DA: 0.50000000\n",
      "Epoch [890/1000], Train Loss: 0.02394406, Train MAE: 0.11643738, Train RMSE: 0.15473868, Train DA: 0.52739726, Val Loss: 0.02200422, Val MAE: 0.11998333, Val RMSE: 0.14833820, Val DA: 0.46774194\n",
      "Epoch [891/1000], Train Loss: 0.02592886, Train MAE: 0.11849302, Train RMSE: 0.16102442, Train DA: 0.54109589, Val Loss: 0.02673010, Val MAE: 0.13236377, Val RMSE: 0.16349342, Val DA: 0.53225806\n",
      "Epoch [892/1000], Train Loss: 0.02338965, Train MAE: 0.11442956, Train RMSE: 0.15293676, Train DA: 0.50684932, Val Loss: 0.02361513, Val MAE: 0.12775284, Val RMSE: 0.15367214, Val DA: 0.51612903\n",
      "Epoch [893/1000], Train Loss: 0.02334276, Train MAE: 0.11235674, Train RMSE: 0.15278336, Train DA: 0.52739726, Val Loss: 0.02381657, Val MAE: 0.12589504, Val RMSE: 0.15432617, Val DA: 0.51612903\n",
      "Epoch [894/1000], Train Loss: 0.02385730, Train MAE: 0.11430616, Train RMSE: 0.15445808, Train DA: 0.54794521, Val Loss: 0.02498929, Val MAE: 0.13199610, Val RMSE: 0.15808001, Val DA: 0.56451613\n",
      "Epoch [895/1000], Train Loss: 0.02447525, Train MAE: 0.11533307, Train RMSE: 0.15644568, Train DA: 0.54109589, Val Loss: 0.02489538, Val MAE: 0.12936382, Val RMSE: 0.15778269, Val DA: 0.54838710\n",
      "Epoch [896/1000], Train Loss: 0.02389587, Train MAE: 0.11209104, Train RMSE: 0.15458290, Train DA: 0.54794521, Val Loss: 0.02348798, Val MAE: 0.12515680, Val RMSE: 0.15325789, Val DA: 0.59677419\n",
      "Epoch [897/1000], Train Loss: 0.02344229, Train MAE: 0.11096932, Train RMSE: 0.15310878, Train DA: 0.55479452, Val Loss: 0.02296057, Val MAE: 0.12466008, Val RMSE: 0.15152745, Val DA: 0.56451613\n",
      "Epoch [898/1000], Train Loss: 0.02338820, Train MAE: 0.11032826, Train RMSE: 0.15293200, Train DA: 0.56164384, Val Loss: 0.02485902, Val MAE: 0.13081519, Val RMSE: 0.15766744, Val DA: 0.54838710\n",
      "Epoch [899/1000], Train Loss: 0.02622939, Train MAE: 0.12195890, Train RMSE: 0.16195489, Train DA: 0.55479452, Val Loss: 0.02918053, Val MAE: 0.14015570, Val RMSE: 0.17082311, Val DA: 0.56451613\n",
      "Epoch [900/1000], Train Loss: 0.02515000, Train MAE: 0.11474014, Train RMSE: 0.15858753, Train DA: 0.54109589, Val Loss: 0.02778318, Val MAE: 0.13646546, Val RMSE: 0.16668288, Val DA: 0.54838710\n",
      "Epoch [901/1000], Train Loss: 0.02480703, Train MAE: 0.11628658, Train RMSE: 0.15750249, Train DA: 0.53424658, Val Loss: 0.02196921, Val MAE: 0.12089551, Val RMSE: 0.14822017, Val DA: 0.56451613\n",
      "Epoch [902/1000], Train Loss: 0.02250227, Train MAE: 0.10902181, Train RMSE: 0.15000758, Train DA: 0.56849315, Val Loss: 0.02155028, Val MAE: 0.11848918, Val RMSE: 0.14680013, Val DA: 0.54838710\n",
      "Epoch [903/1000], Train Loss: 0.02785291, Train MAE: 0.13027388, Train RMSE: 0.16689190, Train DA: 0.54109589, Val Loss: 0.03217232, Val MAE: 0.14639333, Val RMSE: 0.17936645, Val DA: 0.53225806\n",
      "Epoch [904/1000], Train Loss: 0.02401200, Train MAE: 0.11361059, Train RMSE: 0.15495804, Train DA: 0.50684932, Val Loss: 0.02773443, Val MAE: 0.13552113, Val RMSE: 0.16653657, Val DA: 0.56451613\n",
      "Epoch [905/1000], Train Loss: 0.02309518, Train MAE: 0.11046168, Train RMSE: 0.15197100, Train DA: 0.53424658, Val Loss: 0.02191852, Val MAE: 0.11741399, Val RMSE: 0.14804904, Val DA: 0.51612903\n",
      "Epoch [906/1000], Train Loss: 0.02390799, Train MAE: 0.11234063, Train RMSE: 0.15462208, Train DA: 0.56164384, Val Loss: 0.03023988, Val MAE: 0.14485092, Val RMSE: 0.17389618, Val DA: 0.53225806\n",
      "Epoch [907/1000], Train Loss: 0.02536351, Train MAE: 0.11900146, Train RMSE: 0.15925927, Train DA: 0.52054795, Val Loss: 0.02834487, Val MAE: 0.14018482, Val RMSE: 0.16835934, Val DA: 0.58064516\n",
      "Epoch [908/1000], Train Loss: 0.02238833, Train MAE: 0.10768668, Train RMSE: 0.14962728, Train DA: 0.55479452, Val Loss: 0.02230860, Val MAE: 0.12230196, Val RMSE: 0.14936064, Val DA: 0.58064516\n",
      "Epoch [909/1000], Train Loss: 0.02199955, Train MAE: 0.10532025, Train RMSE: 0.14832248, Train DA: 0.56849315, Val Loss: 0.02337094, Val MAE: 0.12607589, Val RMSE: 0.15287557, Val DA: 0.53225806\n",
      "Epoch [910/1000], Train Loss: 0.02895895, Train MAE: 0.13355988, Train RMSE: 0.17017327, Train DA: 0.58219178, Val Loss: 0.03601490, Val MAE: 0.15788549, Val RMSE: 0.18977590, Val DA: 0.54838710\n",
      "Epoch [911/1000], Train Loss: 0.02283205, Train MAE: 0.10995359, Train RMSE: 0.15110280, Train DA: 0.51369863, Val Loss: 0.02500099, Val MAE: 0.12926474, Val RMSE: 0.15811700, Val DA: 0.56451613\n",
      "Epoch [912/1000], Train Loss: 0.02497784, Train MAE: 0.11796944, Train RMSE: 0.15804377, Train DA: 0.54109589, Val Loss: 0.02225143, Val MAE: 0.12102488, Val RMSE: 0.14916913, Val DA: 0.53225806\n",
      "Epoch [913/1000], Train Loss: 0.02362831, Train MAE: 0.11416689, Train RMSE: 0.15371504, Train DA: 0.58219178, Val Loss: 0.02829854, Val MAE: 0.13561480, Val RMSE: 0.16822170, Val DA: 0.54838710\n",
      "Epoch [914/1000], Train Loss: 0.02869637, Train MAE: 0.12832123, Train RMSE: 0.16940002, Train DA: 0.53424658, Val Loss: 0.03564249, Val MAE: 0.15387882, Val RMSE: 0.18879218, Val DA: 0.54838710\n",
      "Epoch [915/1000], Train Loss: 0.02276005, Train MAE: 0.11005918, Train RMSE: 0.15086433, Train DA: 0.50684932, Val Loss: 0.02308486, Val MAE: 0.11393648, Val RMSE: 0.15193702, Val DA: 0.46774194\n",
      "Epoch [916/1000], Train Loss: 0.02095695, Train MAE: 0.10500957, Train RMSE: 0.14476515, Train DA: 0.52739726, Val Loss: 0.02356067, Val MAE: 0.12402091, Val RMSE: 0.15349483, Val DA: 0.51612903\n",
      "Epoch [917/1000], Train Loss: 0.02847619, Train MAE: 0.13403888, Train RMSE: 0.16874890, Train DA: 0.56849315, Val Loss: 0.03914087, Val MAE: 0.16375528, Val RMSE: 0.19784053, Val DA: 0.51612903\n",
      "Epoch [918/1000], Train Loss: 0.02882254, Train MAE: 0.12814441, Train RMSE: 0.16977204, Train DA: 0.57534247, Val Loss: 0.02311409, Val MAE: 0.11614583, Val RMSE: 0.15203321, Val DA: 0.46774194\n",
      "Epoch [919/1000], Train Loss: 0.02181260, Train MAE: 0.10301104, Train RMSE: 0.14769088, Train DA: 0.57534247, Val Loss: 0.01624741, Val MAE: 0.08717527, Val RMSE: 0.12746534, Val DA: 0.53225806\n",
      "Epoch [920/1000], Train Loss: 0.02147717, Train MAE: 0.11191040, Train RMSE: 0.14655092, Train DA: 0.59589041, Val Loss: 0.01605472, Val MAE: 0.09428817, Val RMSE: 0.12670721, Val DA: 0.46774194\n",
      "Epoch [921/1000], Train Loss: 0.02067828, Train MAE: 0.10029042, Train RMSE: 0.14379942, Train DA: 0.58219178, Val Loss: 0.01488036, Val MAE: 0.08205102, Val RMSE: 0.12198508, Val DA: 0.46774194\n",
      "Epoch [922/1000], Train Loss: 0.02037616, Train MAE: 0.10522101, Train RMSE: 0.14274506, Train DA: 0.55479452, Val Loss: 0.01483313, Val MAE: 0.08735936, Val RMSE: 0.12179135, Val DA: 0.51612903\n",
      "Epoch [923/1000], Train Loss: 0.02008088, Train MAE: 0.09940739, Train RMSE: 0.14170702, Train DA: 0.52739726, Val Loss: 0.01433702, Val MAE: 0.08035854, Val RMSE: 0.11973730, Val DA: 0.51612903\n",
      "Epoch [924/1000], Train Loss: 0.01998890, Train MAE: 0.10236025, Train RMSE: 0.14138211, Train DA: 0.53424658, Val Loss: 0.01439010, Val MAE: 0.08376855, Val RMSE: 0.11995874, Val DA: 0.54838710\n",
      "Epoch [925/1000], Train Loss: 0.01988212, Train MAE: 0.09943438, Train RMSE: 0.14100397, Train DA: 0.52739726, Val Loss: 0.01417672, Val MAE: 0.08037461, Val RMSE: 0.11906604, Val DA: 0.53225806\n",
      "Epoch [926/1000], Train Loss: 0.01986070, Train MAE: 0.10113619, Train RMSE: 0.14092799, Train DA: 0.52739726, Val Loss: 0.01422920, Val MAE: 0.08201034, Val RMSE: 0.11928622, Val DA: 0.53225806\n",
      "Epoch [927/1000], Train Loss: 0.01982621, Train MAE: 0.09976117, Train RMSE: 0.14080559, Train DA: 0.54109589, Val Loss: 0.01414423, Val MAE: 0.08070584, Val RMSE: 0.11892950, Val DA: 0.53225806\n",
      "Epoch [928/1000], Train Loss: 0.01982447, Train MAE: 0.10056291, Train RMSE: 0.14079939, Train DA: 0.53424658, Val Loss: 0.01417467, Val MAE: 0.08148681, Val RMSE: 0.11905742, Val DA: 0.53225806\n",
      "Epoch [929/1000], Train Loss: 0.01981532, Train MAE: 0.10004502, Train RMSE: 0.14076687, Train DA: 0.54109589, Val Loss: 0.01414617, Val MAE: 0.08099644, Val RMSE: 0.11893768, Val DA: 0.53225806\n",
      "Epoch [930/1000], Train Loss: 0.01981625, Train MAE: 0.10028610, Train RMSE: 0.14077023, Train DA: 0.54109589, Val Loss: 0.01415721, Val MAE: 0.08122310, Val RMSE: 0.11898407, Val DA: 0.53225806\n",
      "Epoch [931/1000], Train Loss: 0.01981487, Train MAE: 0.10018843, Train RMSE: 0.14076531, Train DA: 0.54109589, Val Loss: 0.01415192, Val MAE: 0.08113442, Val RMSE: 0.11896186, Val DA: 0.53225806\n",
      "Epoch [932/1000], Train Loss: 0.01981483, Train MAE: 0.10018557, Train RMSE: 0.14076516, Train DA: 0.54109589, Val Loss: 0.01415180, Val MAE: 0.08113178, Val RMSE: 0.11896134, Val DA: 0.53225806\n",
      "Epoch [933/1000], Train Loss: 0.01981547, Train MAE: 0.10024314, Train RMSE: 0.14076746, Train DA: 0.54109589, Val Loss: 0.01415492, Val MAE: 0.08118653, Val RMSE: 0.11897446, Val DA: 0.53225806\n",
      "Epoch [934/1000], Train Loss: 0.01981462, Train MAE: 0.10015508, Train RMSE: 0.14076442, Train DA: 0.54109589, Val Loss: 0.01415041, Val MAE: 0.08110277, Val RMSE: 0.11895550, Val DA: 0.53225806\n",
      "Epoch [935/1000], Train Loss: 0.01981557, Train MAE: 0.10025349, Train RMSE: 0.14076778, Train DA: 0.54109589, Val Loss: 0.01415556, Val MAE: 0.08119572, Val RMSE: 0.11897715, Val DA: 0.51612903\n",
      "Epoch [936/1000], Train Loss: 0.01981455, Train MAE: 0.10015440, Train RMSE: 0.14076415, Train DA: 0.54109589, Val Loss: 0.01415048, Val MAE: 0.08110247, Val RMSE: 0.11895578, Val DA: 0.51612903\n",
      "Epoch [937/1000], Train Loss: 0.01981533, Train MAE: 0.10024313, Train RMSE: 0.14076695, Train DA: 0.54109589, Val Loss: 0.01415514, Val MAE: 0.08118718, Val RMSE: 0.11897537, Val DA: 0.51612903\n",
      "Epoch [938/1000], Train Loss: 0.01981450, Train MAE: 0.10016419, Train RMSE: 0.14076398, Train DA: 0.54109589, Val Loss: 0.01415105, Val MAE: 0.08111224, Val RMSE: 0.11895818, Val DA: 0.51612903\n",
      "Epoch [939/1000], Train Loss: 0.01981505, Train MAE: 0.10022832, Train RMSE: 0.14076592, Train DA: 0.54109589, Val Loss: 0.01415444, Val MAE: 0.08117406, Val RMSE: 0.11897243, Val DA: 0.50000000\n",
      "Epoch [940/1000], Train Loss: 0.01981445, Train MAE: 0.10017504, Train RMSE: 0.14076380, Train DA: 0.54109589, Val Loss: 0.01415172, Val MAE: 0.08112312, Val RMSE: 0.11896098, Val DA: 0.51612903\n",
      "Epoch [941/1000], Train Loss: 0.01981477, Train MAE: 0.10021552, Train RMSE: 0.14076495, Train DA: 0.54794521, Val Loss: 0.01415387, Val MAE: 0.08116224, Val RMSE: 0.11897003, Val DA: 0.53225806\n",
      "Epoch [942/1000], Train Loss: 0.01981440, Train MAE: 0.10018351, Train RMSE: 0.14076363, Train DA: 0.56164384, Val Loss: 0.01415227, Val MAE: 0.08113175, Val RMSE: 0.11896329, Val DA: 0.51612903\n",
      "Epoch [943/1000], Train Loss: 0.01981455, Train MAE: 0.10020573, Train RMSE: 0.14076416, Train DA: 0.56164384, Val Loss: 0.01415350, Val MAE: 0.08115333, Val RMSE: 0.11896849, Val DA: 0.50000000\n",
      "Epoch [944/1000], Train Loss: 0.01981433, Train MAE: 0.10018878, Train RMSE: 0.14076337, Train DA: 0.56164384, Val Loss: 0.01415269, Val MAE: 0.08113733, Val RMSE: 0.11896510, Val DA: 0.50000000\n",
      "Epoch [945/1000], Train Loss: 0.01981435, Train MAE: 0.10019893, Train RMSE: 0.14076345, Train DA: 0.56164384, Val Loss: 0.01415333, Val MAE: 0.08114742, Val RMSE: 0.11896779, Val DA: 0.50000000\n",
      "Epoch [946/1000], Train Loss: 0.01981436, Train MAE: 0.10021015, Train RMSE: 0.14076349, Train DA: 0.56164384, Val Loss: 0.01415408, Val MAE: 0.08115869, Val RMSE: 0.11897091, Val DA: 0.50000000\n",
      "Epoch [947/1000], Train Loss: 0.01981490, Train MAE: 0.10025363, Train RMSE: 0.14076540, Train DA: 0.55479452, Val Loss: 0.01415659, Val MAE: 0.08119898, Val RMSE: 0.11898147, Val DA: 0.50000000\n",
      "Epoch [948/1000], Train Loss: 0.01981389, Train MAE: 0.10017643, Train RMSE: 0.14076182, Train DA: 0.55479452, Val Loss: 0.01415254, Val MAE: 0.08112702, Val RMSE: 0.11896443, Val DA: 0.50000000\n",
      "Epoch [949/1000], Train Loss: 0.01981452, Train MAE: 0.10024023, Train RMSE: 0.14076407, Train DA: 0.54794521, Val Loss: 0.01415611, Val MAE: 0.08118822, Val RMSE: 0.11897946, Val DA: 0.50000000\n",
      "Epoch [950/1000], Train Loss: 0.01981366, Train MAE: 0.10016672, Train RMSE: 0.14076099, Train DA: 0.54794521, Val Loss: 0.01415227, Val MAE: 0.08111852, Val RMSE: 0.11896333, Val DA: 0.51612903\n",
      "Epoch [951/1000], Train Loss: 0.01981413, Train MAE: 0.10022517, Train RMSE: 0.14076267, Train DA: 0.54794521, Val Loss: 0.01415548, Val MAE: 0.08117517, Val RMSE: 0.11897681, Val DA: 0.50000000\n",
      "Epoch [952/1000], Train Loss: 0.01981343, Train MAE: 0.10016602, Train RMSE: 0.14076018, Train DA: 0.54794521, Val Loss: 0.01415249, Val MAE: 0.08111888, Val RMSE: 0.11896422, Val DA: 0.50000000\n",
      "Epoch [953/1000], Train Loss: 0.01981369, Train MAE: 0.10021106, Train RMSE: 0.14076112, Train DA: 0.54794521, Val Loss: 0.01415500, Val MAE: 0.08116279, Val RMSE: 0.11897478, Val DA: 0.50000000\n",
      "Epoch [954/1000], Train Loss: 0.01981317, Train MAE: 0.10017025, Train RMSE: 0.14075929, Train DA: 0.55479452, Val Loss: 0.01415299, Val MAE: 0.08112422, Val RMSE: 0.11896633, Val DA: 0.48387097\n",
      "Epoch [955/1000], Train Loss: 0.01981323, Train MAE: 0.10019948, Train RMSE: 0.14075948, Train DA: 0.55479452, Val Loss: 0.01415480, Val MAE: 0.08115309, Val RMSE: 0.11897392, Val DA: 0.48387097\n",
      "Epoch [956/1000], Train Loss: 0.01981287, Train MAE: 0.10017443, Train RMSE: 0.14075819, Train DA: 0.55479452, Val Loss: 0.01415361, Val MAE: 0.08112980, Val RMSE: 0.11896896, Val DA: 0.46774194\n",
      "Epoch [957/1000], Train Loss: 0.01981281, Train MAE: 0.10019109, Train RMSE: 0.14075799, Train DA: 0.54794521, Val Loss: 0.01415463, Val MAE: 0.08114669, Val RMSE: 0.11897322, Val DA: 0.48387097\n",
      "Epoch [958/1000], Train Loss: 0.01981243, Train MAE: 0.10017698, Train RMSE: 0.14075665, Train DA: 0.54109589, Val Loss: 0.01415425, Val MAE: 0.08113415, Val RMSE: 0.11897162, Val DA: 0.48387097\n",
      "Epoch [959/1000], Train Loss: 0.01981228, Train MAE: 0.10018425, Train RMSE: 0.14075609, Train DA: 0.54109589, Val Loss: 0.01415473, Val MAE: 0.08114226, Val RMSE: 0.11897366, Val DA: 0.48387097\n",
      "Epoch [960/1000], Train Loss: 0.01981192, Train MAE: 0.10017736, Train RMSE: 0.14075482, Train DA: 0.54794521, Val Loss: 0.01415482, Val MAE: 0.08113690, Val RMSE: 0.11897404, Val DA: 0.48387097\n",
      "Epoch [961/1000], Train Loss: 0.01981164, Train MAE: 0.10017810, Train RMSE: 0.14075381, Train DA: 0.54794521, Val Loss: 0.01415500, Val MAE: 0.08113921, Val RMSE: 0.11897478, Val DA: 0.48387097\n",
      "Epoch [962/1000], Train Loss: 0.01981127, Train MAE: 0.10017619, Train RMSE: 0.14075252, Train DA: 0.54794521, Val Loss: 0.01415535, Val MAE: 0.08113912, Val RMSE: 0.11897626, Val DA: 0.48387097\n",
      "Epoch [963/1000], Train Loss: 0.01981082, Train MAE: 0.10017234, Train RMSE: 0.14075090, Train DA: 0.54794521, Val Loss: 0.01415549, Val MAE: 0.08113749, Val RMSE: 0.11897685, Val DA: 0.48387097\n",
      "Epoch [964/1000], Train Loss: 0.01981032, Train MAE: 0.10017311, Train RMSE: 0.14074916, Train DA: 0.53424658, Val Loss: 0.01415592, Val MAE: 0.08114079, Val RMSE: 0.11897863, Val DA: 0.48387097\n",
      "Epoch [965/1000], Train Loss: 0.01980974, Train MAE: 0.10016651, Train RMSE: 0.14074709, Train DA: 0.52739726, Val Loss: 0.01415631, Val MAE: 0.08113691, Val RMSE: 0.11898029, Val DA: 0.48387097\n",
      "Epoch [966/1000], Train Loss: 0.01980905, Train MAE: 0.10016647, Train RMSE: 0.14074463, Train DA: 0.52739726, Val Loss: 0.01415689, Val MAE: 0.08114047, Val RMSE: 0.11898273, Val DA: 0.50000000\n",
      "Epoch [967/1000], Train Loss: 0.01980807, Train MAE: 0.10016029, Train RMSE: 0.14074115, Train DA: 0.52739726, Val Loss: 0.01415711, Val MAE: 0.08113930, Val RMSE: 0.11898367, Val DA: 0.50000000\n",
      "Epoch [968/1000], Train Loss: 0.01980698, Train MAE: 0.10015804, Train RMSE: 0.14073727, Train DA: 0.52739726, Val Loss: 0.01415771, Val MAE: 0.08114244, Val RMSE: 0.11898619, Val DA: 0.51612903\n",
      "Epoch [969/1000], Train Loss: 0.01980570, Train MAE: 0.10015108, Train RMSE: 0.14073271, Train DA: 0.52739726, Val Loss: 0.01415844, Val MAE: 0.08113988, Val RMSE: 0.11898923, Val DA: 0.51612903\n",
      "Epoch [970/1000], Train Loss: 0.01980397, Train MAE: 0.10014930, Train RMSE: 0.14072657, Train DA: 0.52054795, Val Loss: 0.01415938, Val MAE: 0.08114529, Val RMSE: 0.11899320, Val DA: 0.50000000\n",
      "Epoch [971/1000], Train Loss: 0.01980071, Train MAE: 0.10015110, Train RMSE: 0.14071499, Train DA: 0.52739726, Val Loss: 0.01416065, Val MAE: 0.08115170, Val RMSE: 0.11899852, Val DA: 0.50000000\n",
      "Epoch [972/1000], Train Loss: 0.01979930, Train MAE: 0.10017276, Train RMSE: 0.14070998, Train DA: 0.54109589, Val Loss: 0.01416230, Val MAE: 0.08118917, Val RMSE: 0.11900546, Val DA: 0.51612903\n",
      "Epoch [973/1000], Train Loss: 0.01979351, Train MAE: 0.10013050, Train RMSE: 0.14068940, Train DA: 0.55479452, Val Loss: 0.01416197, Val MAE: 0.08118252, Val RMSE: 0.11900406, Val DA: 0.51612903\n",
      "Epoch [974/1000], Train Loss: 0.01978726, Train MAE: 0.10015807, Train RMSE: 0.14066721, Train DA: 0.55479452, Val Loss: 0.01416105, Val MAE: 0.08125035, Val RMSE: 0.11900023, Val DA: 0.51612903\n",
      "Epoch [975/1000], Train Loss: 0.01978269, Train MAE: 0.10017049, Train RMSE: 0.14065094, Train DA: 0.55479452, Val Loss: 0.01414492, Val MAE: 0.08134609, Val RMSE: 0.11893243, Val DA: 0.53225806\n",
      "Epoch [976/1000], Train Loss: 0.01978318, Train MAE: 0.10015850, Train RMSE: 0.14065269, Train DA: 0.54794521, Val Loss: 0.01411656, Val MAE: 0.08151703, Val RMSE: 0.11881315, Val DA: 0.56451613\n",
      "Epoch [977/1000], Train Loss: 0.01983258, Train MAE: 0.10061142, Train RMSE: 0.14082819, Train DA: 0.54109589, Val Loss: 0.01410165, Val MAE: 0.08275922, Val RMSE: 0.11875035, Val DA: 0.61290323\n",
      "Epoch [978/1000], Train Loss: 0.02009621, Train MAE: 0.10077978, Train RMSE: 0.14176112, Train DA: 0.58219178, Val Loss: 0.01426817, Val MAE: 0.08563171, Val RMSE: 0.11944945, Val DA: 0.59677419\n",
      "Epoch [979/1000], Train Loss: 0.02190712, Train MAE: 0.10347144, Train RMSE: 0.14801054, Train DA: 0.56164384, Val Loss: 0.01434609, Val MAE: 0.08320029, Val RMSE: 0.11977517, Val DA: 0.50000000\n",
      "Epoch [980/1000], Train Loss: 0.02132674, Train MAE: 0.10443173, Train RMSE: 0.14603677, Train DA: 0.49315068, Val Loss: 0.01414679, Val MAE: 0.08751605, Val RMSE: 0.11894027, Val DA: 0.48387097\n",
      "Epoch [981/1000], Train Loss: 0.02120834, Train MAE: 0.11030874, Train RMSE: 0.14563084, Train DA: 0.52054795, Val Loss: 0.01571016, Val MAE: 0.09263412, Val RMSE: 0.12534019, Val DA: 0.53225806\n",
      "Epoch [982/1000], Train Loss: 0.02034222, Train MAE: 0.09983854, Train RMSE: 0.14262615, Train DA: 0.54109589, Val Loss: 0.01454365, Val MAE: 0.08112761, Val RMSE: 0.12059703, Val DA: 0.53225806\n",
      "Epoch [983/1000], Train Loss: 0.02004329, Train MAE: 0.10277804, Train RMSE: 0.14157431, Train DA: 0.50000000, Val Loss: 0.01440017, Val MAE: 0.08423278, Val RMSE: 0.12000071, Val DA: 0.54838710\n",
      "Epoch [984/1000], Train Loss: 0.01986864, Train MAE: 0.09973770, Train RMSE: 0.14095615, Train DA: 0.49315068, Val Loss: 0.01410385, Val MAE: 0.08073068, Val RMSE: 0.11875962, Val DA: 0.53225806\n",
      "Epoch [985/1000], Train Loss: 0.01988685, Train MAE: 0.10164947, Train RMSE: 0.14102073, Train DA: 0.49315068, Val Loss: 0.01420304, Val MAE: 0.08272830, Val RMSE: 0.11917648, Val DA: 0.51612903\n",
      "Epoch [986/1000], Train Loss: 0.01981189, Train MAE: 0.09991477, Train RMSE: 0.14075471, Train DA: 0.50000000, Val Loss: 0.01407516, Val MAE: 0.08132819, Val RMSE: 0.11863877, Val DA: 0.54838710\n",
      "Epoch [987/1000], Train Loss: 0.01984216, Train MAE: 0.10157013, Train RMSE: 0.14086221, Train DA: 0.50684932, Val Loss: 0.01418582, Val MAE: 0.08322478, Val RMSE: 0.11910424, Val DA: 0.54838710\n",
      "Epoch [988/1000], Train Loss: 0.01994392, Train MAE: 0.10125034, Train RMSE: 0.14122294, Train DA: 0.51369863, Val Loss: 0.01424053, Val MAE: 0.08425912, Val RMSE: 0.11933369, Val DA: 0.54838710\n",
      "Epoch [989/1000], Train Loss: 0.02008143, Train MAE: 0.10298343, Train RMSE: 0.14170894, Train DA: 0.53424658, Val Loss: 0.01461610, Val MAE: 0.08701487, Val RMSE: 0.12089705, Val DA: 0.51612903\n",
      "Epoch [990/1000], Train Loss: 0.02101237, Train MAE: 0.10546787, Train RMSE: 0.14495644, Train DA: 0.53424658, Val Loss: 0.01546148, Val MAE: 0.09404851, Val RMSE: 0.12434421, Val DA: 0.48387097\n",
      "Epoch [991/1000], Train Loss: 0.02294604, Train MAE: 0.11117973, Train RMSE: 0.15147950, Train DA: 0.54109589, Val Loss: 0.01493277, Val MAE: 0.08821065, Val RMSE: 0.12219974, Val DA: 0.46774194\n",
      "Epoch [992/1000], Train Loss: 0.03018244, Train MAE: 0.14667073, Train RMSE: 0.17373094, Train DA: 0.54794521, Val Loss: 0.02767915, Val MAE: 0.14028490, Val RMSE: 0.16637051, Val DA: 0.50000000\n",
      "Epoch [993/1000], Train Loss: 0.02042293, Train MAE: 0.09990937, Train RMSE: 0.14290883, Train DA: 0.52054795, Val Loss: 0.01467892, Val MAE: 0.08159654, Val RMSE: 0.12115662, Val DA: 0.46774194\n",
      "Epoch [994/1000], Train Loss: 0.02042258, Train MAE: 0.10551288, Train RMSE: 0.14290760, Train DA: 0.55479452, Val Loss: 0.01486827, Val MAE: 0.08768498, Val RMSE: 0.12193553, Val DA: 0.51612903\n",
      "Epoch [995/1000], Train Loss: 0.02037695, Train MAE: 0.09985540, Train RMSE: 0.14274786, Train DA: 0.56164384, Val Loss: 0.01458849, Val MAE: 0.08120970, Val RMSE: 0.12078281, Val DA: 0.46774194\n",
      "Epoch [996/1000], Train Loss: 0.02029055, Train MAE: 0.10463034, Train RMSE: 0.14244491, Train DA: 0.56849315, Val Loss: 0.01472682, Val MAE: 0.08664253, Val RMSE: 0.12135411, Val DA: 0.50000000\n",
      "Epoch [997/1000], Train Loss: 0.02013269, Train MAE: 0.09948588, Train RMSE: 0.14188969, Train DA: 0.56164384, Val Loss: 0.01437312, Val MAE: 0.08050693, Val RMSE: 0.11988796, Val DA: 0.50000000\n",
      "Epoch [998/1000], Train Loss: 0.01998768, Train MAE: 0.10234310, Train RMSE: 0.14137781, Train DA: 0.55479452, Val Loss: 0.01438098, Val MAE: 0.08372490, Val RMSE: 0.11992072, Val DA: 0.51612903\n",
      "Epoch [999/1000], Train Loss: 0.01988970, Train MAE: 0.09943875, Train RMSE: 0.14103085, Train DA: 0.53424658, Val Loss: 0.01417584, Val MAE: 0.08037633, Val RMSE: 0.11906233, Val DA: 0.50000000\n",
      "Epoch [1000/1000], Train Loss: 0.01983736, Train MAE: 0.10079636, Train RMSE: 0.14084516, Train DA: 0.54794521, Val Loss: 0.01418845, Val MAE: 0.08169208, Val RMSE: 0.11911529, Val DA: 0.51612903\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "def train_model(model, optimizer, criterion, train_src, train_tgt, val_src, val_tgt, num_epochs, batch_size, patience, convergence_threshold):\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    epochs_without_improvement = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # 训练模式\n",
    "        model.train()\n",
    "        epoch_train_loss = 0\n",
    "        for i in range(0, len(train_src), batch_size):\n",
    "            batch_src = train_src[i:i+batch_size]\n",
    "            batch_tgt = train_tgt[i:i+batch_size]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_src)\n",
    "            loss = criterion(outputs.view(-1), batch_tgt.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_train_loss += loss.item()\n",
    "        # 评估模式\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            train_outputs = model(train_src)\n",
    "            val_outputs = model(val_src)\n",
    "\n",
    "            train_loss = criterion(train_outputs.view(-1), train_tgt.view(-1))\n",
    "            val_loss = criterion(val_outputs.view(-1), val_tgt.view(-1))\n",
    "\n",
    "            train_mae = mean_absolute_error(train_tgt, train_outputs.numpy().flatten())\n",
    "            val_mae = mean_absolute_error(val_tgt, val_outputs.numpy().flatten())\n",
    "            \n",
    "            train_rmse = np.sqrt(mean_squared_error(train_tgt, train_outputs.numpy().flatten()))\n",
    "            val_rmse = np.sqrt(mean_squared_error(val_tgt, val_outputs.numpy().flatten()))\n",
    "\n",
    "            train_da = directional_accuracy(train_tgt, train_outputs.numpy().flatten())\n",
    "            val_da = directional_accuracy(val_tgt, val_outputs.numpy().flatten())\n",
    "        train_losses.append(train_loss.item())\n",
    "        val_losses.append(val_loss.item())\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss.item():.8f}, Train MAE: {train_mae:.8f}, Train RMSE: {train_rmse:.8f}, Train DA: {train_da:.8f}, Val Loss: {val_loss.item():.8f}, Val MAE: {val_mae:.8f}, Val RMSE: {val_rmse:.8f}, Val DA: {val_da:.8f}\")\n",
    "        if len(train_losses) > patience:\n",
    "            recent_train_losses = train_losses[-patience:]\n",
    "            if max(recent_train_losses) - min(recent_train_losses) < convergence_threshold:\n",
    "                print(f\"Training stopped at epoch {epoch+1} due to loss convergence.\")\n",
    "                return\n",
    "        '''if best_val_loss-val_loss.item()>1e-7:\n",
    "            best_val_loss = val_loss.item()\n",
    "            best_model = model.state_dict()\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}. Best Val Loss: {best_val_loss:.9f}\")\n",
    "            model.load_state_dict(best_model)\n",
    "            return'''\n",
    "\n",
    "# 准备训练和验证数据\n",
    "train_src_data = X_train\n",
    "train_tgt_data = Y_train\n",
    "val_src_data = X_test\n",
    "val_tgt_data = Y_test\n",
    "\n",
    "train_src_tensor = torch.tensor(train_src_data,dtype=torch.float32,device=device)\n",
    "#train_src_tensor = train_src_data.clone().detach().to(device)\n",
    "\n",
    "train_tgt_tensor = torch.tensor(train_tgt_data,dtype=torch.float32,device=device)\n",
    "val_src_tensor = torch.tensor(val_src_data,dtype=torch.float32,device=device)\n",
    "#val_src_tensor = val_src_data.clone().detach().to(device)\n",
    "\n",
    "val_tgt_tensor = torch.tensor(val_tgt_data,dtype=torch.float32,device=device)\n",
    "\n",
    "# 设置早停参数\n",
    "patience = 100\n",
    "num_epochs = 1000\n",
    "# 设置优化器和损失函数\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.MSELoss()\n",
    "# 训练模型\n",
    "train_model(model, optimizer, criterion, train_src_tensor, train_tgt_tensor, val_src_tensor, val_tgt_tensor, num_epochs=num_epochs, batch_size=32, patience=patience, convergence_threshold=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict X_test using trained model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    train_outputs = model(train_src_tensor)\n",
    "    test_outputs = model(val_src_tensor)\n",
    "    test_predictions = test_outputs.numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_comparison(sequence1, sequence2):\n",
    "# 假设有两个序列 sequence1 和 sequence2\n",
    "\n",
    "    # 创建一个图形和坐标轴\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # 绘制第一个序列的折线图,使用蓝色\n",
    "    ax.plot(sequence1, color='blue', label='REAL')\n",
    "\n",
    "    # 绘制第二个序列的折线图,使用红色\n",
    "    ax.plot(sequence2, color='red', label='PREDICT')\n",
    "\n",
    "    # 添加图例\n",
    "    ax.legend()\n",
    "\n",
    "    # 添加标题和标签\n",
    "    ax.set_title('Comparison of Two Sequences')\n",
    "    ax.set_xlabel('Index')\n",
    "    ax.set_ylabel('Value')\n",
    "\n",
    "    # 显示图形\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_comparison(Y_test,test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_comparison(Y_train,train_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
