{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 定义Transformer模型\\nclass Transformer(nn.Module):\\n    def __init__(self, input_dim, hidden_dim, output_dim, num_heads, num_layers, ff_dim):\\n        super(Transformer, self).__init__()\\n        self.input_embedding = nn.Linear(input_dim, hidden_dim)\\n        self.pos_encoding = PositionalEncoding(hidden_dim)\\n        self.output_embedding = nn.Linear(hidden_dim, output_dim)\\n\\n        self.encoder_layers = nn.ModuleList([EncoderLayer(hidden_dim, num_heads, ff_dim) for _ in range(num_layers)])\\n        self.decoder_layers = nn.ModuleList([DecoderLayer(hidden_dim, num_heads, ff_dim) for _ in range(num_layers)])\\n\\n    def forward(self, src, tgt_mask=None):\\n        src_embed = self.input_embedding(src)\\n        src_embed = self.pos_encoding(src_embed)\\n\\n        enc_outputs = src_embed\\n        for enc_layer in self.encoder_layers:\\n            enc_outputs = enc_layer(enc_outputs)\\n\\n        dec_outputs = enc_outputs\\n        for dec_layer in self.decoder_layers:\\n            dec_outputs = dec_layer(dec_outputs, enc_outputs, tgt_mask=tgt_mask)\\n\\n        outputs = self.output_embedding(dec_outputs[:, -1, :])\\n        return outputs'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# 定义Add & Norm层\n",
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(AddNorm, self).__init__()\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "    def forward(self, x, residual):\n",
    "        return self.norm(x + residual)\n",
    "\n",
    "# 定义Feed Forward层\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, hidden_dim, ff_dim):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(hidden_dim, ff_dim)\n",
    "        self.linear2 = nn.Linear(ff_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "# 定义Multi-Head Attention层\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "\n",
    "        self.query = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.key = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.value = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        Q = self.query(query).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.key(key).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.value(value).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        energy = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.head_dim)\n",
    "\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "\n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "\n",
    "        x = torch.matmul(attention, V).transpose(1, 2).contiguous().view(batch_size, -1, self.hidden_dim)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# 定义Encoder层\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads, ff_dim):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(hidden_dim, num_heads)\n",
    "        self.ff = FeedForward(hidden_dim, ff_dim)\n",
    "        self.norm1 = AddNorm(hidden_dim)\n",
    "        self.norm2 = AddNorm(hidden_dim)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        residual = x\n",
    "        x = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x, residual)\n",
    "\n",
    "        residual = x\n",
    "        x = self.ff(x)\n",
    "        x = self.norm2(x, residual)\n",
    "\n",
    "        return x\n",
    "\n",
    "# 定义Decoder层\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads, ff_dim):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(hidden_dim, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(hidden_dim, num_heads)\n",
    "        self.ff = FeedForward(hidden_dim, ff_dim)\n",
    "        self.norm1 = AddNorm(hidden_dim)\n",
    "        self.norm2 = AddNorm(hidden_dim)\n",
    "        self.norm3 = AddNorm(hidden_dim)\n",
    "\n",
    "    def forward(self, x, enc_outputs, src_mask=None, tgt_mask=None):\n",
    "        residual = x\n",
    "        x = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x, residual)\n",
    "\n",
    "        residual = x\n",
    "        x = self.cross_attn(x, enc_outputs, enc_outputs, src_mask)\n",
    "        x = self.norm2(x, residual)\n",
    "\n",
    "        residual = x\n",
    "        x = self.ff(x)\n",
    "        x = self.norm3(x, residual)\n",
    "\n",
    "        return x\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, hidden_dim, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, hidden_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, hidden_dim, 2).float() * (-np.log(10000.0) / hidden_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_heads, num_layers, ff_dim):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.input_embedding = nn.Linear(input_dim, hidden_dim)\n",
    "        self.output_embedding = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(hidden_dim, num_heads, ff_dim) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(hidden_dim, num_heads, ff_dim) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, src, tgt_mask=None):\n",
    "        src_embed = self.input_embedding(src)\n",
    "\n",
    "        enc_outputs = src_embed\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_outputs = enc_layer(enc_outputs)\n",
    "\n",
    "        dec_outputs = enc_outputs\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_outputs = dec_layer(dec_outputs, enc_outputs, tgt_mask=tgt_mask)\n",
    "\n",
    "        outputs = self.output_embedding(dec_outputs[:, -1, :])\n",
    "        return outputs\n",
    "'''\n",
    "# 定义Transformer模型\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_heads, num_layers, ff_dim):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.input_embedding = nn.Linear(input_dim, hidden_dim)\n",
    "        self.pos_encoding = PositionalEncoding(hidden_dim)\n",
    "        self.output_embedding = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(hidden_dim, num_heads, ff_dim) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(hidden_dim, num_heads, ff_dim) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, src, tgt_mask=None):\n",
    "        src_embed = self.input_embedding(src)\n",
    "        src_embed = self.pos_encoding(src_embed)\n",
    "\n",
    "        enc_outputs = src_embed\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_outputs = enc_layer(enc_outputs)\n",
    "\n",
    "        dec_outputs = enc_outputs\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_outputs = dec_layer(dec_outputs, enc_outputs, tgt_mask=tgt_mask)\n",
    "\n",
    "        outputs = self.output_embedding(dec_outputs[:, -1, :])\n",
    "        return outputs'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('tmp2.csv')\n",
    "#data_stock=data[data.name=='万科A']\n",
    "\n",
    "features = ['company_id','open', 'close', 'high', 'low', 'volume', 'money_netflow', 'money_inflow', 'money_outflow',\n",
    "       'net_inflow_rate', 'list_sector', 'CPI', '无风险利率',\n",
    "       'total_market_cap', 'float_market_cap', 'pe_ttm', 'pb',\n",
    "       'dividend_yield_ratio', 'major_id', 'minor_id',\n",
    "       'change_ratio']\n",
    "data = data[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_dataset(data, look_back=5):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(data) - look_back - 1):\n",
    "        X.append(data[i:(i + look_back), :-1])\n",
    "        Y.append(data[i + look_back, -1])\n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_dataset(data, look_back=5):\n",
    "    X, Y = [], []\n",
    "    # 划分\n",
    "    for i in range(len(data.company_id.unique())):\n",
    "        minidata=data[data.company_id==i]\n",
    "        x, y = create_dataset(minidata.values, look_back) # array合并\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "    return np.concatenate(X), np.concatenate(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scale_need_cols = [ 'open', 'close', 'high', 'low', 'volume', 'money_netflow', 'money_inflow', 'money_outflow',\n",
    "       'net_inflow_rate', 'CPI', '无风险利率',\n",
    "       'total_market_cap', 'float_market_cap', 'pe_ttm', 'pb',\n",
    "       'dividend_yield_ratio',\n",
    "       'change_ratio']\n",
    "# 创建MinMaxScaler对象\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data.loc[:, scale_need_cols] = scaler.fit_transform(data[scale_need_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_back = 5\n",
    "X, Y = create_all_dataset(data, look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (input_embedding): Linear(in_features=20, out_features=128, bias=True)\n",
       "  (output_embedding): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (encoder_layers): ModuleList(\n",
       "    (0): EncoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): EncoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (2): EncoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (3): EncoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (4): EncoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (5): EncoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder_layers): ModuleList(\n",
       "    (0): DecoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (cross_attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm3): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): DecoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (cross_attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm3): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (2): DecoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (cross_attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm3): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (3): DecoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (cross_attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm3): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (4): DecoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (cross_attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm3): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (5): DecoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (cross_attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm3): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 设置模型参数\n",
    "input_dim = 20\n",
    "hidden_dim = 128\n",
    "output_dim = 1\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "ff_dim = 512\n",
    "\n",
    "# 创建模型实例\n",
    "model = Transformer(input_dim, hidden_dim, output_dim, num_heads, num_layers, ff_dim)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def directional_accuracy(actual, predicted):\n",
    "    actual_diff = np.diff(actual)\n",
    "    predicted_diff = np.diff(predicted)\n",
    "    return np.mean(np.sign(actual_diff) == np.sign(predicted_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, criterion, train_src, train_tgt, val_src, val_tgt, num_epochs, batch_size, patience, convergence_threshold):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # 训练模式\n",
    "        model.train()\n",
    "        epoch_train_loss = 0\n",
    "\n",
    "        for i in range(0, len(train_src), batch_size):\n",
    "            batch_src = train_src[i:i+batch_size]\n",
    "            batch_tgt = train_tgt[i:i+batch_size]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_src)\n",
    "            loss = criterion(outputs.view(-1), batch_tgt.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_train_loss += loss.item()\n",
    "\n",
    "        # 评估模式\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            train_outputs = model(train_src)\n",
    "            val_outputs = model(val_src)\n",
    "            train_loss = criterion(train_outputs.view(-1), train_tgt.view(-1))\n",
    "            val_loss = criterion(val_outputs.view(-1), val_tgt.view(-1))\n",
    "\n",
    "            train_mae = torch.mean(torch.abs(train_tgt - train_outputs.view(-1)))\n",
    "            val_mae = torch.mean(torch.abs(val_tgt - val_outputs.view(-1)))\n",
    "            train_rmse = torch.sqrt(torch.mean((train_tgt - train_outputs.view(-1))**2))\n",
    "            val_rmse = torch.sqrt(torch.mean((val_tgt - val_outputs.view(-1))**2))\n",
    "            train_da = directional_accuracy(train_tgt, train_outputs.view(-1))\n",
    "            val_da = directional_accuracy(val_tgt, val_outputs.view(-1))\n",
    "\n",
    "        train_losses.append(train_loss.item())\n",
    "        val_losses.append(val_loss.item())\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss.item():.8f}, Train MAE: {train_mae.item():.8f}, Train RMSE: {train_rmse.item():.8f}, Train DA: {train_da:.8f}, Val Loss: {val_loss.item():.8f}, Val MAE: {val_mae.item():.8f}, Val RMSE: {val_rmse.item():.8f}, Val DA: {val_da:.8f}\")\n",
    "\n",
    "        if len(train_losses) > patience:\n",
    "            recent_train_losses = train_losses[-patience:]\n",
    "            if max(recent_train_losses) - min(recent_train_losses) < convergence_threshold:\n",
    "                print(f\"Training stopped at epoch {epoch+1} due to loss convergence.\")\n",
    "                return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 准备训练和验证数据\n",
    "train_src_data = X_train\n",
    "train_tgt_data = Y_train\n",
    "val_src_data = X_test\n",
    "val_tgt_data = Y_test\n",
    "\n",
    "train_src_tensor = torch.tensor(train_src_data,dtype=torch.float32,device=device)\n",
    "#train_src_tensor = train_src_data.clone().detach().to(device)\n",
    "\n",
    "train_tgt_tensor = torch.tensor(train_tgt_data,dtype=torch.float32,device=device)\n",
    "val_src_tensor = torch.tensor(val_src_data,dtype=torch.float32,device=device)\n",
    "#val_src_tensor = val_src_data.clone().detach().to(device)\n",
    "\n",
    "val_tgt_tensor = torch.tensor(val_tgt_data,dtype=torch.float32,device=device)\n",
    "\n",
    "# 设置早停参数\n",
    "patience = 1000\n",
    "num_epochs = 2000\n",
    "# 设置优化器和损失函数\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.MSELoss()\n",
    "# 训练模型\n",
    "train_model(model, optimizer, criterion, train_src_tensor, train_tgt_tensor, val_src_tensor, val_tgt_tensor, num_epochs=num_epochs, batch_size=8, patience=patience, convergence_threshold=1e-8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
